%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Developing some intuition for Multivariable and Multivector Taylor Series}\label{chap:PJmultiTaylors}
\index{Taylor series}
%\date{April 28, 2009.  multivectorTaylors.tex}

The book \citep{doran2003gap} uses Geometric Calculus heavily in its Lagrangian treatment.  In particular it is used in some incomprehensible seeming ways in the stress energy tensor treatment.

In the treatment of transformation of the dependent variables (not the field variables themselves) of field Lagrangians, there is one bit that appears to be the first order linear term from a multivariable Taylor series expansion.  Play with multivariable Taylor series here a bit to develop some intuition with it.

\section{Single variable case, and generalization of it}

For the single variable case, Taylor series takes the form

\begin{equation}\label{eqn:multivectorTaylors:20}
\begin{aligned}
f(x) = \sum \frac{x^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=0}
\end{aligned}
\end{equation}

or
\begin{equation}\label{eqn:multivectorTaylors:40}
\begin{aligned}
f(x_0 + \epsilon) = \sum \frac{\epsilon^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=x_0}
\end{aligned}
\end{equation}

As pointed out in \citep{byron1992mca}, this can (as they demonstrated for polynomials) be put into exponential
operator form

\begin{equation}\label{eqn:multivectorTaylors:60}
\begin{aligned}
f(x_0 + \epsilon) = \left. e^{\epsilon d/dx} f(x) \right\vert_{x=x_0}
\end{aligned}
\end{equation}

Without proof, the multivector generalization of this is

\begin{equation}\label{eqn:multivectorTaylors:taylorsExponential}
\begin{aligned}
f(x_0 + \epsilon)
&= \left. e^{\epsilon \cdot \grad} f(x) \right\vert_{x=x_0}
\end{aligned}
\end{equation}

Or in full,

\begin{equation}\label{eqn:multivectorTaylors:taylorMulti}
\begin{aligned}
f(x_0 + \epsilon)
&= \sum \inv{k!} \left. {(\epsilon \cdot \grad)^k} f(x) \right\vert_{x=x_0}
\end{aligned}
\end{equation}

Let us work with this, and develop some comfort with what it means, then revisit the proof.

\section{Directional Derivatives}

First a definition of directional derivative is required.

In
\href{http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx}{standard two variable vector calculus} the directional derivative is defined in one of the following ways
\begin{equation}\label{eqn:multivectorTaylors:80}
\begin{aligned}
\spacegrad_\Bu f(x,y) &= \lim_{h \rightarrow 0} \frac{f(x + a h, y + b h) - f(x,y)}{h} \\
\Bu &= (a,b)
\end{aligned}
\end{equation}

Or \href{http://en.wikipedia.org/wiki/Directional_derivative}{in a more general vector form} as

\begin{equation}\label{eqn:multivectorTaylors:100}
\begin{aligned}
\spacegrad_\Bu f(\Bx) &= \lim_{h \rightarrow 0} \frac{f(\Bx + h\Bu) - f(\Bx)}{h}
\end{aligned}
\end{equation}

Or \href{http://mathworld.wolfram.com/DirectionalDerivative.html}{in terms of the gradient} as
\begin{equation}\label{eqn:multivectorTaylors:120}
\begin{aligned}
\spacegrad_\Bu f(\Bx) &= \frac{\Bu}{\Abs{\Bu}} \cdot \spacegrad f
\end{aligned}
\end{equation}

Each of these was for a vector parametrized scalar function, although the wikipedia article does mention a vector valued form that is identical to that use by \citep{doran2003gap}.  Specifically, that is

\begin{equation}\label{eqn:multivectorTaylors:140}
\begin{aligned}
(\epsilon \cdot \grad) f(x)
&= \lim_{h \rightarrow 0} \frac{f(x + h \epsilon) - f(x)}{h} \\
&= \left. \PD{h}{f(x + h \epsilon)} \right\vert_{h=0}
\end{aligned}
\end{equation}

Observe that this definition as a limit avoids the requirement to define the gradient upfront.  That definition is not necessarily obvious especially for multivector valued functions.

%\section{Work some examples}

\makeexample{First order linear vector polynomial}{example:multivectorTaylors:141}{

Let

\begin{equation}\label{eqn:multivectorTaylors:160}
\begin{aligned}
f(x) = a + x
\end{aligned}
\end{equation}

For this simplest of vector valued vector parametrized functions we have

\begin{equation}\label{eqn:multivectorTaylors:180}
\begin{aligned}
\PD{h}{f(x + h \epsilon)}
&= \PD{h}{} (a + x + h \epsilon) \\
&= \epsilon \\
&= (\epsilon \cdot \grad) f
\end{aligned}
\end{equation}

with no requirement to evaluate at \(h=0\) to complete the directional derivative computation.

The Taylor series expansion about \(0\) is thus

\begin{equation}\label{eqn:multivectorTaylors:200}
\begin{aligned}
f(\epsilon)
&= \left. (\epsilon \cdot \grad)^0 f \right\vert_{x=0} + \left. (\epsilon \cdot \grad)^1 f  \right\vert_{x=0} \\
&= a + \epsilon \\
\end{aligned}
\end{equation}

Nothing else could be expected.
}

\makeexample{Second order vector parametrized multivector polynomial}{example:multivectorTaylors:201}{

Now, step up the complexity slightly, and introduce a multivector valued second degree polynomial, say,

\begin{equation}\label{eqn:multivectorTaylors:secondOrder}
\begin{aligned}
f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{aligned}
\end{equation}

Here \(\alpha\) is a scalar, and all the other variables are vectors, so we have grades \(\le 3\).

For the first order partial we have
\begin{equation}\label{eqn:multivectorTaylors:220}
\begin{aligned}
&\PD{h}{f(x + h \epsilon)} \\
&= \PD{h}{} ( \alpha + a + (x + h\epsilon) y + w (x + h\epsilon) + c (x + h\epsilon)^2 + d (x + h\epsilon) e + (x + h\epsilon) g (x + h\epsilon) ) \\
&=
\epsilon y
+ w \epsilon
+ c \epsilon (x + h\epsilon)
+ c (x + h\epsilon) \epsilon
+ c \epsilon
+ d \epsilon e
+ \epsilon g (x + h\epsilon)
+ (x + h\epsilon) g \epsilon \\
\end{aligned}
\end{equation}

Evaluation at \(h=0\) we have

\begin{equation}\label{eqn:multivectorTaylors:240}
\begin{aligned}
(\epsilon \cdot \grad) f
&=
\epsilon y
+ w \epsilon
+ c \epsilon x
+ c x \epsilon
+ c \epsilon
+ d \epsilon e
+ \epsilon g x
+ x g \epsilon \\
\end{aligned}
\end{equation}

By inspection we have

\begin{equation}\label{eqn:multivectorTaylors:260}
\begin{aligned}
(\epsilon \cdot \grad)^2 f
&=
+ 2 c \epsilon^2
+ 2 \epsilon g \epsilon \\
\end{aligned}
\end{equation}

Combining things forming the Taylor series expansion about the origin we should recover our function

\begin{equation}\label{eqn:multivectorTaylors:280}
\begin{aligned}
f(\epsilon)
&= \inv{0!} \left. (\epsilon \cdot \grad)^0 f \right\vert_{x=0}
+ \inv{1!} \left. (\epsilon \cdot \grad)^1 f \right\vert_{x=0}
+ \inv{2} \left. (\epsilon \cdot \grad)^2 f \right\vert_{x=0} \\
&= \inv{1} (\alpha + a) + \inv{1} (\epsilon y + w \epsilon + c \epsilon + d \epsilon e ) + \inv{2}(2 c \epsilon^2 + 2 \epsilon g \epsilon ) \\
&= \alpha + a + \epsilon y + w \epsilon + c \epsilon + d \epsilon e + c \epsilon^2 + \epsilon g \epsilon \\
% cf:
%&f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{aligned}
\end{equation}

This should match \eqnref{eqn:multivectorTaylors:secondOrder}, with an \(x = \epsilon\) substitution, and does.  With the vector factors in these functions commutativity assumptions could not be made.  These calculations help provide a small verification that this form of Taylor series does in fact work out fine with such non-commutative variables.

Observe as well that there was really no requirement in this example that \(x\) or any of the other factors to be vectors.  If they were all bivectors or trivectors or some mix the calculations would have had the same results.
}

\section{Proof of the multivector Taylor expansion}

A peek back into \citep{hestenes1999nfc} shows that \eqnref{eqn:multivectorTaylors:taylorMulti} was in fact proved, but it was done in a very sneaky and clever way.  Rather than try to prove treat the multivector parameters explicitly, the following scalar parametrized hybrid function was created

\begin{equation}\label{eqn:multivectorTaylors:300}
\begin{aligned}
G(\tau) &= F(\Bx_0 + \tau\Ba)
\end{aligned}
\end{equation}

The scalar parametrized function \(G(\tau)\) can be Taylor expanded about the origin, and then evaluated at \(1\) resulting in \eqnref{eqn:multivectorTaylors:taylorMulti} in terms of powers of \((\Ba \cdot \grad)\).  I will not reproduce or try to enhance that proof for myself here since it is actually quite clear in the text.  Obviously the trick is non-intuitive enough that when thinking about how to prove this myself it did not occur to me.

\section{Explicit expansion for a scalar function}

Now, despite the \(a \cdot \grad\) notation being unfamiliar seeming, the end result is not.  Explicit expansion of this for a vector to scalar mapping will show this.  In fact this will also account for the \href{http://en.wikipedia.org/wiki/Hessian_matrix}{Hessian matrix}, as in

\begin{equation}\label{eqn:multivectorTaylors:320}
\begin{aligned}
y = f(\mathbf{x}+\Delta\mathbf{x}) \approx f(\mathbf{x}) + J(\mathbf{x}) \Delta \mathbf{x} %+\frac{1}{2} \Delta {\mathbf{x}}^\txtT H(\mathbf{x}) \Delta \mathbf{x}
\end{aligned}
\end{equation}

providing not only the background on where this comes from, but also the so often omitted third order and higher generalizations (most often referred to as \(\cdots\)).  Poking around a bit I see that the \href{http://en.wikipedia.org/wiki/Taylor_expansion}{wikipedia Taylor Series} does explicitly define the higher order case, but if I had seen that before the connection to the Hessian was not obvious.

\makeexample{Two variable case}{example:multivectorTaylors:341}{

Rather than start with the general case, the expansion of the first few powers of \((\Ba \cdot \spacegrad) f\) for the two variable case is enough to show the pattern.  How to further generalize this scalar function case will be clear from inspection.

Starting with the first order term, writing \(\Ba = (a,b)\) we have

\begin{equation}\label{eqn:multivectorTaylors:340}
\begin{aligned}
(\Ba \cdot \spacegrad) f(x,y)
&= \left. \PD{\tau}{} f(x + a\tau, y + b\tau) \right\vert_{\tau=0} \\
&=
\left. \left( \PD{x + a\tau}{} f(x + a\tau, y + b\tau) \PD{\tau}{(x + a\tau)} \right) \right\vert_{\tau=0} \\
&\quad+\left. \left( \PD{y + b\tau}{} f(x + a\tau, y + b\tau) \PD{\tau}{(y + b\tau)} \right) \right\vert_{\tau=0} \\
&=
a \PD{x}{f} +b \PD{y}{f} \\
&=
\Ba \cdot (\spacegrad f)
\end{aligned}
\end{equation}

For the second derivative operation we have
\begin{equation}\label{eqn:multivectorTaylors:360}
\begin{aligned}
(\Ba \cdot \spacegrad)^2 f(x,y)
&=
(\Ba \cdot \spacegrad)
\left( (\Ba \cdot \spacegrad) f(x,y) \right) \\
&=
(\Ba \cdot \spacegrad) \left( a \PD{x}{f} +b \PD{y}{f} \right) \\
&= \left. \PD{\tau}{} \left( a \PD{x}{f}(x + a\tau, y + b\tau) + b \PD{y}{f}(x + a\tau, y + b\tau) \right) \right\vert_{\tau=0} \\
\end{aligned}
\end{equation}

Especially if one makes a temporary substitution of the partials for some other named variables, it is clear this follows as
before, and one gets

\begin{equation}\label{eqn:multivectorTaylors:380}
\begin{aligned}
(\Ba \cdot \spacegrad)^2 f(x,y)
&=
a^2 \PDSq{x}{f} + b a \PDD{y}{x}{f}
+a b \PDD{x}{y}{f} + b^2 \PDSq{y}{f} \\
\end{aligned}
\end{equation}

Similarly the third order derivative operator gives us

\begin{equation}\label{eqn:multivectorTaylors:400}
\begin{aligned}
(\Ba \cdot \spacegrad)^3 f(x,y)
&=
a a a \PD{x}{}\PD{x}{}\PD{x}{} f + a b a \PD{x}{}\PD{y}{}\PD{x}{}{f}  \\
\quad&+a a b \PD{x}{}\PD{y}{}\PD{x}{} f + a b b \PD{x}{}\PD{y}{}\PD{y}{}{f} \\
&\quad+b a a \PD{y}{}\PD{x}{}\PD{x}{} f + b b a \PD{y}{}\PD{y}{}\PD{x}{}{f} \\
&\quad+b a b \PD{y}{}\PD{y}{}\PD{x}{} f + b b b \PD{y}{}\PD{y}{}\PD{y}{}{f} \\
&=
a^3 \frac{\partial^3 f}{\partial x^3}
+ 3 a^2 b \PDSq{x}{}\PD{y}{f}
+ 3 a b^2 \PD{x}{}\PDSq{y}{f}
+b^3 \frac{\partial^3 f}{\partial y^3}
\end{aligned}
\end{equation}

We no longer have the notational nicety of being able to use the gradient notation as was done for the first derivative term.  For the
first and second order derivative operations, one has the
option of using the gradient and Hessian matrix notations

\begin{equation}\label{eqn:multivectorTaylors:420}
\begin{aligned}
(\Ba \cdot \spacegrad) f(x,y) &=
\transpose{\Ba}
\begin{bmatrix}
f_{x} \\
f_{y}
\end{bmatrix}
\\
(\Ba \cdot \spacegrad)^2 f(x,y)
&=
\transpose{\Ba}
\begin{bmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{bmatrix}
\Ba
\end{aligned}
\end{equation}

But this will not be helpful past the second derivative.

Additionally, if we continue to restrict oneself to the two variable case,
it is clear that we have

\begin{equation}\label{eqn:multivectorTaylors:440}
\begin{aligned}
(\Ba \cdot \spacegrad)^n f(x,y)
&=
\sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k}
\left( \PD{x}{} \right)^{n-k}
\left( \PD{y}{} \right)^{k} f(x,y)
\end{aligned}
\end{equation}

But it is also clear that if we switch to more than two variables, a binomial
series expansion of derivative powers in this fashion will no longer work.  For
example for three (or more) variables, writing for example \(\Ba = (a_1, a_2, a_3)\),
we have

\begin{equation}\label{eqn:multivectorTaylors:460}
\begin{aligned}
(\Ba \cdot \spacegrad) f(\Bx)
&=
\sum_{i}
\left( a_i \PD{x_i}{} \right)
f(\Bx) \\
(\Ba \cdot \spacegrad)^2 f(\Bx)
&=
\sum_{ij}
\left( a_i \PD{x_i}{} \right)
\left( a_j \PD{x_j}{} \right)
f(\Bx) \\
(\Ba \cdot \spacegrad)^3 f(\Bx)
&=
\sum_{ijk}
\left( a_i \PD{x_i}{} \right)
\left( a_j \PD{x_j}{} \right)
\left( a_k \PD{x_k}{} \right)
f(\Bx)
%\\
%&\vdots
\end{aligned}
\end{equation}

If the partials are all collected into a single indexed object, one really has a tensor.  For the first and second orders we
can represent this tensor in matrix form (as the gradient and Hessian respectively)
}

\section{Gradient with non-Euclidean basis}

The directional derivative has been calculated above for a scalar function.  There is nothing intrinsic to that argument
that requires an orthonormal basis.

Suppose we have a basis \(\{\gamma_\mu\}\), and a reciprocal frame \(\{\gamma^\mu\}\).  Let

\begin{equation}\label{eqn:multivectorTaylors:480}
\begin{aligned}
x &= x^\mu \gamma_\mu = x_\mu \gamma^\mu \\
a &= a^\mu \gamma_\mu = a_\mu \gamma^\mu
\end{aligned}
\end{equation}

The first order directional derivative is then

\begin{equation}\label{eqn:multivectorTaylors:500}
\begin{aligned}
(a \cdot \grad) f(x)
&=
\left. \PD{\tau}{f}(x + \tau a) \right\vert_{\tau=0} \\
\end{aligned}
\end{equation}

This is
\begin{equation}\label{eqn:multivectorTaylors:gradDotF}
\begin{aligned}
(a \cdot \grad) f(x) &= \sum_\mu a^\mu \PD{x^\mu}{f}(x)
\end{aligned}
\end{equation}

Now, we are used to \(\grad\) as a standalone object, and want that operator defined such that we can also write \eqnref{eqn:multivectorTaylors:gradDotF}
as
\begin{equation}\label{eqn:multivectorTaylors:520}
\begin{aligned}
a \cdot (\grad f(x))
&=
\left(a^\mu \gamma_\mu \right) \cdot (\grad f(x))
\end{aligned}
\end{equation}

Comparing these we see that our partials in \eqnref{eqn:multivectorTaylors:gradDotF} do the job provided that we form the vector operator

\begin{equation}\label{eqn:multivectorTaylors:gradForNonOrtho}
\begin{aligned}
\grad &= \sum_\mu \gamma^\mu \PD{x^\mu}{}
\end{aligned}
\end{equation}

The text \citep{doran2003gap} defines \(\grad\) in this fashion, but has no logical motivation of this idea.  One sees quickly enough that this definition works, and is the required form, but building up to the construction in a way that builds on previously established ideas is still desirable.  We see here that this reciprocal frame definition of the gradient follows inevitably from the definition of the directional derivative.  Additionally this is a definition with how the directional derivative is defined in a standard Euclidean space with an orthonormal basis.

\section{Work out Gradient for a few specific multivector spaces}

The directional derivative result expressed in \eqnref{eqn:multivectorTaylors:gradDotF} holds for arbitrarily parametrized multivector spaces, and the image space can also be a generalized one.  However, the corresponding result \eqnref{eqn:multivectorTaylors:gradForNonOrtho} for the gradient itself is good only when the parameters are vectors.  These vector parameters may be non-orthonormal, and the function this is applied to does not have to be a scalar function.

If we switch to functions parametrized by multivector spaces the vector dot gradient notation also becomes misleading.  The natural generalization of the Taylor expansion for such a function, instead of \eqnref{eqn:multivectorTaylors:taylorsExponential}, or \eqnref{eqn:multivectorTaylors:taylorMulti} should instead be

\begin{equation}\label{eqn:multivectorTaylors:taylorsExponentialScalarProd}
\begin{aligned}
f(x_0 + \epsilon)
&= \left. e^{\gpgradezero{\epsilon \grad}} f(x) \right\vert_{x=x_0}
\end{aligned}
\end{equation}

Or in full,

\begin{equation}\label{eqn:multivectorTaylors:taylorMultiScalarProd}
\begin{aligned}
f(x_0 + \epsilon)
&= \sum \inv{k!} \left. {\gpgradezero{\epsilon \grad}^k} f(x) \right\vert_{x=x_0}
\end{aligned}
\end{equation}

One could alternately express this in a notationally less different form using the scalar product operator instead of grade selection, if one writes

\begin{equation}\label{eqn:multivectorTaylors:540}
\begin{aligned}
{\epsilon \stardot \grad} &\equiv \gpgradezero{\epsilon \grad}
\end{aligned}
\end{equation}

However, regardless of the notation used, the fundamental definition is still going to be the same (and the same as in the vector case), which operationally is

\begin{equation}\label{eqn:multivectorTaylors:560}
\begin{aligned}
{\epsilon \conj \grad} f(x) = \gpgradezero{\epsilon \grad} f(x)
= \left. \PD{h}{f(x + h \epsilon)} \right\vert_{h=0}
\end{aligned}
\end{equation}

\makeexample{Complex numbers}{example:multivectorTaylors:581}{
\index{complex numbers}

The simplest grade mixed multivector space is that of the complex numbers.  Let us write out the directional derivative and gradient in this space explicitly.  Writing
\begin{equation}\label{eqn:multivectorTaylors:580}
\begin{aligned}
z_0 &= u + i v \\
z &= x + i y \\
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:multivectorTaylors:600}
\begin{aligned}
\gpgradezero{z_0 \grad} f(z)
&= u \PD{x}{f} + v \PD{y}{f} \\
&= u \PD{x}{f} + i v \inv{i} \PD{y}{f} \\
&= \gpgradezero{ z_0 \left( \PD{x}{} + \inv{i} \PD{y}{} \right) } f(z) \\
\end{aligned}
\end{equation}

and we can therefore identify the gradient operator as

\begin{equation}\label{eqn:multivectorTaylors:620}
\begin{aligned}
\grad_{0,2} &= \PD{x}{} + \inv{i} \PD{y}{}
\end{aligned}
\end{equation}

Observe the similarity here between the vector gradient for a 2D Euclidean space, where we can form complex numbers by (left) factoring out a unit vector, as in

\begin{equation}\label{eqn:multivectorTaylors:640}
\begin{aligned}
\Bx
&= e_1 x + e_2 y \\
&= e_1 ( x + e_1 e_2 y ) \\
&= e_1 ( x + i y ) \\
&= e_1 z
\end{aligned}
\end{equation}

It appears that we can form this complex gradient, by (right) factoring out of the same unit vector from the vector gradient

\begin{equation}\label{eqn:multivectorTaylors:660}
\begin{aligned}
e_1 \PD{x}{} + e_2 \PD{y}{}
&=
\left( \PD{x}{} + e_2 e_1 \PD{y}{} \right) e_1 \\
&=
\left( \PD{x}{} + \inv{i} \PD{y}{} \right) e_1 \\
&=
\grad_{0,2} e_1 \\
\end{aligned}
\end{equation}

So, if we write \(\spacegrad\) as the \R{2} vector gradient, with \(\Bx = e_1 x + e_2 y = e_1 z\) as above, we have

\begin{equation}\label{eqn:multivectorTaylors:680}
\begin{aligned}
\spacegrad \Bx
&= \grad_{0,2} e_1 e_1 z \\
&= \grad_{0,2} z \\
\end{aligned}
\end{equation}

This is a rather curious equivalence between 2D vectors and complex numbers.

\paragraph{Comparison of contour integral and directional derivative Taylor series}
\index{contour integral}
\index{directional derivative}

Having a complex gradient is not familiar from standard complex variable theory.  Then again, neither is a non-contour integral formulation of complex Taylor series.  The two of these ought to be equivalent, which seems to imply there is a contour integral representation of the gradient in a complex number space too (one of the Hestenes paper's mentioned this but I did not understand the notation).

Let us do an initial comparison of the two.  We need a reminder of the contour integral form of the complex derivative.  For a function \(f(z)\) and its derivatives regular in a neighborhood of a point \(z_0\), we can evaluate

\begin{equation}\label{eqn:multivectorTaylors:700}
\begin{aligned}
\ointctrclockwise \frac{f(z) dz}{(z - z_0)^k}
&=
-\inv{k-1} \ointctrclockwise {f(z) dz}\left( \inv{(z - z_0)^{k-1}} \right)' \\
&=
\inv{k-1} \ointctrclockwise {f'(z) dz}\left( \inv{(z - z_0)^{k-1}} \right) \\
&=
\inv{(k-1)(k-2)} \ointctrclockwise {f^2(z) dz}\left( \inv{(z - z_0)^{k-2}} \right) \\
&=
\inv{(k-1)(k-2)\cdots(k-n)} \ointctrclockwise {f^n(z) dz}\left( \inv{(z - z_0)^{k-n}} \right) \\
% k-n = 1
% n = k-1
&=
\inv{(k-1)(k-2)\cdots(1)} \ointctrclockwise \frac{f^{k-1}(z) dz}{z - z_0} \\
&= \frac{2 \pi i}{(k-1)!} f^{k-1}(z_0)
\end{aligned}
\end{equation}

So we have

\begin{equation}\label{eqn:multivectorTaylors:720}
\begin{aligned}
\left. \frac{d^k}{dz^k} f(z) \right\vert_{z_0}
&=
\frac{k!}{2 \pi i}\ointctrclockwise \frac{f(z) dz}{(z - z_0)^{k+1}}
\end{aligned}
\end{equation}

Given this we now have a few alternate forms of complex Taylor series

\begin{equation}\label{eqn:multivectorTaylors:740}
\begin{aligned}
f(z_0 + \epsilon)
&= \sum \inv{k!} \left. \gpgradezero{\epsilon \grad}^k f(z) \right\vert_{z=z_0} \\
&= \sum \inv{k!} \epsilon^k \left. \frac{d^k}{dz^k} f(z) \right\vert_{z_0} \\
&= \inv{2 \pi i} \sum \epsilon^k \ointctrclockwise \frac{f(z) dz}{(z - z_0)^{k+1}}
\end{aligned}
\end{equation}

Observe that the the \(0,2\) subscript for the gradient has been dropped above (ie: this is the complex gradient, not the vector
form).

\paragraph{Complex gradient compared to the derivative}

A gradient operator has been identified by factoring it out of the directional derivative.  Let us compare this to a plain old complex derivative.

\begin{equation}\label{eqn:multivectorTaylors:760}
\begin{aligned}
f'(z_0) &= \lim_{z \rightarrow z_0} \frac{ f(z) - f(z_0) }{ z - z_0}
\end{aligned}
\end{equation}

In particular, evaluating this limit for \(z = z_0 + h\), approaching \(z_0\) along the x-axis, we have

\begin{equation}\label{eqn:multivectorTaylors:780}
\begin{aligned}
f'(z_0)
&= \lim_{z \rightarrow z_0} \frac{ f(z) - f(z_0) }{ z - z_0} \\
&= \lim_{h \rightarrow 0} \frac{ f(z_0 + h) - f(z_0) }{ h } \\
&= \PD{x}{f}(z_0)
\end{aligned}
\end{equation}

Evaluating this limit for \(z = z_0 + i h\), approaching \(z_0\) along the y-axis, we have

\begin{equation}\label{eqn:multivectorTaylors:800}
\begin{aligned}
f'(z_0)
&= \lim_{h \rightarrow 0} \frac{ f(z_0 + i h) - f(z_0) }{ i h } \\
&= -i \PD{y}{f}(z_0)
\end{aligned}
\end{equation}

We have the Cauchy equations by equating these, and if the derivative exists (ie: independent of path) we require at least

\begin{equation}\label{eqn:multivectorTaylors:820}
\begin{aligned}
\PD{x}{f}(z_0) =
-i \PD{y}{f}(z_0)
\end{aligned}
\end{equation}

Or
\begin{equation}\label{eqn:multivectorTaylors:840}
\begin{aligned}
0
&=
\PD{x}{f}(z_0) + i \PD{y}{f}(z_0) \\
&=
\tilde{\grad} f(z_0)
\end{aligned}
\end{equation}

Premultiplying by \(\grad\) produces the harmonic equation

\begin{equation}\label{eqn:multivectorTaylors:860}
\begin{aligned}
\grad \tilde{\grad} f = \left( \PDSq{x}{} + \PDSq{y}{} \right) f
\end{aligned}
\end{equation}

\paragraph{First order expansion around a point}

The above, while interesting or curious,
does not provide a way to express the differential operator directly in terms of the gradient.

We can write
\begin{equation}\label{eqn:multivectorTaylors:880}
\begin{aligned}
\left. \gpgradezero{ \epsilon \grad } f(z) \right\vert_{z_0}
&=
\frac{\epsilon}{ 2 \pi i } \ointctrclockwise \frac{f(z) dz}{(z - z_0)^{2}} \\
&=
\epsilon f'(z_0)
\end{aligned}
\end{equation}

One can probably integrate this in some circumstances (perhaps when f(z) is regular along the straight path from \(z_0\) to \(z = z_0 + \epsilon\)).  If so, then we have

\begin{equation}\label{eqn:multivectorTaylors:900}
\begin{aligned}
\epsilon \int_{s=z_0}^{z} f'(s) ds &= \int_{s=z_0}^{z} \left. \gpgradezero{ \epsilon \grad } f(z) \right\vert_{z=s} ds
\end{aligned}
\end{equation}

Or
\begin{equation}\label{eqn:multivectorTaylors:920}
\begin{aligned}
f(z) &= f(z_0) + \int_{s=z_0}^{z} \left. \inv{\epsilon}\gpgradezero{ \epsilon \grad } f(z) \right\vert_{z=s} ds
\end{aligned}
\end{equation}

Is there any validity to doing this?  The idea here is to play with some circumstances where we could see where the multivector gradient may show up.  Much more play is required, some of which for discovery and the rest to do things more rigorously.
}

\makeexample{4D scalar plus bivector space}{example:multivectorTaylors:801}{

Suppose we form a scalar, bivector space by factoring out the unit time vector in a Dirac vector representation

\begin{equation}\label{eqn:multivectorTaylors:940}
\begin{aligned}
x
&= x^\mu \gamma_\mu \\
&= \left( x^0 + x^k \gamma_k \gamma_0 \right) \gamma_0 \\
&= \left( x^0 + x^k \sigma_k \right) \gamma_0 \\
&= q \gamma_0 \\
\end{aligned}
\end{equation}

This \(q\) has the structure of a quaternion-like object (scalar, plus bivector), but the bivectors all have positive square.  Our directional derivative, for multivector direction \(Q = Q^0 + Q^k \sigma_k\) is

\begin{equation}\label{eqn:multivectorTaylors:960}
\begin{aligned}
\gpgradezero{Q \grad} f(q)
&= Q^0 \PD{x^0}{f} + \sum_k Q^k \PD{x^k}{f} \\
\end{aligned}
\end{equation}

So, we can write

\begin{equation}\label{eqn:multivectorTaylors:980}
\begin{aligned}
\grad
&= \PD{x^0}{} + \sum_k \sigma_k \PD{x^k}{} \\
\end{aligned}
\end{equation}

We can do something similar for an Euclidean four vector space

\begin{equation}\label{eqn:multivectorTaylors:1000}
\begin{aligned}
x
&= x^\mu e_\mu \\
&= \left( x^0 + x^k e_k e_0 \right) e_0 \\
&= \left( x^0 + x^k i_k \right) e_0 \\
&= q e_0 \\
\end{aligned}
\end{equation}

Here each of the bivectors \(i_k\) have a negative square, much more quaternion-like (and could easily be defined in an isomorphic fashion).  This time we have

\begin{equation}\label{eqn:multivectorTaylors:1020}
\begin{aligned}
\grad
&= \PD{x^0}{} + \sum_k \inv{i_k} \PD{x^k}{} \\
\end{aligned}
\end{equation}
}
