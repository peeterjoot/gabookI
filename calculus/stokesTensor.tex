%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
%\input{../peeter_prologue_print.tex}
%\input{../peeter_prologue_widescreen.tex}

\chapter{Exploring Stokes Theorem in tensor form}
\label{chap:stokesTensor}
%\useCCL
%\blogpage{http://sites.google.com/site/peeterjoot/math2011/stokesTensor.pdf}
%\date{Feb 20, 2011}
%\revisionInfo{stokesTensor.tex}

%\beginArtWithToc
%\beginArtNoToc

\section{Motivation}

I have worked through Stokes theorem concepts a couple times on my own now.  One of the first times, I was trying to formulate this in a Geometric Algebra context.  I had to resort to a tensor decomposition, and pictures, before ending back in the Geometric Algebra description.  Later I figured out how to do it entirely with a Geometric Algebra description, and was able to eliminate reliance on the pictures that made the path to generalization to higher dimensional spaces unclear.

It is my expectation that if one started with a tensor description, the proof entirely in tensor form would not be difficult.  This is what I had like to try this time.  To start off, I will temporarily use the Geometric Algebra curl expression so I know what my tensor equation starting point will be, but once that starting point is found, we can work entirely in coordinate representation.  For somebody who already knows that this is the starting point, all of this initial motivation can be skipped.

\section{Translating the exterior derivative to a coordinate representation}

Our starting point is a curl, dotted with a volume element of the same grade, so that the result is a scalar

\begin{equation}\label{eqn:stokesTensor:10}
\int d^n x \cdot (\grad \wedge A).
\end{equation}

Here \(A\) is a blade of grade \(n-1\), and we wedge this with the gradient for the space

\begin{equation}\label{eqn:stokesTensor:30}
\grad \equiv e^i \partial_i = e_i \partial^i,
\end{equation}

where we with with a basis (not necessarily orthonormal) \(\{e_i\}\), and the reciprocal frame for that basis \(\{e^i\}\) defined by the relation

\begin{equation}\label{eqn:stokesTensor:50}
e^i \cdot e_j = {\delta^i}_j.
\end{equation}

Our coordinates in these basis sets are

\begin{equation}\label{eqn:stokesTensor:70}
\begin{aligned}
x \cdot e^i &\equiv x^i \\
x \cdot e_i &\equiv x_i
\end{aligned}
\end{equation}

so that

\begin{equation}\label{eqn:stokesTensor:90}
x = x^i e_i = x_i e^i.
\end{equation}

The operator coordinates of the gradient are defined in the usual fashion
\begin{equation}\label{eqn:stokesTensor:110}
\begin{aligned}
\partial_i &\equiv \PD{x^i}{} \\
\partial^i &\equiv \PD{x_i}{}
\end{aligned}
\end{equation}

The volume element for the subspace that we are integrating over we will define in terms of an arbitrary parametrization

\begin{equation}\label{eqn:stokesTensor:130}
x = x(\alpha_1, \alpha_2, \cdots, \alpha_n)
\end{equation}

The subspace can be considered spanned by the differential elements in each of the respective curves where all but the \(i\)th parameter are held constant.

\begin{equation}\label{eqn:stokesTensor:150}
dx_{\alpha_i}
= d\alpha_i \PD{\alpha_i}{x}
= d\alpha_i \PD{\alpha_i}{x^j} e_j.
\end{equation}

We assume that the integral is being performed in a subspace for which none of these differential elements in that region are linearly dependent (i.e. our Jacobian determinant must be non-zero).

The magnitude of the wedge product of all such differential elements provides the volume of the parallelogram, or parallelepiped (or higher dimensional analogue), and is

\begin{equation}\label{eqn:stokesTensor:170}
d^n x
=
d\alpha_1
d\alpha_2
\cdots
d\alpha_n
\PD{\alpha_n}{x} \wedge
\cdots
\wedge
\PD{\alpha_2}{x}
\wedge
\PD{\alpha_1}{x}.
\end{equation}

The volume element is a oriented quantity, and may be adjusted with an arbitrary sign (or equivalently an arbitrary permutation of the differential elements in the wedge product), and we will see that it is convenient for the translation to tensor form, to express these in reversed order.

Let us write

\begin{equation}\label{eqn:stokesTensor:190}
d^n \alpha = d\alpha_1 d\alpha_2 \cdots d\alpha_n,
\end{equation}

so that our volume element in coordinate form is

\begin{equation}\label{eqn:stokesTensor:210}
d^n x = d^n \alpha
\PD{\alpha_1}{x^i}
\PD{\alpha_2}{x^j}
\cdots
\PD{\alpha_{n-1}}{x^k}
\PD{\alpha_n}{x^l}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i ).
\end{equation}

Our curl will also also be a grade \(n\) blade.  We write for the \(n-1\) grade blade

\begin{equation}\label{eqn:stokesTensor:230}
A = A_{b c \cdots d} (e^b \wedge e^c \wedge \cdots e^d),
\end{equation}

where \(A_{b c \cdots d}\) is antisymmetric (i.e. \(A = a_1 \wedge a_2 \wedge \cdots a_{n-1}\) for a some set of vectors \(a_i, i \in 1 .. n-1\)).

With our gradient in coordinate form
\begin{equation}\label{eqn:stokesTensor:250}
\grad = e^a \partial_a,
\end{equation}

the curl is then
\begin{equation}\label{eqn:stokesTensor:270}
\grad \wedge A = \partial_a A_{b c \cdots d} (e^a \wedge e^b \wedge e^c \wedge \cdots e^d).
\end{equation}

The differential form for our integral can now be computed by expanding out the dot product.  We want

\begin{equation}\label{eqn:stokesTensor:290}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i )
\cdot
(e^a \wedge e^b \wedge e^c \wedge \cdots e^d)
=
((((( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i ) \cdot e^a ) \cdot e^b ) \cdot e^c ) \cdot \cdots ) \cdot e^d.
\end{equation}

Evaluation of the interior dot products introduces the intrinsic antisymmetry required for Stokes theorem.  For example, with

\begin{equation}\label{eqn:stokesTensor:960}
\begin{aligned}
( e_n \wedge e_{n-1} \wedge \cdots \wedge e_2 \wedge e_1 ) \cdot e^a
&=
( e_n \wedge e_{n-1} \wedge \cdots \wedge e_3 \wedge e_2 ) (e_1 \cdot e^a) \\
&-( e_n \wedge e_{n-1} \wedge \cdots \wedge e_3 \wedge e_1 ) (e_2 \cdot e^a) \\
&+( e_n \wedge e_{n-1} \wedge \cdots \wedge e_2 \wedge e_1 ) (e_3 \cdot e^a) \\
&\cdots \\
&(-1)^{n-1}
( e_{n-1} \wedge e_{n-2} \wedge \cdots \wedge e_2 \wedge e_1 ) (e_n \cdot e^a)
\end{aligned}
\end{equation}

Since \(e_i \cdot e^a = {\delta_i}^a\) our end result is a completely antisymmetric set of permutations of all the deltas

\begin{equation}\label{eqn:stokesTensor:310}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i )
\cdot
(e^a \wedge e^b \wedge e^c \wedge \cdots e^d)
=
{\delta^{[a}}_i
{\delta^b}_j
\cdots
{\delta^{d]}}_l,
\end{equation}

and the curl integral takes its coordinate form

\begin{equation}\label{eqn:stokesTensor:330}
\int d^n x \cdot ( \grad \wedge A ) =
\int
d^n \alpha
\PD{\alpha_1}{x^i}
\PD{\alpha_2}{x^j}
\cdots
\PD{\alpha_{n-1}}{x^k}
\PD{\alpha_n}{x^l}
\partial_a A_{b c \cdots d}
{\delta^{[a}}_i
{\delta^b}_j
\cdots
{\delta^{d]}}_l.
\end{equation}

One final contraction of the paired indices gives us our Stokes integral in its coordinate representation

\boxedEquation{eqn:stokesTensor:350}{
\int d^n x \cdot ( \grad \wedge A ) =
\int
d^n \alpha
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\partial_a A_{b c \cdots d}
}

We now have a starting point that is free of any of the abstraction of Geometric Algebra or differential forms.  We can identify the products of partials here as components of a scalar hypervolume element (possibly signed depending on the orientation of the parametrization)

\begin{equation}\label{eqn:stokesTensor:370}
d\alpha_1
d\alpha_2
\cdots
d\alpha_n
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\end{equation}

This is also a specific computation recipe for these hypervolume components, something that may not be obvious when we allow for general metrics for the space.  We are also allowing for non-orthonormal coordinate representations, and arbitrary parametrization of the subspace that we are integrating over (our integral need not have the same dimension as the underlying vector space).

Observe that when the number of parameters equals the dimension of the space, we can write out the antisymmetric term utilizing the determinant of the Jacobian matrix

\begin{equation}\label{eqn:stokesTensor:390}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
= \epsilon^{a b \cdots d} \Abs{ \frac{\partial(x^1, x^2, \cdots x^n)}{\partial(\alpha_1, \alpha_2, \cdots \alpha_n)} }
\end{equation}

When the dimension of the space \(n\) is greater than the number of parameters for the integration hypervolume in question, the antisymmetric sum of partials is still the determinant of a Jacobian matrix

\begin{equation}\label{eqn:stokesTensor:390b}
\PD{\alpha_1}{x^{[a_1}}
\PD{\alpha_2}{x^{a_2}}
\cdots
\PD{\alpha_{n-1}}{x^{a_{n-1}}}
\PD{\alpha_n}{x^{a_n]}}
= \Abs{ \frac{\partial(x^{a_1}, x^{a_2}, \cdots x^{a_n})}{\partial(\alpha_1, \alpha_2, \cdots \alpha_n)} },
\end{equation}

however, we will have one such Jacobian for each unique choice of indices.

\section{The Stokes work starts here}

The task is to relate our integral to the boundary of this volume, coming up with an explicit recipe for the description of that bounding surface, and determining the exact form of the reduced rank integral.  This job is essentially to reduce the ranks of the tensors that are being contracted in our Stokes integral.  With the derivative applied to our rank \(n-1\) antisymmetric tensor \(A_{b c \cdots d}\), we can apply the chain rule and examine the permutations so that this can be rewritten as a contraction of \(A\) itself with a set of rank \(n-1\) surface area elements.

\begin{equation}\label{eqn:stokesTensor:410}
%\inv{n!}
\int
d^n \alpha
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\partial_a A_{b c \cdots d} = ?
\end{equation}

Now, while the setup here has been completely general, this task is motivated by study of special relativity, where there is a requirement to work in a four dimensional space.  Because of that explicit goal, I am not going to attempt to formulate this in a completely abstract fashion.  That task is really one of introducing sufficiently general notation.  Instead, I am going to proceed with a simpleton approach, and do this explicitly, and repeatedly for each of the rank 1, rank 2, and rank 3 tensor cases.  It will be clear how this all generalizes by doing so, should one wish to work in still higher dimensional spaces.

\subsection{The rank 1 tensor case}

The equation we are working with for this vector case is

\begin{equation}\label{eqn:stokesTensor:430}
\int d^2 x \cdot (\grad \wedge A) =
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2)
\end{equation}

Expanding out the antisymmetric partials we have

\begin{equation}\label{eqn:stokesTensor:980}
\begin{aligned}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
&=
\PD{\alpha_1}{x^{a}}
\PD{\alpha_2}{x^{b}}
-
\PD{\alpha_1}{x^{b}}
\PD{\alpha_2}{x^{a}},
\end{aligned}
\end{equation}

with which we can reduce the integral to

\begin{equation}\label{eqn:stokesTensor:1000}
\begin{aligned}
\int d^2 x \cdot (\grad \wedge A)
&=
\int
\left( d{\alpha_1}
\PD{\alpha_1}{x^{a}}
\PD{x^a}{A_{b}} \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( d{\alpha_2}
\PD{\alpha_2}{x^{a}}
\PD{x^a}{A_{b}} \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1} \\
&=
\int
\left( d\alpha_1 \PD{\alpha_1}{A_b} \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( d\alpha_2 \PD{\alpha_2}{A_b} \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1} \\
\end{aligned}
\end{equation}

Now, if it happens that

\begin{equation}\label{eqn:stokesTensor:500}
\PD{\alpha_1}{}\PD{\alpha_2}{x^{a}} = \PD{\alpha_2}{}\PD{\alpha_1}{x^{a}} = 0
\end{equation}

then each of the individual integrals in \(d\alpha_1\) and \(d\alpha_2\) can be carried out.  In that case, without any real loss of generality we can designate the integration bounds over the unit parametrization space square \(\alpha_i \in [0,1]\), allowing this integral to be expressed as

\begin{equation}\label{eqn:stokesTensor:450}
\begin{aligned}
&\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2) \\
&=
\int
\left( A_b(1, \alpha_2) - A_b(0, \alpha_2) \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( A_b(\alpha_1, 1) - A_b(\alpha_1, 0) \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1}.
\end{aligned}
\end{equation}

It is also fairly common to see \(\evalbar{A}{\partial \alpha_i}\) used to designate evaluation of this first integral on the boundary, and using this we write

\begin{equation}\label{eqn:stokesTensor:520}
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2)
=
\int
\evalbar{A_b}{\partial \alpha_1}
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\evalbar{A_b}{\partial \alpha_2}
\PD{\alpha_1}{x^{b}} d{\alpha_1}.
\end{equation}

Also note that since we are summing over all \(a,b\), and have

\begin{equation}\label{eqn:stokesTensor:540}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
=-\PD{\alpha_1}{x^{[b}}
\PD{\alpha_2}{x^{a]}},
\end{equation}

we can write this summing over all unique pairs of \(a,b\) instead, which eliminates a small bit of redundancy (especially once the dimension of the vector space gets higher)
\boxedEquation{eqn:stokesTensor:470}{
\sum_{a < b}
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\left( \partial_a A_{b}
-\partial_b A_{a} \right)
=
\int
\evalbar{A_b}{\partial \alpha_1}
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\evalbar{A_b}{\partial \alpha_2}
\PD{\alpha_1}{x^{b}} d{\alpha_1}.
}

In this form we have recovered the original geometric structure, with components of the curl multiplied by the component of the area element that shares the orientation and direction of that portion of the curl bivector.

This form of the result with evaluation at the boundaries in this form, assumed that \(\PDi{\alpha_1}{x^a}\) was not a function of \(\alpha_2\) and \(\PDi{\alpha_2}{x^a}\) was not a function of \(\alpha_1\).  When that is not the case, we appear to have a less pretty result

\boxedEquation{eqn:stokesTensor:470g}{
\sum_{a < b}
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\left( \partial_a A_{b}
-\partial_b A_{a} \right)
=
\int d\alpha_2
\int d\alpha_1
\PD{\alpha_1}{A_b}
\PD{\alpha_2}{x^{b}}
-
\int d\alpha_2
\int d\alpha_1
\PD{\alpha_2}{A_b}
\PD{\alpha_1}{x^{b}}
}

Can this be reduced any further in the general case?  Having seen the statements of Stokes theorem in its differential forms formulation, I initially expected the answer was yes, and only when I got to evaluating my \R{4} spacetime example below did I realize that the differentials displacements for the parallelogram that constituted the area element were functions of both parameters.  Perhaps this detail is there in the differential forms version of the general Stokes theorem too, but is just hidden in a tricky fashion by the compact notation.

\subsubsection{Sanity check: \texorpdfstring{\R{2}}{2D} case in rectangular coordinates}

For \(x^1 = x, x^2 = y\), and \(\alpha_1 = x, \alpha_2 = y\), we have for the LHS

\begin{equation}\label{eqn:stokesTensor:1020}
\begin{aligned}
&\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left(
\PD{\alpha_1}{x^{1}}
\PD{\alpha_2}{x^{2}}
-\PD{\alpha_1}{x^{2}}
\PD{\alpha_2}{x^{1}}
\right)
\partial_1 A_{2}
+
\left(
\PD{\alpha_1}{x^{2}}
\PD{\alpha_2}{x^{1}}
-\PD{\alpha_1}{x^{1}}
\PD{\alpha_2}{x^{2}}
\right)
\partial_2 A_{1} \\
&=
\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left( \PD{x}{A_y} - \PD{y}{A_x} \right)
\end{aligned}
\end{equation}

Our RHS expands to

\begin{equation}\label{eqn:stokesTensor:1040}
\begin{aligned}
&\int_{y=y_0}^{y_1} dy
\left(
\left( A_1(x_1, y) - A_1(x_0, y) \right)
\PD{y}{x^{1}}
+
\left( A_2(x_1, y) - A_2(x_0, y) \right)
\PD{y}{x^{2}}
\right) \\
&\qquad-
\int_{x=x_0}^{x_1} dx
\left(
\left( A_1(x, y_1) - A_1(x, y_0) \right)
\PD{x}{x^{1}}
+
\left( A_2(x, y_1) - A_2(x, y_0) \right)
\PD{x}{x^{2}}
\right) \\
&=
\int_{y=y_0}^{y_1} dy
\left( A_y(x_1, y) - A_y(x_0, y) \right)
-
\int_{x=x_0}^{x_1} dx
\left( A_x(x, y_1) - A_x(x, y_0) \right)
\end{aligned}
\end{equation}

We have

\begin{equation}\label{eqn:stokesTensor:490}
\begin{aligned}
&\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left( \PD{x}{A_y} - \PD{y}{A_x} \right) \\
&=
\int_{y=y_0}^{y_1} dy
\left( A_y(x_1, y) - A_y(x_0, y) \right)
-
\int_{x=x_0}^{x_1} dx
\left( A_x(x, y_1) - A_x(x, y_0) \right)
\end{aligned}
\end{equation}

The RHS is just a positively oriented line integral around the rectangle of integration

\begin{equation}\label{eqn:stokesTensor:510}
\int
A_x(x, y_0) \xcap \cdot ( \xcap dx )
+ A_y(x_1, y) \ycap \cdot ( \ycap dy )
+ A_x(x, y_1) \xcap \cdot ( -\xcap dx )
+ A_y(x_0, y) \ycap \cdot ( -\ycap dy )
= \oint \BA \cdot d\Br.
\end{equation}

This special case is also recognizable as Green's theorem, evident with the substitution \(A_x = P\), \(A_y = Q\), which gives us

\begin{equation}\label{eqn:stokesTensor:530}
\int_A dx dy \left( \PD{x}{Q} - \PD{y}{P} \right)
=
\oint_C P dx + Q dy.
\end{equation}

Strictly speaking, Green's theorem is more general, since it applies to integration regions more general than rectangles, but that generalization can be arrived at easily enough, once the region is broken down into adjoining elementary regions.

\subsubsection{Sanity check: \texorpdfstring{\R{3}}{3D} case in rectangular coordinates}

It is expected that we can recover the classical Kelvin-Stokes theorem if we use rectangular coordinates in \R{3}.  However, we see that we have to consider three different parametrizations.  If one picks rectangular parametrizations \((\alpha_1, \alpha_2) = \{ (x,y), (y,z), (z,x) \}\) in sequence, in each case holding the value of the additional coordinate fixed, we get three different independent Green's function like relations

\begin{equation}\label{eqn:stokesTensor:550}
\begin{aligned}
\int_A dx dy \left( \PD{x}{A_y} - \PD{y}{A_x} \right) &= \oint_C A_x dx + A_y dy \\
\int_A dy dz \left( \PD{y}{A_z} - \PD{z}{A_y} \right) &= \oint_C A_y dy + A_z dz \\
\int_A dz dx \left( \PD{z}{A_x} - \PD{x}{A_z} \right) &= \oint_C A_z dz + A_x dx.
\end{aligned}
\end{equation}

Note that we cannot just add these to form a complete integral \(\oint \BA \cdot d\Br\) since the curves are all have different orientations.  To recover the \R{3} Stokes theorem in rectangular coordinates, it appears that we would have to consider a Riemann sum of triangular surface elements, and relate that to the loops over each of the surface elements.  In that limiting argument, only the boundary of the complete surface would contribute to the RHS of the relation.

All that said, we should not actually have to go to all this work.  Instead we can stick to a two variable parametrization of the surface, and use \eqnref{eqn:stokesTensor:470} directly.

\subsubsection{An illustration for a \R{4} spacetime surface}

Suppose we have a particle trajectory defined by an active Lorentz transformation from an initial spacetime point

\begin{equation}\label{eqn:stokesTensor:560}
x^i = O^{ij} x_j(0) = O^{ij} g_{jk} x^k = {O^{i}}_k x^k(0)
\end{equation}

Let the Lorentz transformation be formed by a composition of boost and rotation

\begin{equation}\label{eqn:stokesTensor:580}
\begin{aligned}
{O^i}_j &= {L^i}_k {R^k}_j \\
{L^i}_j &=
\begin{bmatrix}
\cosh_\alpha & -\sinh\alpha & 0 & 0 \\
-\sinh_\alpha & \cosh\alpha & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix} \\
{R^i}_j &=
\begin{bmatrix}
1 & 0 & 0 & 0 \\
\cos_\alpha & \sin\alpha & 0 & 0 \\
-\sin_\alpha & \cos\alpha & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\end{aligned}
\end{equation}

Different rates of evolution of \(\alpha\) and \(\theta\) define different trajectories, and taken together we have a surface described by the two parameters

\begin{equation}\label{eqn:stokesTensor:600}
x^i(\alpha, \theta) = {L^i}_k {R^k}_j x^j(0, 0).
\end{equation}

We can compute displacements along the trajectories formed by keeping either \(\alpha\) or \(\theta\) fixed and varying the other.  Those are

\begin{equation}\label{eqn:stokesTensor:620}
\begin{aligned}
\PD{\alpha}{x^i} d\alpha &= \frac{d{L^i}_k}{d\alpha} {R^k}_j x^j(0, 0) \\
\PD{\theta}{x^i} d\theta &= {L^i}_k \frac{d{R^k}_j}{d\theta} x^j(0, 0) .
\end{aligned}
\end{equation}

Writing \(y^i = x^i(0,0)\) the computation of the partials above yields

\begin{equation}\label{eqn:stokesTensor:640}
\begin{aligned}
\PD{\alpha}{x^i}
&=
\begin{bmatrix}
\sinh\alpha y^0 -\cosh\alpha (\cos\theta y^1 + \sin\theta y^2) \\
-\cosh\alpha y^0 +\sinh\alpha (\cos\theta y^1 + \sin\theta y^2) \\
0 \\
0
\end{bmatrix} \\
\PD{\theta}{x^i}
&=
\begin{bmatrix}
-\sinh\alpha (-\sin\theta y^1 + \cos\theta y^2 ) \\
\cosh\alpha (-\sin\theta y^1 + \cos\theta y^2 ) \\
-(\cos\theta y^1 + \sin\theta y^2 ) \\
0
\end{bmatrix}
\end{aligned}
\end{equation}

Different choices of the initial point \(y^i\) yield different surfaces, but we can get the idea by picking a simple starting point \(y^i = (0, 1, 0, 0)\) leaving

\begin{equation}\label{eqn:stokesTensor:660}
\begin{aligned}
\PD{\alpha}{x^i}
&=
\begin{bmatrix}
-\cosh\alpha \cos\theta  \\
\sinh\alpha \cos\theta  \\
0 \\
0
\end{bmatrix} \\
\PD{\theta}{x^i}
&=
\begin{bmatrix}
\sinh\alpha \sin\theta  \\
-\cosh\alpha \sin\theta  \\
-\cos\theta  \\
0
\end{bmatrix}.
\end{aligned}
\end{equation}

We can now compute our Jacobian determinants

\begin{equation}\label{eqn:stokesTensor:680}
\PD{\alpha}{x^{[a}} \PD{\theta}{x^{b]}}
=
\Abs{\frac{\partial(x^a, x^b)}{\partial(\alpha, \theta)}}.
\end{equation}

Those are
\begin{equation}\label{eqn:stokesTensor:700}
\begin{aligned}
\Abs{\frac{\partial(x^0, x^1)}{\partial(\alpha, \theta)}} &= \cos\theta \sin\theta \\
\Abs{\frac{\partial(x^0, x^2)}{\partial(\alpha, \theta)}} &= \cosh\alpha \cos^2\theta \\
\Abs{\frac{\partial(x^0, x^3)}{\partial(\alpha, \theta)}} &= 0 \\
\Abs{\frac{\partial(x^1, x^2)}{\partial(\alpha, \theta)}} &= -\sinh\alpha \cos^2\theta \\
\Abs{\frac{\partial(x^1, x^3)}{\partial(\alpha, \theta)}} &= 0 \\
\Abs{\frac{\partial(x^2, x^3)}{\partial(\alpha, \theta)}} &= 0
\end{aligned}
\end{equation}

Using this, let us see a specific 4D example in spacetime for the integral of the curl of some four vector \(A^i\), enumerating all the non-zero components of \eqnref{eqn:stokesTensor:470g} for this particular spacetime surface

\begin{equation}\label{eqn:stokesTensor:720}
\sum_{a < b}
\int
d{\alpha} d{\theta}
\Abs{\frac{\partial(x^a, x^b)}{\partial(\alpha, \theta)}}
\left( \partial_a A_{b}
-\partial_b A_{a} \right)
=
%\int
%\evalbar{A_b}{\partial \alpha}
%\PD{\theta}{x^{b}} d{\theta}
%-
%\evalbar{A_b}{\partial \theta}
%\PD{\alpha}{x^{b}} d{\alpha}.
\int d\theta
\int d\alpha
\PD{\alpha}{A_b}
\PD{\theta}{x^{b}}
-
\int d\theta
\int d\alpha
\PD{\theta}{A_b}
\PD{\alpha}{x^{b}}
\end{equation}

The LHS is thus found to be

\begin{equation}\label{eqn:stokesTensor:1060}
\begin{aligned}
&\int
d{\alpha} d{\theta}
\left(
\Abs{\frac{\partial(x^0, x^1)}{\partial(\alpha, \theta)}} \left( \partial_0 A_{1} -\partial_1 A_{0} \right)
+\Abs{\frac{\partial(x^0, x^2)}{\partial(\alpha, \theta)}} \left( \partial_0 A_{2} -\partial_2 A_{0} \right)
+\Abs{\frac{\partial(x^1, x^2)}{\partial(\alpha, \theta)}} \left( \partial_1 A_{2} -\partial_2 A_{1} \right)
\right) \\
&=
\int
d{\alpha} d{\theta}
\left(
\cos\theta \sin\theta \left( \partial_0 A_{1} -\partial_1 A_{0} \right)
+\cosh\alpha \cos^2\theta \left( \partial_0 A_{2} -\partial_2 A_{0} \right)
-\sinh\alpha \cos^2\theta \left( \partial_1 A_{2} -\partial_2 A_{1} \right)
\right)
\end{aligned}
\end{equation}

On the RHS we have

\begin{equation}\label{eqn:stokesTensor:1080}
\begin{aligned}
\int d\theta
\int d\alpha
&\PD{\alpha}{A_b}
\PD{\theta}{x^{b}}
-
\int d\theta
\int d\alpha
\PD{\theta}{A_b}
\PD{\alpha}{x^{b}} \\
&=
\int d\theta
\int d\alpha
\begin{bmatrix}
\sinh\alpha \sin\theta &
-\cosh\alpha \sin\theta &
-\cos\theta &
0
\end{bmatrix}
\PD{\alpha}{}
\begin{bmatrix}
A_0 \\
A_1 \\
A_2 \\
A_3 \\
\end{bmatrix} \\
&-
\int d\theta
\int d\alpha
\begin{bmatrix}
-\cosh\alpha \cos\theta & \sinh\alpha \cos\theta & 0 & 0
\end{bmatrix}
\PD{\theta}{}
\begin{bmatrix}
A_0 \\
A_1 \\
A_2 \\
A_3 \\
\end{bmatrix} \\
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:stokesTensor:740}
\begin{aligned}
&\int d{\alpha} d{\theta}
\cos\theta \sin\theta \left( \partial_0 A_{1} -\partial_1 A_{0} \right) \\
&\qquad+
\int d{\alpha} d{\theta}
\cosh\alpha \cos^2\theta \left( \partial_0 A_{2} -\partial_2 A_{0} \right) \\
&\qquad-
\int d{\alpha} d{\theta}
\sinh\alpha \cos^2\theta \left( \partial_1 A_{2} -\partial_2 A_{1} \right) \\
&=
\int d\theta \sin\theta \int d\alpha \left( \sinh\alpha \PD{\alpha}{A_0} - \cosh\alpha \PD{\alpha}{A_1} \right) \\
&\qquad-\int d\theta \cos\theta \int d\alpha \PD{\alpha}{A_2} \\
&\qquad+\int d\alpha \cosh\alpha \int d\theta \cos\theta \PD{\theta}{A_0} \\
&\qquad-\int d\alpha \sinh\alpha \int d\theta \cos\theta \PD{\theta}{A_1}
\end{aligned}
\end{equation}

Because of the complexity of the surface, only the second term on the RHS has the ``evaluate on the boundary'' characteristic that may have been expected from a Green's theorem like line integral.

It is also worthwhile to point out that we have had to be very careful with upper and lower indices all along (and have done so with the expectation that our application would include the special relativity case where our metric determinant is minus one.)  Because we worked with upper indices for the area element, we had to work with lower indices for the four vector and the components of the gradient that we included in our curl evaluation.

\subsection{The rank 2 tensor case}

Let us consider briefly the terms in the contraction sum

\begin{equation}\label{eqn:stokesTensor:800}
\Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_a A_{bc}
\end{equation}

For any choice of a set of three distinct indices \((a, b, c) \in \{(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)\}\), we have \(6 = 3!\) ways of permuting those indices in this sum

\begin{equation}\label{eqn:stokesTensor:1100}
\begin{aligned}
\Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_a A_{bc}
&=
\sum_{a < b < c}
  \Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_a A_{bc}
 + \Abs{ \frac{\partial(x^a, x^c, x^b)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_a A_{cb}
 + \Abs{ \frac{\partial(x^b, x^c, x^a)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_b A_{ca} \\
&\qquad + \Abs{ \frac{\partial(x^b, x^a, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_b A_{ac}
 + \Abs{ \frac{\partial(x^c, x^a, x^b)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_c A_{ab}
 + \Abs{ \frac{\partial(x^c, x^b, x^a)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_c A_{ba} \\
&=
2!
\sum_{a < b < c}
\Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} }
\left( \partial_a A_{bc} + \partial_b A_{c a} + \partial_c A_{a b} \right)
\end{aligned}
\end{equation}

Observe that we have no sign alternation like we had in the vector (rank 1 tensor) case.  That sign alternation in this summation expansion appears to occur only for odd grade tensors.

Returning to the problem, we wish to expand the determinant in order to apply a chain rule contraction as done in the rank-1 case.  This can be done along any of rows or columns of the determinant, and we can write any of

\begin{equation}\label{eqn:stokesTensor:1120}
\begin{aligned}
\Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} }
&=
\PD{\alpha_1}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_2)} }  \\
&=
\PD{\alpha_1}{x^b} \Abs{ \frac{\partial(x^c, x^a)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{x^b} \Abs{ \frac{\partial(x^c, x^a)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{x^b} \Abs{ \frac{\partial(x^c, x^a)}{\partial(\alpha_1, \alpha_2)} }  \\
&=
\PD{\alpha_1}{x^c} \Abs{ \frac{\partial(x^a, x^b)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{x^c} \Abs{ \frac{\partial(x^a, x^b)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{x^c} \Abs{ \frac{\partial(x^a, x^b)}{\partial(\alpha_1, \alpha_2)} }  \\
\end{aligned}
\end{equation}

This allows the contraction of the index \(a\), eliminating it from the result
\begin{equation}\label{eqn:stokesTensor:1140}
\begin{aligned}
\Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \partial_a A_{bc}
&=
\left( \PD{\alpha_1}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{x^a} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_2)} }  \right) \PD{x^a}{A_{bc}} \\
&=
\PD{\alpha_1}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_2)} }   \\
&=
2!
\sum_{b < c}
\PD{\alpha_1}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_2, \alpha_3)} }
-
\PD{\alpha_2}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_3)} }
+
\PD{\alpha_3}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_2)} }   \\
\end{aligned}
\end{equation}

Dividing out the common \(2!\) terms, we can summarize this result as

\boxedEquation{eqn:stokesTensor:820}{
\begin{aligned}
\sum_{a < b < c}
&\int d\alpha_1 d\alpha_2 d\alpha_3 \Abs{ \frac{\partial(x^a, x^b, x^c)}{\partial(\alpha_1, \alpha_2, \alpha_3)} }
\left( \partial_a A_{bc} + \partial_b A_{c a} + \partial_c A_{a b} \right) \\
&=
\sum_{b < c}
\int d\alpha_2 d\alpha_3 \int d\alpha_1
\PD{\alpha_1}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_2, \alpha_3)} } \\
&-
\sum_{b < c}
\int d\alpha_1 d\alpha_3 \int d\alpha_2
\PD{\alpha_2}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_3)} } \\
&+
\sum_{b < c}
\int d\alpha_1 d\alpha_2 \int d\alpha_3
\PD{\alpha_3}{A_{bc}} \Abs{ \frac{\partial(x^b, x^c)}{\partial(\alpha_1, \alpha_2)} }
\end{aligned}
}

In general, as observed in the spacetime surface example above, the two index Jacobians can be functions of the integration variable first being eliminated.  In the special cases where this is not the case (such as the \R{3} case with rectangular coordinates), then we are left with just the evaluation of the tensor element \(A_{bc}\) on the boundaries of the respective integrals.

\subsection{The rank 3 tensor case}

The key step is once again just a determinant expansion

\begin{equation}\label{eqn:stokesTensor:1160}
\begin{aligned}
&\Abs{ \frac{\partial(x^a, x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \\
&=
\PD{\alpha_1}{x^a} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_2, \alpha_3, \alpha_4)} }
-
\PD{\alpha_2}{x^a} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_3, \alpha_4)} }
+
\PD{\alpha_3}{x^a} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_4)} }
+
\PD{\alpha_4}{x^a} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3)} }
\\
\end{aligned}
\end{equation}

so that the sum can be reduced from a four index contraction to a 3 index contraction

\begin{equation}\label{eqn:stokesTensor:1180}
\begin{aligned}
&\Abs{ \frac{\partial(x^a, x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \partial_a A_{bcd} \\
&=
\PD{\alpha_1}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_2, \alpha_3, \alpha_4)} }
-
\PD{\alpha_2}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_3, \alpha_4)} }
+
\PD{\alpha_3}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_4)} }
+
\PD{\alpha_4}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3)} }
\end{aligned}
\end{equation}

That is the essence of the theorem, but we can play the same combinatorial reduction games to reduce the built in redundancy in the result

\boxedEquation{eqn:stokesTensor:840}{
\begin{aligned}
\inv{3!} &\int d^4 \alpha
\Abs{ \frac{\partial(x^a, x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \partial_a A_{bcd} \\
&=
\sum_{a < b < c < d}
\int d^4 \alpha
\Abs{ \frac{\partial(x^a, x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} }
\left( \partial_a A_{bcd}
-\partial_b A_{cda}
+\partial_c A_{dab}
-\partial_d A_{abc} \right) \\
&=
\qquad \sum_{b < c < d}\int d\alpha_2 d\alpha_3 d\alpha_4 \int d\alpha_1
\PD{\alpha_1}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_2, \alpha_3, \alpha_4)} } \\
&\qquad -
\sum_{b < c < d}\int d\alpha_1 d\alpha_3 d\alpha_4 \int d\alpha_2
\PD{\alpha_2}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_3, \alpha_4)} } \\
&\qquad +
\sum_{b < c < d}\int d\alpha_1 d\alpha_2 d\alpha_4 \int d\alpha_3
\PD{\alpha_3}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_4)} } \\
&\qquad +
\sum_{b < c < d}\int d\alpha_1 d\alpha_2 d\alpha_3 \int d\alpha_4
\PD{\alpha_4}{A_{bcd}} \Abs{ \frac{\partial(x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3)} } \\
\end{aligned}
}

\subsection{A note on Four diverence}

Our four divergence integral has the following form

\begin{equation}\label{eqn:stokesTensor:860}
\int d^4 \alpha
\Abs{ \frac{\partial(x^1, x^2, x^2, x^4)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \partial_a A^a
\end{equation}

We can relate this to the rank 3 Stokes theorem with a duality transformation, multiplying with a pseudoscalar

\begin{equation}\label{eqn:stokesTensor:880}
A^a = \epsilon^{abcd} T_{bcd},
\end{equation}

where \(T_{bcd}\) can also be related back to the vector by the same sort of duality transformation

\begin{equation}\label{eqn:stokesTensor:900}
A^a \epsilon_{a b c d} = \epsilon^{abcd} \epsilon_{a b c d} T_{bcd} = 4! T_{bcd}.
\end{equation}

The divergence integral in terms of the rank 3 tensor is

\begin{equation}\label{eqn:stokesTensor:920}
\int d^4 \alpha
\Abs{ \frac{\partial(x^1, x^2, x^2, x^4)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \partial_a \epsilon^{abcd} T_{bcd}
=
\int d^4 \alpha
\Abs{ \frac{\partial(x^a, x^b, x^c, x^d)}{\partial(\alpha_1, \alpha_2, \alpha_3, \alpha_4)} } \partial_a T_{bcd},
\end{equation}

and we are free to perform the same Stokes reduction of the integral.  Of course, this is particularly simple in rectangular coordinates.  I still have to think though one sublty that I feel may be important.  We could have started off with an integral of the following form

\begin{equation}\label{eqn:stokesTensor:940}
\int dx^1 dx^2 dx^3 dx^4 \partial_a A^a,
\end{equation}

and I think this differs from our starting point slightly because this has none of the antisymmetric structure of the signed 4 volume element that we have used.  We do not take the absolute value of our Jacobians anywhere.

%\EndArticle
%\EndNoBibArticle
