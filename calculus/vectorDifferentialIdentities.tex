%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Vector Differential Identities}
\index{vector identities}
\label{chap:vectorDifferentialIdentities}
%\date{Jan 05, 2009.  vectorDifferentialIdentities.tex}

\citep{feynman1963flp} electrodynamics \textchapref{II} lists a number of
differential vector identities.

\begin{enumerate}
\item \(\spacegrad \cdot (\spacegrad T) = \spacegrad^2 T = \mbox{a scalar field}\)
\item \(\spacegrad \cross (\spacegrad T) = 0\)
\item \(\spacegrad (\spacegrad \cdot \Bh) = \mbox{a vector field}\)
\item \(\spacegrad \cdot (\spacegrad \cross \Bh) = 0\)
\item \(\spacegrad \cross (\spacegrad \cross \Bh) = \spacegrad(\spacegrad \cdot \Bh) - \spacegrad^2 \Bh\)
\item \((\spacegrad \cdot \spacegrad) \Bh = \mbox{a vector field}\)
\end{enumerate}

Let us see how all these translate to GA form.

\section{Divergence of a gradient}
\index{gradient!divergence}

This one has the same form, but expanding it can be evaluated by grade
selection

\begin{equation}\label{eqn:vectorDifferentialIdentities:20}
\begin{aligned}
\spacegrad \cdot (\spacegrad T)
&= \gpgradezero{\spacegrad \spacegrad T} \\
&= (\spacegrad^2) T
\end{aligned}
\end{equation}

A less sneaky expansion would be by coordinates

\begin{equation}\label{eqn:vectorDifferentialIdentities:40}
\begin{aligned}
\spacegrad \cdot (\spacegrad T)
&= {\sum_{k,j} (\sigma_k \partial_k) \cdot (\sigma_j \partial_j T)} \\
&= \gpgradezero{\sum_{k,j} (\sigma_k \partial_k) (\sigma_j \partial_j T)} \\
&= \gpgradezero{\left(\sum_{k,j} \sigma_k \partial_k \sigma_j \partial_j\right) T} \\
%&= {\left(\sum_{k,j} \sigma_k \partial_k \sigma_j \partial_j\right) } T \\
&= \gpgradezero{\spacegrad^2 T} \\
&= \spacegrad^2 T \\
\end{aligned}
\end{equation}

\section{Curl of a gradient is zero}
\index{gradient!curl}

The duality analogue of this is
\begin{equation}\label{eqn:vectorDifferentialIdentities:60}
\begin{aligned}
\spacegrad \cross (\spacegrad T) = -i(\spacegrad \wedge (\spacegrad T))
\end{aligned}
\end{equation}

Let us verify that this bivector curl is zero.  This can also be done by grade selection

\begin{equation}\label{eqn:vectorDifferentialIdentities:80}
\begin{aligned}
\spacegrad \wedge (\spacegrad T)
&= \gpgradetwo{\spacegrad (\spacegrad T)} \\
&= \gpgradetwo{(\spacegrad \spacegrad) T} \\
&= (\spacegrad \wedge \spacegrad) T \\
&= 0
\end{aligned}
\end{equation}

Again, this is sneaky and side steps the continuity requirement for mixed partial equality.  Again by coordinates is better
\begin{equation}\label{eqn:vectorDifferentialIdentities:100}
\begin{aligned}
\spacegrad \wedge (\spacegrad T)
&= \gpgradetwo{\sum_{k,j}\sigma_k \partial_k (\sigma_j \partial_j T)} \\
&= \gpgradetwo{\sum_{k<j}\sigma_k \sigma_j (\partial_k \partial_j - \partial_j \partial_k) T} \\
&= \sum_{k<j} \sigma_k \wedge \sigma_j (\partial_k \partial_j - \partial_j \partial_k) T \\
\end{aligned}
\end{equation}

So provided the mixed partials are zero the curl of a gradient is zero.

\section{Gradient of a divergence}
\index{divergence!gradient}

Nothing more to say about this one.

\section{Divergence of curl}
\index{curl!divergence}

This one looks like it will have a dual form using bivectors.

\begin{equation}\label{eqn:vectorDifferentialIdentities:120}
\begin{aligned}
\spacegrad \cdot (\spacegrad \cross \Bh)
&= \spacegrad \cdot (-i (\spacegrad \wedge \Bh)) \\
&= \gpgradezero{\spacegrad (-i (\spacegrad \wedge \Bh))} \\
&= \gpgradezero{-i \spacegrad (\spacegrad \wedge \Bh)} \\
&= -(i \spacegrad) \cdot (\spacegrad \wedge \Bh) \\
\end{aligned}
\end{equation}

Is this any better than the cross product relationship?

I do not really think so.  They both say the same thing, and only possible value to this duality form is if more than three dimensions are required (in which case the sign of the pseudoscalar \(i\) has to be dealt with more carefully).  Geometrically one has the dual of the gradient (a plane normal to the vector itself) dotted with the plane formed by the gradient and the vector operated on.  The corresponding statement for the cross product form is that we have a dot product of a vector with a vector normal to it, so also intuitively expect a zero.  In either case, because we are talking about operators here
just saying this is zero because of geometrical arguments is not necessarily convincing.  Let us evaluate this explicitly in
coordinates to verify

\begin{equation}\label{eqn:vectorDifferentialIdentities:140}
\begin{aligned}
(i \spacegrad) \cdot (\spacegrad \wedge \Bh)
&= \gpgradezero{i \spacegrad (\spacegrad \wedge \Bh) } \\
&= \gpgradezero{i \sum_{k,j,l} \sigma_k \partial_k \left((\sigma_j \wedge \sigma_l) \partial_j h^l\right) } \\
&= -i \sum_{l} \sigma_l \wedge \left( \sum_{k<j} (\sigma_k \wedge \sigma_j) (\partial_k \partial_j -\partial_j \partial_k) h^l \right) \\
\end{aligned}
\end{equation}

This inner quantity is zero, again by equality of mixed partials.  While the dual form of this identity was not really any better than the cross
product form, there is nothing in this zero equality proof that was tied to the dimension of the vectors involved, so we do have a more general form
than can be expressed by the cross product, which could be of value in Minkowski space later.

\section{Curl of a curl}
\index{curl!curl}

This will also have a dual form.  That is

\begin{equation}\label{eqn:vectorDifferentialIdentities:160}
\begin{aligned}
\spacegrad \cross (\spacegrad \cross \Bh)
&= -i (\spacegrad \wedge (\spacegrad \cross \Bh)) \\
&= -i (\spacegrad \wedge (-i (\spacegrad \wedge \Bh))) \\
&= -i \gpgradetwo{\spacegrad (-i (\spacegrad \wedge \Bh))} \\
&= i \gpgradetwo{i \spacegrad (\spacegrad \wedge \Bh)} \\
&= i^2 \spacegrad \cdot (\spacegrad \wedge \Bh) \\
&= - \spacegrad \cdot (\spacegrad \wedge \Bh) \\
\end{aligned}
\end{equation}

Now, let us expand this quantity
\begin{equation}\label{eqn:vectorDifferentialIdentities:180}
\begin{aligned}
\spacegrad \cdot (\spacegrad \wedge \Bh)
\end{aligned}
\end{equation}

If the gradient could be treated as a plain old vector we could just do
\begin{equation}\label{eqn:vectorDifferentialIdentities:200}
\begin{aligned}
\Ba \cdot (\Ba \wedge \Bh) &= \Ba^2 \Bh - \Ba(\Ba \cdot \Bh) \\
\end{aligned}
\end{equation}

With the gradient substituted this is exactly the desired identity (with the expected sign difference)

\begin{equation}\label{eqn:vectorDifferentialIdentities:220}
\begin{aligned}
\spacegrad \cdot (\spacegrad \wedge \Bh) &= \spacegrad^2 \Bh - \spacegrad(\spacegrad \cdot \Bh) \\
\end{aligned}
\end{equation}

A coordinate expansion to truly verify that this is valid is logically still required, but having done the others above, it is clear how this
would work out.

\section{Laplacian of a vector}
\index{vector!Laplacian}

This one is not interesting seeming.

\section{Zero curl implies gradient solution}

This theorems is mentioned in the text without proof.

Theorem was

\begin{equation*}
\begin{array}{l l l}
\text{If} &          \spacegrad \cross \BA &= 0 \\
\text{there is a } &                  \psi &    \\
\text{such that  } & \BA &= \spacegrad \psi \\
\end{array}
\end{equation*}

This appears to be half of an if and only if theorem.  The unstated part is if one has a gradient then the curl is zero

\begin{equation}\label{eqn:vectorDifferentialIdentities:240}
\begin{aligned}
\BA = \spacegrad \psi \\
\implies \\
\spacegrad \cross \BA &= \spacegrad \cross \spacegrad \psi = 0
\end{aligned}
\end{equation}

This last was proven above, and follows from the assumed mixed partial equality.  Now, the real problem here is to find \(\psi\) given \(\BA\).
First note that we can remove the three dimensionality of the theorem by duality writing \(\spacegrad \cross \BA = -i (\spacegrad \wedge \BA)\).
In one sense changing the theorem to use the wedge instead of cross makes the problem harder since the wedge product is defined not just
for \R{3}.  However, this also allows starting with the simpler \R{2} case, so let us do that one first.

Write

\begin{equation}\label{eqn:vecDiffIdent:aDefined}
\begin{aligned}
\BA = \sigma^1 A_1 + \sigma^2 A_2 = \sigma^1 (\partial_1 \psi) + \sigma^2 (\partial_2 \psi)
\end{aligned}
\end{equation}

The gradient is
\begin{equation}\label{eqn:vectorDifferentialIdentities:260}
\begin{aligned}
\spacegrad = \sigma^1 \partial_1 + \sigma^2 \partial_2
\end{aligned}
\end{equation}

Our curl is then

\begin{equation}\label{eqn:vectorDifferentialIdentities:280}
\begin{aligned}
(\sigma^1 \partial_1 + \sigma^2 \partial_2) \wedge (\sigma^1 A_1 + \sigma^2 A_2)
&=
(\sigma^1 \wedge \sigma^2) (\partial_1 A_2 - \partial_2 A_1)
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:vecDiffIdent:curlZero}
\begin{aligned}
\partial_1 A_2 = \partial_2 A_1
\end{aligned}
\end{equation}

Now from \eqnref{eqn:vecDiffIdent:aDefined} this means we must have

\begin{equation}\label{eqn:vectorDifferentialIdentities:300}
\begin{aligned}
\partial_1 \partial_2 \psi = \partial_2 \partial_1 \psi
\end{aligned}
\end{equation}

This is just a statement of mixed partial equality, and does not look particularly useful for solving for \(\psi\).  It really shows that the
is redundancy in the problem, and instead of substituting for both of \(A_1\) and \(A_2\)
in \eqnref{eqn:vecDiffIdent:curlZero}, we can use one or the other.

Doing so we have two equations, either of which we can solve for
\begin{equation}\label{eqn:vectorDifferentialIdentities:320}
\begin{aligned}
\partial_2 \partial_1 \psi &= \partial_1 A_2 \\
\partial_1 \partial_2 \psi &= \partial_2 A_1
\end{aligned}
\end{equation}

Integrating once gives
\begin{equation}\label{eqn:vectorDifferentialIdentities:340}
\begin{aligned}
\partial_1 \psi &= \int \partial_1 A_2 dy + B(x) \\
\partial_2 \psi &= \int \partial_2 A_1 dx + C(y)
\end{aligned}
\end{equation}

And a second time produces solutions for \(\psi\) in terms of the vector coordinates
\begin{equation}\label{eqn:vecDiffIdent:twoIntegrations}
\begin{aligned}
\psi &= \iint \PD{x}{A_2} dy dx + \int B(x) dx + D(y) \\
\psi &= \iint \PD{y}{A_1} dx dy + \int C(y) dy + E(x)
\end{aligned}
\end{equation}

Is there a natural way to merge these so that \(\psi\) can be expressed more symmetrically in terms of both coordinates?
Looking at \eqnref{eqn:vecDiffIdent:twoIntegrations} I am led to guess that its possible to
combine these into a single equation expressing \(\psi\) in terms of both \(A_1\) and \(A_2\).  One way to do so is perhaps just to average the
two as in

\begin{equation}\label{eqn:vectorDifferentialIdentities:360}
\begin{aligned}
\psi &= \alpha \iint \PD{x}{A_2} dy dx + (1-\alpha) \iint \PD{y}{A_1} dx dy + \int C(y) dy + E(x) + \int B(x) dx + D(y)
\end{aligned}
\end{equation}

But that seems pretty arbitrary.  Perhaps that is the point?

FIXME: work some examples.

%above it looks like these will combine naturally via Green's theorem which introduces a difference in
%sign intrinsically related to the curl.  That is been explored in \chapcite{PJStokes1}, and \chapcite{PJStokes2}, and intuitively one expects that
%a complexification of the area element will produce this result.

%Lets get rid of
%the integration constant functions to start with, pulling them into the integrals.  Starting over this way, the first integration gives us

%\begin{align*}
%\PD{x}{ \psi(x,y)} &= \int_{v = b_1(x)}^{b_2(x)} \PD{x}{A_2(x,v)} dv \\
%\PD{y}{ \psi(x,y)} &= \int_{u = c_1(y)}^{c_2(y)} \PD{y}{A_1(u,y)} du
%\end{align*}
%
%FIXME: second integration gives?
%\begin{align*}
%\psi(x,y) &= \int_{u=d_1(x,y)}^{d_2(x,y)} \left(\int_{v = b_1(u)}^{b_2(u)} \PD{u}{A_2(u,v)} dv \right) du \\
%\psi(x,y) &= \int_{v=e_1(x,y)}^{e_2(x,y)} \left(\int_{u = c_1(v)}^{c_2(v)} \PD{v}{A_1(u,v)} du \right) dv
%\end{align*}
%
%... this does not seem right.  Think it through better.
%

FIXME: look at more than the \R{2} case.

\section{Zero divergence implies curl solution}

This theorems is mentioned in the text without proof.

Theorem was

\begin{equation*}
\begin{array}{l l l}
\text{If} &                 \spacegrad \cdot \BD &= 0 \\
\text{there is a } &                         \BC &    \\
\text{such that  } & \BD &= \spacegrad \cross \BC \\
\end{array}
\end{equation*}

As above, if \(\BD = \spacegrad \cross \BC\), then we have

\begin{equation}\label{eqn:vectorDifferentialIdentities:380}
\begin{aligned}
\spacegrad \cdot \BD &= \spacegrad \cdot (\spacegrad \cross \BC)
\end{aligned}
\end{equation}

and this has already been shown to be zero.  So the problem becomes find \(\BC\) given \(\BD\).

Also, as before an equivalent generalized (or de-generalized) problem can be expressed.

That is
\begin{equation}\label{eqn:vectorDifferentialIdentities:400}
\begin{aligned}
\spacegrad \cdot \BD
&= \gpgradezero{\spacegrad \BD} \\
&= \gpgradezero{\spacegrad (\spacegrad \cross \BC)} \\
&= \gpgradezero{\spacegrad -i (\spacegrad \wedge \BC)} \\
&= -\gpgradezero{i\spacegrad \cdot (\spacegrad \wedge \BC)} -\gpgradezero{i\spacegrad \wedge (\spacegrad \wedge \BC)} \\
&= -\gpgradezero{i\spacegrad \cdot (\spacegrad \wedge \BC)} \\
\end{aligned}
\end{equation}

So if \(\spacegrad \cdot \BD\) it is also true that \(\spacegrad \cdot (\spacegrad \wedge \BC) = 0\)

Thus the (de)generalized theorem to prove is

\begin{equation*}
\begin{array}{l l l}
\text{If} &                 \spacegrad \cdot D &= 0 \\
\text{there is a } &                       C &    \\
\text{such that  } & D &= \spacegrad \wedge C \\
\end{array}
\end{equation*}

In the \R{3} case, to prove the original theorem we want a bivector \(D = -i\BD\), and seek a vector \(C\) such that
\(D = \spacegrad \wedge C\) (\(\BD = -i (\spacegrad \wedge C)\)).

\begin{equation}\label{eqn:vectorDifferentialIdentities:420}
\begin{aligned}
\spacegrad \cdot D
&= \spacegrad \cdot (\spacegrad \wedge C) \\
&= (\sigma^k \partial_k) \cdot (\sigma^i \wedge \sigma^j \partial_i C_j) \\
&= \sigma^k \cdot (\sigma^i \wedge \sigma^j) \partial_k \partial_i C_j \\
&= ( \sigma^j \delta^{ki} - \sigma^i \delta^{kj} ) \partial_k \partial_i C_j \\
&= \sigma^j \partial_i \partial_i C_j - \sigma^i \partial_j \partial_i C_j \\
&= \sigma^j \partial_i (\partial_i C_j -\partial_j C_i) \\
\end{aligned}
\end{equation}

If this is to equal zero we must have the following constraint on C
\begin{equation}\label{eqn:vecDiffIdent:blah}
\begin{aligned}
\partial_{ii} C_j = \partial_{ij} C_i
\end{aligned}
\end{equation}

If the following equality was also true
\begin{equation}\label{eqn:vectorDifferentialIdentities:440}
\begin{aligned}
\partial_{i} C_j = \partial_j C_i
\end{aligned}
\end{equation}

Then this would also work, but would also mean \(D\) equals zero so that is not an interesting solution.  So, we must go back to
\eqnref{eqn:vecDiffIdent:blah} and solve for \(C_k\) in terms of \(D\).

Suppose we have D explicitly in terms of coordinates

\begin{equation}\label{eqn:vectorDifferentialIdentities:460}
\begin{aligned}
D
&= D_{ij} \sigma^i \wedge \sigma^j \\
&= \sum_{i<j} (D_{ij} -D_{ji})\sigma^i \wedge \sigma^j
\end{aligned}
\end{equation}

compare this to \(\spacegrad \wedge C\)

\begin{equation}\label{eqn:vectorDifferentialIdentities:480}
\begin{aligned}
C &= (\partial_i C_j ) \sigma^i \wedge \sigma^j \\
  &= \sum_{i<j} (\partial_i C_j -\partial_j C_i) \sigma^i \wedge \sigma^j
\end{aligned}
\end{equation}

With the identity
\begin{equation}\label{eqn:vectorDifferentialIdentities:500}
\begin{aligned}
\partial_i C_j = D_ij
\end{aligned}
\end{equation}

\Eqnref{eqn:vecDiffIdent:blah} becomes
\begin{equation}\label{eqn:vectorDifferentialIdentities:520}
\begin{aligned}
\partial_{ij} C_i &= \partial_{i} D_{ij}  \\
\implies \\
\partial_{j} C_i &= D_{ij} + \alpha_{ij}(x^{k \ne i})
\end{aligned}
\end{equation}

Where \(\alpha_{ij}(x^{k \ne i})\) is some function of all the \(x^k \ne x^i\).

Integrating once more we have
\begin{equation}\label{eqn:vectorDifferentialIdentities:540}
\begin{aligned}
C_i &= \int \left(D_{ij} + \alpha_{ij}(x^{k \ne i}) \right) dx^j + \beta_{ij}(x^{k \ne j})
\end{aligned}
\end{equation}
