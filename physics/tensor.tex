%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Gradient and tensor notes}
\index{gradient}
\index{tensor}
\label{chap:tensor}
%\date{June 6, 2008.  tensor.tex}

\section{Motivation}

Some notes on tensors filling in assumed details covered in
\citep{doran2003gap}.

%in response to having to clarify
%, mostly from Geometric Algebra for Physicists (GAFP).  Write up enough notes for myself that I can understand the topics, filling in details
%omitted from GAFP.  Goal for this is to ensure I can explain the ideas to myself, since if I can not then I do not understand them sufficiently.

Conclude with the
solution of problem 6.1 to demonstrate the frame independence of the
vector derivative.
%covariant derivative.

Despite being notes associated with a Geometric Algebra text, there is no
GA content.  Outside of the eventual GA application of the gradient as
described in this form, the only GA connection is the fact that the
the reciprocal frame vectors can be thought of as a result of a duality
calculation.  That connection is not necessary though since one can just as
easily define the reciprocal frame in terms of matrix operations.  As an example, for a Euclidean metric the reciprocal frame vectors are the columns of
\(F(F^{\text{T}}F)^{-1}\) where the columns of \(F\) are the vectors in question.

These notes may not stand well on their own without the text, at least
as learning material.

\subsection{Raised and lowered indices. Coordinates of vectors with non-orthonormal frames}

Let \(\{ e_i \}\) represent a frame of not necessarily orthonormal basis vectors for a metric space, and \(\{ e^i \}\) represent the reciprocal frame.

The reciprocal frame vectors are defined by the relation:

\begin{equation}
e_i \cdot e^j = {\delta_i}^j.
\end{equation}

Lets compute the coordinates of a vector \(x\) in terms of both frames:

\begin{equation}\label{eqn:tensor:20}
x = \sum \alpha_j e_j = \sum \beta_j e^j
\end{equation}

Forming \(x \cdot e^i\), and \(x \cdot e_i\) respectively solves for the \(\alpha\), and \(\beta\) coefficients

\begin{equation}\label{eqn:tensor:40}
x \cdot e^i = \sum \alpha_j e_j \cdot e^i = \sum \alpha_j {\delta_j}^i = \alpha_i
\end{equation}

\begin{equation}\label{eqn:tensor:60}
x \cdot e_i = \sum \beta_j e^j \cdot e_i = \sum \beta_j {\delta_i}^j = \beta_i
\end{equation}

Thus, the reciprocal frame vectors allow for simple determination of coordinates for an arbitrary frame. We can summarize this as follows:

\begin{equation}\label{eqn:tensor:80}
x = \sum ( x \cdot e^i ) e_i = \sum ( x \cdot e_i ) e^i
\end{equation}

Now, for orthonormal frames, where \(e_i = e^i\) we are used to writing:

\begin{equation}\label{eqn:tensor:100}
x = \sum x_i e_i,
\end{equation}

however for non-orthonormal frames the convention is to mix raised and lowered indices as follows:

\begin{equation}\label{eqn:tensor:120}
x = \sum x^i e_i = \sum x_i e^i.
\end{equation}

Where, as demonstrated above these generalized coordinates have the values, \(x^i = x \cdot e^i\), and \(x_i = x \cdot e_i\).  This is a strange seeming notation at
first especially since most of linear algebra is done with always lowered (or always upper for some authors) indices.  However one quickly gets used to it, especially after seeing how powerful the reciprocal frame concept is for dealing with non-orthonormal frames.  The alternative is probably the use of matrices and their inverses to express the same vector decompositions.

\subsection{Metric tensor}
\index{metric tensor}

It is customary in tensor formulations of physics to utilize a metric tensor to express the dot product.

Compute the dot product using the coordinate vectors

\begin{equation}\label{eqn:tensor:140}
x \cdot y = \left(\sum x^i e_i \right)\left(\sum y^j e_j \right) = \sum x^i y^j \left( e_i \cdot e_j \right)
\end{equation}

\begin{equation}\label{eqn:tensor:160}
x \cdot y = \left(\sum x_i e^i \right)\left(\sum y_j e^j \right) = \sum x_i y_j \left( e^i \cdot e^j \right)
\end{equation}

Introducing second rank (symmetric) tensors for the dot product pairs \( e_i \cdot e_j = g_{ij}\), and \( g^{ij} = e^i \cdot e^j \) we have

\begin{equation}\label{eqn:tensor:180}
x \cdot y = \sum x_i y_j g^{ij} = \sum x^i y^j g_{ij} = \sum x_i y^i = \sum x^i y_i
\end{equation}

We see that the metric tensor provides a way to specify the dot product in index notation, and removes the explicit references to the original frame vectors.  Mixed indices also removes the references to the original frame vectors, but additionally eliminates the need for either of the metric tensors.

Note that it is also common to see Einstein summation convention employed, which omits the \(\sum\):

\begin{equation}\label{eqn:tensor:200}
x \cdot y = x_i y_j g^{ij} = x^i y^j g_{ij} = x^i y_i = x_i y^i
\end{equation}

Here summation over all matched upper, lower index pairs is implied.

\subsection{Metric tensor relations to coordinates}

Given a coordinate expression of a vector, we dot that with the frame vectors to observe the relation between coordinates and the metric tensor:

\begin{equation}\label{eqn:tensor:220}
x \cdot e_i = \sum x^j e_j \cdot e_i = \sum x^j g_{ij}
\end{equation}

\begin{equation}\label{eqn:tensor:240}
x \cdot e^i = \sum x_j e^j \cdot e^i = \sum x_j g^{ij}
\end{equation}

The metric tensors can therefore be used be used to express the relations between the upper and lower index coordinates:

\begin{align}
x_i &= \sum g_{ij} x^j \label{eqn:tensor:metric_upper_to_lower} \\
x^i &= \sum g^{ij} x_j \label{eqn:tensor:metric_lower_to_upper}
\end{align}

It is therefore apparent that the matrix of the index lowered metric tensor \(g_{ij}\) is the inverse of the matrix for the raised index metric tensor \(g^{ij}\).

Expressed more exactly,

\begin{equation}\label{eqn:tensor:300}
\begin{aligned}
x_i
&= \sum_{ij} g_{ij} x^j \\
&= \sum_{ijk} g_{ij} g^{jk} x_k \\
&= \sum_{ik} x_k \sum_j g_{ij} g^{jk} \\
\end{aligned}
\end{equation}

Since the left and right hand sides are equal for any \(x_i, x_k\), we have:

\begin{equation}
{\delta_{i}}^j = \sum_m g_{im} g^{mj} \\
\end{equation}

Demonstration of the inverse property required for summation on other set of indices too for completeness, but since these functions are symmetric, there
is no potential that this would have a ``left'' or ``right'' inverse type of action.

\subsection{Metric tensor as a Jacobian}
\index{Jacobian}

The relations of equations \eqnref{eqn:tensor:metric_upper_to_lower}, and \eqnref{eqn:tensor:metric_lower_to_upper} show that the metric tensor can be expressed in terms of partial derivatives:

\begin{equation}\label{eqn:tensor:320}
\begin{aligned}
\frac{\partial x_i }{\partial x^j } &= g_{ij} \\
\frac{\partial x^i }{\partial x_j } &= g^{ij}
\end{aligned}
\end{equation}

Therefore the metric tensors can also be expressed as Jacobian matrices (not Jacobian determinants) :

\begin{equation}\label{eqn:tensor:340}
\begin{aligned}
g_{ij} &= \frac{\partial (x_1, \cdots, x_n) }{\partial (x^1, \cdots, x^n) } \\
g^{ij} &= \frac{\partial (x^1, \cdots, x^n) }{\partial (x_1, \cdots, x_n) }
\end{aligned}
\end{equation}

Will this be useful in any way?

\subsection{Change of basis}
\index{change of basis}

To perform a change of basis from one non-orthonormal basis \(\{e_i\}\) to a second \(\{f_i\}\), relations between the sets of vectors
are required.  Using Greek indices for the \(f\) frame, and English for the \(e\) frame, those are:

\begin{equation}\label{eqn:tensor:360}
\begin{aligned}
e_i 		&= \sum f^{\mu} e_i \cdot f_{\mu} 	= \sum f_{\mu} e_i \cdot f^{\mu} \\
f_{\alpha} 	&= \sum e^k f_{\alpha} \cdot e_k 	= \sum e_k f_{\alpha} \cdot e^k \\
e^i 		&= \sum f^{\mu} e^i \cdot f_{\mu} 	= \sum f_{\mu} e^i \cdot f^{\mu} \\
f^{\alpha} 	&= \sum e^k f^{\alpha} \cdot e_k 	= \sum e_k f^{\alpha} \cdot e^k
\end{aligned}
\end{equation}

Following GAFP we can write the dot product terms as a second order tensors \(f\) (ie: matrix relation) :

\begin{equation}\label{eqn:tensor:380}
\begin{aligned}
e_i 		&= \sum f^{\mu} f_{i\mu}  	= \sum f_{\mu} {f_i}^{\mu} \\
f_{\alpha} 	&= \sum e^k f_{k\alpha} 	= \sum e_k {f^k}_{\alpha} \\
e^i 		&= \sum f^{\mu} {f^i}_{\mu} 	= \sum f_{\mu} f^{i \mu} \\
f^{\alpha} 	&= \sum e^k {f_k}^{\alpha}  	= \sum e_k f^{k\alpha}
\end{aligned}
\end{equation}

Note that all these various tensors are related to each other using the metric tensors for \(f\) and \(e\).  FIXME: show example.  Also note that using this notation the metric tensors \(g_{ij}\) and \(g_{\alpha\beta}\) are two completely different linear functions, and careful use of the index conventions are required to keep these straight.

\subsection{Inverse relationships}

Looking at these relations in pairs, such as

\begin{equation}\label{eqn:tensor:400}
\begin{aligned}
f_{\alpha} 	&= \sum e^k f_{k\alpha} \\
e^i 		&= \sum f_{\mu} f^{i \mu}
\end{aligned}
\end{equation}

and

\begin{equation}\label{eqn:tensor:420}
\begin{aligned}
e_i 		&= \sum f^{\mu} f_{i\mu} \\
f^{\alpha} 	&= \sum e_k f^{k\alpha}
\end{aligned}
\end{equation}

It is clear that \(f_{i\alpha}\) is the inverse of \(f^{i\alpha}\).

To be more precise
\begin{equation}\label{eqn:tensor:440}
\begin{aligned}
f_{\alpha}
&= \sum e^k f_{k\alpha} \\
&= \sum f_{\mu} f^{k \mu} f_{k\alpha} \\
\end{aligned}
\end{equation}

Thus
\begin{equation}
\sum_k f^{k \beta} f_{k\alpha} = \delta_{\alpha}^{\beta}
\end{equation}

To verify both ``left'' and ``right'' inverse properties we also need:

\begin{equation}\label{eqn:tensor:460}
\begin{aligned}
e^i
&= \sum f_{\mu} f^{i \mu}  \\
&= \sum e^k f_{k\mu} f^{i \mu} \\
\end{aligned}
\end{equation}

which shows that summation on the Greek indices also yields an inverse:

\begin{equation}
\sum_{\mu} f_{i\mu} f^{j \mu} = \delta_{i}^j
\end{equation}

There are also inverse relationships for the mixed index tensors above.  Specifically,

\begin{equation}\label{eqn:tensor:480}
\begin{aligned}
x^{\alpha}
&= \sum x^i f_i^{\alpha} \\
&= \sum x^{\beta} f_{\beta}^i f_i^{\alpha} \\
\end{aligned}
\end{equation}

Thus,
\begin{equation}
\sum_i f_{\beta}^i f_i^{\alpha} = \delta_{\beta}^{\alpha}
\end{equation}

And,
\begin{equation}\label{eqn:tensor:500}
\begin{aligned}
x^{i}
&= \sum x^{\beta} f_{\beta}^i \\
&= \sum x^{j} f_j^{\beta} f_{\beta}^i \\
\end{aligned}
\end{equation}

Thus,
\begin{equation}
\sum_{\alpha} f_j^{\alpha} f_{\alpha}^i = \delta_j^i
\end{equation}

This completely demonstrates the inverse relationship.

\subsection{Vector derivative}
\index{vector derivative}

GAFP exercise 6.1.  Show that the vector derivative:

\begin{equation}
\nabla = \sum e^i \frac{\partial}{\partial x^i}
\end{equation}

is not frame dependent.

To show this we will need to utilize the chain rule to rewrite the partials in terms of the alternate frame:

\begin{equation}\label{eqn:tensor:520}
\begin{aligned}
\frac{\partial}{\partial x^i} &= \sum \frac{\partial x^\alpha}{\partial x^i} \frac{\partial}{\partial x^\alpha}
\end{aligned}
\end{equation}

To evaluate the first partial here, we write the coordinates of a vector in terms of both, and take dot products:

\begin{equation}\label{eqn:tensor:540}
\begin{aligned}
\left(\sum x^{\gamma} f_{\gamma}\right) \cdot f^{\alpha} = \left(\sum x^i e_i\right) \cdot f^{\alpha} \\
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:tensor:560}
\begin{aligned}
x^{\alpha} &= \sum x^i {f_i}^{\alpha} \\
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:tensor:580}
\begin{aligned}
\frac{\partial x^{\alpha}}{\partial x^i} &= {f_i}^{\alpha}
\end{aligned}
\end{equation}

Similar expressions for the other change of basis tensors is also possible, but
not required for this problem.

With this result we have the partial re-expressed in terms of coordinates
in the new frame.

\begin{equation}\label{eqn:tensor:600}
\begin{aligned}
\frac{\partial}{\partial x^i} &= \sum {f_i}^{\alpha} \frac{\partial}{\partial x^\alpha}
\end{aligned}
\end{equation}

Combine this with the alternate contra-variant frame vector as calculated above:

\begin{equation}\label{eqn:tensor:260}
e^i = \sum f^{\mu} {f^i}_{\mu}
\end{equation}

and we have:

\begin{equation}\label{eqn:tensor:620}
\begin{aligned}
\sum_i e^i \frac{\partial}{\partial x^i}
&= \sum_i \left(\sum_{\mu} f^{\mu} {f^i}_{\mu} \right) \left( \sum_{\alpha} {f_i}^{\alpha} \frac{\partial}{\partial x^\alpha}\right) \\
&= \sum_{\mu \alpha} \left(f^{\mu} \frac{\partial}{\partial x^\alpha} \right) \sum_i {f^i}_{\mu} {f_i}^{\alpha} \\
&= \sum_{\mu \alpha} \left(f^{\mu} \frac{\partial}{\partial x^\alpha} \right) {\delta_{\mu}}^{\alpha} \\
&= \sum_{\alpha} f^{\alpha} \frac{\partial}{\partial x^\alpha} \\
\end{aligned}
\end{equation}

%Note that my original paper derivation of the above used only the tensors \(f_{i\alpha}\), and \(f^{i\alpha}\) instead of the mixed index versions used here.  That worked but also required a pair of metric tensors, and one more step to sum over those tensors to get at the final result.

\subsection{Why a preference for index upper vector and coordinates in the gradient?}

We can express the gradient in terms of index lower variables and vectors too, as follows:

\begin{equation*}
\frac{\partial}{\partial x^i} = \sum \frac{\partial x_j}{\partial x^i} \frac{\partial}{\partial x_j}
\end{equation*}

Employing the coordinate relations we have:
\begin{equation}\label{eqn:tensor:640}
\begin{aligned}
\sum x_i e^i \cdot e_j = x_j = \sum x^i e_i \cdot e_j = \sum x^i g_{ij}
\end{aligned}
\end{equation}

and can thus calculate the partials:
\begin{equation}\label{eqn:tensor:660}
\begin{aligned}
\frac{\partial x_j}{\partial x^i} = g_{ij},
\end{aligned}
\end{equation}

and can use that to do the change of variables to index lower coordinates:

\begin{equation*}
\frac{\partial}{\partial x^i} = \sum g_{ij} \frac{\partial}{\partial x_j}
\end{equation*}

Now we also can write the reciprocal frame vectors:

\begin{equation*}
e^i = \sum e_j e^i \cdot e^j = \sum e_j g^{ij}
\end{equation*}

Thus the gradient is:

\begin{equation}\label{eqn:tensor:680}
\begin{aligned}
\sum_i e^i \frac{\partial}{\partial x^i}
&= \sum_{ijk} e_j g^{ij} g_{ik} \frac{\partial}{\partial x_k} \\
&= \sum_{jk} \left(e_j \frac{\partial}{\partial x_k} \right) \sum_i g^{ij} g_{ik} \\
&= \sum_{jk} \left(e_j \frac{\partial}{\partial x_k} \right) \delta_k^j \\
&= \sum_{i} e_i \frac{\partial}{\partial x_i} \\
\end{aligned}
\end{equation}

My conclusion is that there is not any preference for the index upper form of the gradient in GAFP.  Both should be equivalent.  That said consistency is likely required.  FIXME: To truly get a feel for why index upper is used in this definition one likely needs to step back and look at the defining directional derivative relation for the gradient.
