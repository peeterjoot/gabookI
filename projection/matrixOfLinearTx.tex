%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Matrix of grade k multivector linear transformations}
\label{chap:matrixOfLinearTx}

%\date{May 16, 2008.  matrixOfLinearTx.tex}

\section{Motivation}

The following shows explicitly the calculation required to form the matrix of a linear transformation between two grade k multivector subspaces (is there a name for a multivector of fixed grade that is not neccessarily a blade?).
This is nothing fancy or original, but just helpful
to have written out showing naturally how one generates this matrix from a consideration of the two sets of basis vectors.  After so much exposure to linear transformations only in matrix
form it is good to write this out in a way so that it is clear exactly how the coordinate matrices come in to the picture when and if they are introduced.

\section{Content}

Given \(T\), a linear transformation between two grade k multivector subspaces,
let \(\sigma = \{\sigma_i\}_{i=1}^m\) be a basis for a grade k multivector subspace.
For \(T(x) \in span\{ \beta_i \}\) (ie: image of T contained in this span).  Let \(\beta = \{\beta_i\}_{i=1}^n\) be a basis for this (possibly different) grade k multivector subspace.

Additionally introduce a set of respective reciprocal frames \(\{\sigma^i\}\), and \(\{\beta^i\}\).
Define the reciprocal frame with respect to the dot product for the space.  For a linearly independent, but not necessary orthogonal (or orthonormal), set of vectors \(\{u_i\}\) this set
has as its defining property:

\begin{equation}\label{eqn:matrixOfLinearTx:20}
u_i \cdot u^j = \delta_{ij}
\end{equation}

I have chosen to use this covariant, contravariant coordinate notation since that works well for both vectors (not necessarily orthogonal or orthonormal), as well as higher grade vectors.  When the basis is orthonormal these reciprocal frame grade k multivectors can be computed with just reversion.  For example, suppose that \(\{\beta_i\}\) is an orthonormal bivector basis for the image of \(T\), then the reciprocal frame bivectors are just \(\beta^i = {\beta_i}^\dagger\).

With this we can decompose the linear transformation into components generated by each of the \(\sigma_i\) grade k multivectors:

\begin{equation}
T(x) = T(\sum x \cdot \sigma_j \sigma^j) = \sum x_j T(\sigma^j)
\end{equation}

This we can write as a matrix equation:

\begin{equation}
T(x) =
\begin{bmatrix}
T(\sigma^1) & T(\sigma^1) & \cdots & T(\sigma^n)
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{bmatrix}
\end{equation}

Now, decompose the \(T(\sigma^j)\) in terms of the basis \(\beta\):

\begin{equation}
T(\sigma^j) = \sum T(\sigma^j) \cdot \beta^i \beta_i
\end{equation}

This we can also write as a matrix equation

\begin{equation}
\begin{aligned}
T(\sigma^j) &=
\begin{bmatrix}
\beta_1 & \beta_2 & \cdots & \beta_m
\end{bmatrix}
\begin{bmatrix}
T(\sigma^j) \cdot \beta^1 \\
T(\sigma^j) \cdot \beta^2 \\
\vdots \\
T(\sigma^j) \cdot \beta^m \\
\end{bmatrix} \\
&=
\begin{bmatrix}
\beta_1 & \beta_2 & \cdots & \beta_m
\end{bmatrix}
\begin{bmatrix}
\beta^1 \cdot T(\sigma^j) \\
\beta^2 \cdot T(\sigma^j) \\
\vdots \\
\beta^m \cdot T(\sigma^j) \\
\end{bmatrix}
\end{aligned}
\end{equation}

These two sets of matrix equations, can be combined into a single equation:

\begin{equation}\label{eqn:matOfLinTx:matrixexpansion}
T(x) =
{
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_m
\end{bmatrix}
}^{\text{T}}
\begin{bmatrix}
\beta^1 \cdot T(\sigma^1) & \beta^1 \cdot T(\sigma^2) & \cdots & \beta^1 \cdot T(\sigma^n) \\
\beta^2 \cdot T(\sigma^1) & \beta^2 \cdot T(\sigma^2) & \cdots & \beta^2 \cdot T(\sigma^n) \\
\vdots & \cdots & \ddots & \vdots \\
\beta^m \cdot T(\sigma^1) & \beta^m \cdot T(\sigma^2) & \cdots & \beta^m \cdot T(\sigma^n) \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{bmatrix}
\end{equation}

Here the matrix \(( x_1, x_2, \cdots, x_n )\) is a coordinate vector with respect to basis \(\sigma\), but the vector
\(( \beta_1, \beta_2, \cdots, \beta_n )\) is matrix of the basis vectors \(\beta_i \in \beta\).  This makes sense since the end result has not been defined in terms of
a coordinate vector space, but the space of T itself.

This can all be written more compactly as

\begin{equation}
T(x)
=
{
\begin{bmatrix}
\beta_i \\
\end{bmatrix}
}^{\text{T}}
\begin{bmatrix}
\beta^i \cdot T(\sigma^j)
\end{bmatrix}
\begin{bmatrix}
x_i \\
\end{bmatrix}
\end{equation}

We can also recover the original result from this by direct expansion and then regrouping:

\begin{equation}\label{eqn:matrixOfLinearTx:40}
\begin{aligned}
{
\begin{bmatrix}
\beta_i \\
\end{bmatrix}
}^{\text{T}}
\begin{bmatrix}
\sum_j \beta^i \cdot T(\sigma^j) x_j
\end{bmatrix}
&=
\begin{bmatrix}
\sum_{kj} \beta_k \beta^k \cdot T(\sigma^j) x_j
\end{bmatrix} \\
&=
\sum_{kj} \beta_k \beta^k \cdot T(\sigma^j) x_j \\
&=
\sum_{k} \beta_k \beta^k \cdot T(\sum_j \sigma^j x_j) \\
&=
\sum_{k} \beta_k \beta^k \cdot T(x) \\
&=
T(x) \\
\end{aligned}
\end{equation}

Observe that this demonstrates that we can write the coordinate vector \([T]_\beta\) as the two left most matrices above

\begin{equation}\label{eqn:matrixOfLinearTx:60}
\begin{aligned}
[T(x)]_\beta
&=
{
\begin{bmatrix}
\beta^i \cdot T(x)
\end{bmatrix}
}_i \\
&=
{
\begin{bmatrix}
\sum_{j} \beta^i \cdot T(\sigma^j) x_j
\end{bmatrix}
}_i \\
&=
\begin{bmatrix}
\beta^i \cdot T(\sigma^j)
\end{bmatrix}
\begin{bmatrix}
x_i \\
\end{bmatrix}
\end{aligned}
\end{equation}

Looking at the above I found it interesting that \eqnref{eqn:matOfLinTx:matrixexpansion} which embeds the coordinate vector of \(T(x)\) has the structure of a bilinear form, so in a sense one can view the
matrix of a linear transformation:

\begin{equation}
[T]_\sigma^\beta =
\begin{bmatrix}
\beta^i \cdot T(\sigma^j)
\end{bmatrix}
\end{equation}

as a bilinear form that can act as a mapping from the generating basis to the image basis.
