%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Reciprocal Frame Vectors}
\index{reciprocal frame}
\label{chap:reciprocalFrame}
%\date{March 29, 2008.  reciprocalFrame.tex}

\section{Approach without Geometric Algebra}

Without employing geometric algebra, one can use the projection
operation expressed as a dot product and calculate the a vector
orthogonal to a set of other vectors, in the direction of a reference
vector.

Such a calculation also yields \R{N} results in terms of determinants, and as a side
effect produces equations for
parallelogram area, parallelepiped volume and higher dimensional analogues as a side effect
(without having to employ change of basis diagonalization arguments that do not work well
for higher dimensional subspaces).

\subsection{Orthogonal to one vector}

The simplest case is the vector perpendicular to another.  In anything
but \R{2} there are a whole set of such vectors, so to express this as a
non-set result a reference vector is required.

Calculation of the coordinate vector for this case follows directly from
the dot product.  Borrowing the GA term, we subtract the projection
to calculate the rejection.

\begin{equation}\label{eqn:reciprocalFrame:363}
\begin{aligned}
\Rej{\ucap}{\Bv}
&= \Bv - \Bv \cdot \ucap \ucap \\
&= \inv{\Bu^2}(\Bv\Bu^2 - \Bv \cdot \Bu \Bu) \\
&= \inv{\Bu^2}\sum{v_i\Be_i u_j u_j - v_j u_j u_i \Be_i} \\
&= \inv{\Bu^2}\sum{u_j\Be_i\DETuvij{v}{u}{i}{j}} \\
&= \inv{\Bu^2}\sum_{i<j}{(u_i \Be_j -u_j\Be_i)\DETuvij{u}{v}{i}{j}} \\
\end{aligned}
\end{equation}

Thus we can write the rejection of \(\Bv\) from \(\ucap\) as:

\begin{equation}\label{eqn:reciprocal_frame:rejonevector}
\Rej{\ucap}{\Bv} = \inv{\Bu^2}\sum_{i<j}\DETuvij{u}{v}{i}{j}\DETuvij{u}{\Be}{i}{j}
\end{equation}

Or introducing some shorthand:

\begin{equation}\label{eqn:reciprocalFrame:383}
\begin{aligned}
D_{ij}^{\Bu \Bv} &= \DETuvij{u}{v}{i}{j} \\
D_{ij}^{\Bu \Be} &= \DETuvij{u}{\Be}{i}{j} \\
\end{aligned}
\end{equation}

\eqnref{eqn:reciprocal_frame:rejonevector} can be expressed in a form that will be slightly more convenient for larger sets of
vectors:

\begin{equation}\label{eqn:reciprocal_frame:rejonevectorD}
\Rej{\ucap}{\Bv} = \inv{\Bu^2}\sum_{i<j} D_{ij}^{\Bu \Bv} D_{ij}^{\Bu \Be}
\end{equation}

Note that although the GA axiom \(\Bu^2 = \Bu \cdot \Bu\) has been used
in equations \eqnref{eqn:reciprocal_frame:rejonevector} and \eqnref{eqn:reciprocal_frame:rejonevectorD} above and the derivation, that was
not necessary to prove them.
This can, for now, be thought of as a notational convenience, to avoid having to write \(\Bu \cdot \Bu\), or
\(\norm{\Bu}^2\).

This result can be used to express the \R{N} area of a parallelogram since we just have to multiply the length
of \(\Rej{\ucap}{\Bv}\):

\begin{equation}\label{eqn:reciprocalFrame:23}
\norm{\Rej{\ucap}{\Bv}}^2 =
\Rej{\ucap}{\Bv} \cdot \Bv =
\inv{\Bu^2}\sum_{i<j} {\left(D_{ij}^{\Bu \Bv}\right)}^2
\end{equation}

with the length of the base \(\norm{\Bu}\). [FIXME: insert figure.]

Thus the area (squared) is:

\begin{equation}\label{eqn:reciprocal_frame:parallogramarea}
\AreaOp{\Bu,\Bv}^2 = \sum_{i<j} {\left(D_{ij}^{\Bu \Bv}\right)}^2
\end{equation}

For the special case of a vector in \R{2} this is
\begin{equation}\label{eqn:reciprocal_frame:parallogramarear2}
\AreaOp{\Bu,\Bv} = \abs{D_{12}^{\Bu \Bv}} = \AbsName\left(\DETuvij{u}{v}{i}{j}\right)
\end{equation}

\subsection{Vector orthogonal to two vectors in direction of a third}

The same procedure can be followed for three vectors, but the algebra gets messier.  Given three vectors \(\Bu\), \(\Bv\), and \(\Bw\)
we can calculate the component \(\Bw'\) of \(\Bw\) perpendicular to \(\Bu\) and \(\Bv\).  That is:

\begin{equation}\label{eqn:reciprocalFrame:403}
\begin{aligned}
\Bv' &= \Bv - \Bv \cdot \ucap \ucap \\
\implies & \\
\Bw' &= \Bw - \Bw \cdot \ucap \ucap - \Bw \cdot \hat{\Bv'} \hat{\Bv'}
\end{aligned}
\end{equation}

After expanding this out, a number of the terms magically cancel out and one is left with

\begin{equation}\label{eqn:reciprocalFrame:423}
\begin{aligned}
\Bw'' = \Bw' (\Bu^2\Bv^2 - (\Bu \cdot \Bv)^2)
&= \Bu \left(-\Bu \cdot \Bw \Bv^2 + (\Bu \cdot \Bv)(\Bv \cdot \Bw)\right)  \\
&+ \Bv \left(-\Bu^2(\Bv \cdot \Bw) - (\Bu \cdot \Bv)(\Bu \cdot \Bw)\right)  \\
&+ \Bw \left(\Bu^2\Bv^2 - (\Bu \cdot \Bv)^2\right) \\
\end{aligned}
\end{equation}

And this in turn can be expanded in terms of coordinates and the results collected yielding

\begin{equation}\label{eqn:reciprocalFrame:443}
\begin{aligned}
\Bw'' &= \sum \Be_i u_j v_k \left(
u_i \DETuvij{v}{w}{j}{k}
-v_i \DETuvij{u}{w}{j}{k}
w_i \DETuvij{u}{v}{j}{k}
\right) \\
&= \sum \Be_i u_j v_k \DETuvwijk{u}{v}{w}{i}{j}{k} \\
&= \sum_{i,j<k} \Be_i \DETuvij{u}{v}{j}{k} \DETuvwijk{u}{v}{w}{i}{j}{k} \\
&=
\left(\sum_{i<j<k} + \sum_{j<i<k} + \sum_{j<k<i} \right) \Be_i \DETuvij{u}{v}{j}{k} \DETuvwijk{u}{v}{w}{i}{j}{k}.
\end{aligned}
\end{equation}

Expanding the sum of the denominator in terms of coordinates:
\begin{equation}\label{eqn:reciprocalFrame:43}
\Bu^2\Bv^2 - (\Bu \cdot \Bv)^2 = \sum_{i<j} \DETuvij{u}{v}{i}{j}^2
\end{equation}

and using a change of summation indices, our final result for the vector perpendicular to two others in the direction of a third is:

\begin{equation}\label{eqn:reciprocal_frame:orthotwovectors}
\Rej{\ucap,\vcap}{\Bw} =
\frac{\sum_{i<j<k} \DETuvwijk{u}{v}{w}{i}{j}{k} \DETuvwijk{u}{v}{\Be}{i}{j}{k}}
{\sum_{i<j} \DETuvij{u}{v}{i}{j}^2}
\end{equation}

As a small aside, it is notable here to observe that
\(\Span\left\{\DETuvij{u}{\Be}{i}{j}\right\}\) is the null space for the vector \(\Bu\), and
the set \(\Span\left\{\DETuvwijk{u}{v}{\Be}{i}{j}{k}\right\}\) is the null space for the two vectors \(\Bu\) and \(\Bv\) respectively.

Since the rejection is a normal to the set of vectors it must necessarily include these cross product like determinant terms.

As in \eqnref{eqn:reciprocal_frame:rejonevectorD}, use of a \(D_{ijk}^{\Bu\Bv\Bw}\) notation allows for a more compact
result:

\begin{equation}\label{eqn:reciprocal_frame:rejtwovectorsD}
\Rej{\ucap\vcap}{\Bw} =
{\left(\sum_{i<j} \left(D_{ij}^{\Bu\Bv}\right)^2\right)}^{-1}
\sum_{i<j<k} D_{ijk}^{\Bu\Bv\Bw} D_{ijk}^{\Bu\Bv\Be}
\end{equation}

And, as before this yields the Volume of the parallelepiped by multiplying perpendicular height:

\begin{equation}\label{eqn:reciprocalFrame:63}
\norm{\Rej{\ucap\vcap}{\Bw}} =
\Rej{\ucap\vcap}{\Bw} \cdot \Bw =
{\left(\sum_{i<j} \left(D_{ij}^{\Bu\Bv}\right)^2\right)}^{-1}
\sum_{i<j<k} \left(D_{ijk}^{\Bu\Bv\Bw} \right)^2
\end{equation}

by the base area.

Thus the squared volume of a parallelepiped spanned by the three vectors is:

\begin{equation}\label{eqn:reciprocal_frame:parallopipedvolume}
\VolumeOp{\Bu,\Bv,\Bw}^2 = \sum_{i<j<k} {\left(D_{ijk}^{\Bu \Bv \Bw}\right)}^2.
\end{equation}

The simplest case is for \R{3} where we have only one summand:

\begin{equation}\label{eqn:reciprocal_frame:parallopipedvolumer3}
\VolumeOp{\Bu,\Bv,\Bw}
= \abs{D_{ijk}^{\Bu \Bv \Bw}}
= \AbsName\left(
\DETuvwijk{u}{v}{w}{1}{2}{3}
\right).
\end{equation}

\subsection{Generalization.  Inductive Hypothesis}

There are two things to prove

\begin{enumerate}
\item hypervolume of parallelepiped spanned by vectors \(\Bu_1, \Bu_2, \dots, \Bu_k\)

\begin{equation} \label{eqn:reciprocal_frame:hypervolume}
\VolumeOp{\Bu_1, \Bu_2, \cdots, \Bu_k}^2
=
\sum_{i_1 < i_2 < \cdots < i_k } \left(
D_{i_1 i_2 \cdots i_k}^{\Bu_{i_1} \Bu_{i_2} \cdots \Bu_{i_k}}
\right)^2
\end{equation}

\item Orthogonal rejection of a set of vectors in direction of another.

\begin{equation} \label{eqn:reciprocal_frame:hyperrejection}
\Rej{\ucap_1\cdots\ucap_{k-1}}{\Bu_k} =
\frac{
\sum_{i_1 < \cdots < i_{k} }
D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}}
D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k-1}} \Be }}
{
\sum_{i_1 < \cdots < i_{k-1} } \left(D_{i_1 \cdots i_{k-1}}^{\Bu_{i_1} \cdots \Bu_{i_{k-1}}}\right)^2
}
\end{equation}
\end{enumerate}

I cannot recall if I ever did the inductive proof for this.
Proving for the initial case is done (since it is proved for both the
two and three vector cases).  For the limiting case where \(k=n\) it can be observed that this is normal to all the others, so the
only thing to prove for that case is if the scaling provided by hypervolume \eqnref{eqn:reciprocal_frame:hypervolume} is correct.

\subsection{Scaling required for reciprocal frame vector}

Presuming an inductive proof of the general result of \eqnref{eqn:reciprocal_frame:hyperrejection} is possible, this rejection
has the property

\begin{equation*}
\Rej{\ucap_1\cdots\ucap_{k-1}}{\Bu_k} \cdot \Bu_i \propto \delta_{ki}
\end{equation*}

With the scaling factor picked so that this equals \(\delta_{ki}\), the resulting ``reciprocal frame vector'' is

\begin{equation} \label{eqn:reciprocal_frame:framevec}
\Bu^k =
\frac{
\sum_{i_1 < \cdots < i_{k} }
D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}}
D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k-1}} \Be }}
{
\sum_{i_1 < \cdots < i_{k} } \left(D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}}\right)^2
}
\end{equation}

The superscript notation is borrowed from Doran/Lasenby, and denotes not a vector raised to a power, but this
this special vector satisfying the following orthogonality and scaling criteria:

\begin{equation}\label{eqn:reciprocal_frame:reciportho}
\Bu^k \cdot \Bu_i = \delta_{ki}.
\end{equation}

Note that for \(k=n-1\), \eqnref{eqn:reciprocal_frame:framevec} reduces to

\begin{equation} \label{eqn:reciprocal_frame:framevecnminus}
\Bu^n =
\frac{ D_{1 \cdots (n-1)}^{\Bu_1 \cdots \Bu_{n-1} \Be} } { D_{1 \cdots n}^{\Bu_1 \cdots \Bu_n} }.
\end{equation}

This or some other scaled version of this is likely as close as we can come to generalizing the cross product
as an operation that takes vectors to vectors.

\subsection{Example.  \texorpdfstring{\R{3}}{3D} case.  Perpendicular to two vectors}

Observe that for \R{3}, writing \(\Bu = \Bu_1\), \(\Bv = \Bu_2\), \(\Bw = \Bu_3\), and \(\Bw' = {\Bu_3}^3\) this is:

\begin{equation}
\Bw' =
\frac{\DETuvwijk{u}{v}{\Be}{1}{2}{3}}{\DETuvwijk{u}{v}{w}{1}{2}{3}}
=
\frac{\Bu \cross \Bv}{(\Bu \cross \Bv) \cdot \Bw}
\end{equation}

This is the cross product scaled by the (signed) volume for the parallelepiped spanned by the three vectors.

\section{Derivation with GA}

Regression with respect to a set of vectors can be expressed directly.  For vectors \({\Bu_i}\) write \(\BB = \Bu_1 \wedge \Bu_2 \cdots \Bu_k\).  Then for any vector we have:

\begin{equation}\label{eqn:reciprocalFrame:463}
\begin{aligned}
\Bx
&= \Bx \BB \inv{\BB}  \\
&= \gpgradeone{ \Bx \BB \inv{\BB} } \\
&= \gpgradeone{ (\Bx \cdot \BB + \Bx \wedge \BB) \inv{\BB} }
\end{aligned}
\end{equation}

All the grade three and grade five terms are selected out by the grade one operation, leaving just

\begin{equation}
\Bx = (\Bx \cdot \BB) \cdot \inv{\BB} + (\Bx \wedge \BB) \cdot \inv{\BB}.
\end{equation}

This last term is the rejective component.

\begin{equation}\label{eqn:reciprocal_frame:bladerejection}
\Rej{\BB}{\Bx} =
(\Bx \wedge \BB) \cdot \inv{\BB}
=
\frac{
(\Bx \wedge \BB) \cdot {\BB}^\dagger
}
{
\BB \BB^\dagger
}
\end{equation}

Here we see in the denominator the squared sum of determinants in the denominator of \eqnref{eqn:reciprocal_frame:hyperrejection}:

\begin{equation}\label{eqn:reciprocalFrame:83}
\BB \BB^\dagger =
\sum_{i_1 < \cdots < i_{k} } \left(D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}}\right)^2
\end{equation}

In the numerator we have the dot product of two wedge products, each expressible as sums of determinants:

\begin{equation}\label{eqn:reciprocalFrame:103}
\BB^\dagger = (-1)^{k(k-1)/2}
\sum_{i_1 < \cdots < i_{k} }
D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}} \Be_{i_1} \Be_{i_2} \cdots \Be_{i_{k}}
\end{equation}

And
\begin{equation}\label{eqn:reciprocalFrame:123}
\Bx \wedge \BB =
\sum_{i_1 < \cdots < i_{k+1} }
D_{i_1 \cdots i_{k+1}}^{\Bx \Bu_{i_1} \cdots \Bu_{i_{k}}} \Be_{i_1} \Be_{i_2} \cdots \Be_{i_{k+1}}
\end{equation}

Dotting these is all the grade one components of the product.
Performing that calculation would likely provide an explicit confirmation of the inductive hypothesis of
\eqnref{eqn:reciprocal_frame:hyperrejection}.  This can be observed directly for the \(k+1=n\) case.  That product produces a Laplace
expansion sum.

\begin{equation}\label{eqn:reciprocalFrame:483}
\begin{aligned}
(\Bx \wedge \BB) \cdot \BB^\dagger
&=
%((-1)^{k(k-1)/2})^2
D_{1 2 \cdots n}^{\Bx \Bu_{1} \cdots \Bu_{n-1}}
\left(
\Be_{1} D_{2 3 4 \cdots n}^{\Bu_{1} \cdots \Bu_{n-1}}
-\Be_{2} D_{1 3 4 \cdots n}^{\Bu_{1} \cdots \Bu_{n-1}}
+\Be_{3} D_{1 2 4 \cdots n}^{\Bu_{1} \cdots \Bu_{n-1}}
\right)
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:reciprocal_frame:hyperrejectionganminus}
(\Bx \wedge \BB) \cdot \inv{\BB}
=
\frac{
D_{1 2 \cdots n}^{\Bx \Bu_{1} \cdots \Bu_{n-1}}
D_{1 2 \cdots n}^{\Be \Bu_{1} \cdots \Bu_{n-1}}
}
{
\sum_{i_1 < \cdots < i_{k} } \left(D_{i_1 \cdots i_{k}}^{\Bu_{i_1} \cdots \Bu_{i_{k}}}\right)^2
}
\end{equation}

Thus \eqnref{eqn:reciprocal_frame:hyperrejection} for the \(k = n-1\) case is proved without induction.  A proof for the \(k+1<n\) case would be harder.
No proof is required if one picks the set of basis vectors \({\Be_i}\) such that \(\Be_i \wedge \BB = 0\) (then the \(k+1=n\) result applies).
I believe that proves the general case too if one observes that a rotation to any other basis in the span of the set of vectors only
changes the sign of the each of the determinants, and the product of the two sign changes will then have value one.

Follow through of the details for a proof of original non GA induction hypothesis is probably not worthwhile since this
reciprocal frame vector problem can be
tackled with a different approach using a subspace pseudovector.

It is notable that although this had no induction in the argument above, note that it is fundamentally required.
That is because there is an inductive proof
required to prove that the general wedge and dot product vector formulas:

\begin{equation}\label{eqn:reciprocalFrame:143}
\Bx \cdot \BB = \inv{2}(\Bx \BB - (-1)^k\BB \Bx)
\end{equation}
\begin{equation}\label{eqn:reciprocalFrame:163}
\Bx \wedge \BB = \inv{2}(\Bx \BB + (-1)^k\BB \Bx)
\end{equation}

from the GA axioms (that is an easier proof without the mass of indices and determinant products.)

\section{Pseudovector from rejection}

As noted in the previous section the reciprocal frame vector \(\Bu^k\) is the vector in the direction of \(\Bu_k\) that has no component
in \(\Span{ \Bu_1, \cdots, \Bu_{k-1}}\), normalized such that \(\Bu_k \cdot \Bu^k = 1\).  Explicitly, with
\(\BB = \Bu_1 \wedge \Bu_2 \cdots \wedge \Bu_{k-1}\) this is:

\begin{equation}\label{eqn:reciprocal_frame:reciprej}
\Bu^k =
\frac
{
(\Bu_k \wedge \BB) \cdot \BB
}
{
\Bu_k \cdot \left((\Bu_k \wedge \BB) \cdot \BB\right)
}
\end{equation}

This is derived from \eqnref{eqn:reciprocal_frame:bladerejection}, after noting that
\(\frac{\BB^\dagger}{\BB\BB^\dagger} \propto \BB\), and further
scaling to produce the desired orthonormal property of equation
\eqnref{eqn:reciprocal_frame:reciportho}
that defines the reciprocal frame vector.

\subsection{back to reciprocal result}

Now,
\eqnref{eqn:reciprocal_frame:reciprej}
looks considerably different from the Doran/Lasenby result.
Reduction to a direct pseudovector/blade product is possible since the
dot product here can be converted to a direct product.

\begin{equation}\label{eqn:reciprocalFrame:503}
\begin{aligned}
(\Bu_k \wedge \BB) \cdot \BB
&=
\mathLabelBox
[
   labelstyle={xshift=2cm},
   linestyle={out=270,in=90, latex-}
]
{(\Bx \BB)}{\(\Bx = \Bu_k - (\Bu_k \cdot \BB) \cdot \inv{\BB}\)}
\cdot \BB \\
&= \gpgradeone{
\Bx\BB \BB
} \\
&= \Bx \BB^2 \\
&= \left(\left(\Bu_k - (\Bu_k \cdot \BB) \cdot \inv{\BB}\right) \wedge \BB\right) \BB \\
&= (\Bu_k \wedge \BB) \BB \\
\end{aligned}
\end{equation}

Thus \eqnref{eqn:reciprocal_frame:reciprej} is a scaled pseudovector for the subspace
defined by \(\Span {\Bu_i}\), multiplied by a k-1 blade.

\section{Components of a vector}

The delta property of
\eqnref{eqn:reciprocal_frame:reciportho} allows one to use the reciprocal frame
vectors and the basis that generated them to calculate the coordinates
of the a vector with respect to this (not necessarily orthonormal) basis.

That is a pretty powerful result, but somewhat obscured by the Doran/Lasenby
super/sub script notation.

Suppose one writes a vector in \(\Span{\Bu_i}\) in terms of unknown coefficients

\begin{equation}\label{eqn:reciprocalFrame:183}
\Ba = \sum a_i \Bu_i
\end{equation}

Dotting with \(\Bu^j\) gives:

\begin{equation}\label{eqn:reciprocalFrame:203}
\Ba \cdot \Bu^j
= \sum a_i \Bu_i \cdot \Bu^j
= \sum a_i \delta_{ij}
= a_j
\end{equation}

Thus
\begin{equation}\label{eqn:reciprocal_frame:nonrecipcomponents}
\Ba = \sum (\Ba \cdot \Bu^i) \Bu_i
\end{equation}

Similarly, writing this vectors in terms of \(\Bu^i\) we have

\begin{equation}\label{eqn:reciprocalFrame:223}
\Ba = \sum b_i \Bu^i
\end{equation}

Dotting with \(\Bu_j\) gives:

\begin{equation}\label{eqn:reciprocalFrame:243}
\Ba \cdot \Bu_j
= \sum b_i \Bu^i \cdot \Bu_j
= \sum b_i \delta_{ij}
= b_j
\end{equation}

Thus
\begin{equation}\label{eqn:reciprocal_frame:recipcomponents}
\Ba = \sum (\Ba \cdot \Bu_i) \Bu^i
\end{equation}

We are used to seeing the equation for components of a vector in terms of a
basis in the following form:

\begin{equation}
\Ba = \sum (\Ba \cdot \Bu_i) \Bu_i
\end{equation}

This is true only when the basis vectors are orthonormal.
Equations
\eqnref{eqn:reciprocal_frame:nonrecipcomponents} and \eqnref{eqn:reciprocal_frame:recipcomponents} provide the
general decomposition of a vector in terms of a general linearly independent
set.

\subsection{Reciprocal frame vectors by solving coordinate equation}

A more natural way to these results are to take repeated wedge products.
Given a vector decomposition in terms of a basis \({\Bu_i}\), we want to solve for \(a_i\):

\begin{equation}\label{eqn:reciprocalFrame:263}
\Ba = \sum_{i=1}^k a_i \Bu_i
\end{equation}

The solution, from the wedge is:

\begin{equation}\label{eqn:reciprocalFrame:283}
\Ba \wedge (\Bu_1 \wedge \Bu_2 \cdots \check{\Bu_i} \cdots \wedge \Bu_k  = a_i (-1)^{i-1} \Bu_1 \wedge \cdots \wedge \Bu_k
\end{equation}
\begin{equation}\label{eqn:reciprocalFrame:303}
\implies
a_i =
(-1)^{i-1}
\frac{\Ba \wedge (\Bu_1 \wedge \Bu_2 \cdots \check{\Bu_i} \cdots \wedge \Bu_k}{ \Bu_1 \wedge \cdots \wedge \Bu_k })
\end{equation}

The complete vector in terms of components is thus:

\begin{equation}\label{eqn:reciprocal_frame:coordinateswedge}
\Ba =
\sum(-1)^{i-1}
\frac{\Ba \wedge \left(\Bu_1 \wedge \Bu_2 \cdots \check{\Bu_i} \cdots \wedge \Bu_k\right)}{ \Bu_1 \wedge \cdots \wedge \Bu_k } \Bu_i
\end{equation}


We are used to seeing the coordinates expressed in terms of dot products instead of wedge products.  As in \R{3} where
the pseudovector allows wedge products to be expressed in terms of the dot product we can do the same for the general case.

Writing \(\BB \in \bigwedge^{k-1}\) and \(\BI \in \bigwedge^k\) we want to reduce an equation of the following form

\begin{equation}\label{eqn:reciprocal_frame:wedgereduction}
\frac{\Ba \wedge \BB}{\BI} = \inv{\BI} \frac{\Ba\BB + (-1)^{k-1}\BB\Ba}{2}
\end{equation}

The pseudovector either commutes or anticommutes with a vector in the subspace depending on the grade

\begin{equation}\label{eqn:reciprocalFrame:523}
\begin{aligned}
\BI \Ba
&= \BI \cdot \Ba + \mathLabelBox{\BI \wedge \Ba}{\(=0\)} \\
&= (-1)^{k-1}\Ba \cdot \BI \\
&= (-1)^{k-1}\Ba \BI \\
\end{aligned}
\end{equation}

Substituting back into \eqnref{eqn:reciprocal_frame:wedgereduction} we have

\begin{equation}\label{eqn:reciprocalFrame:543}
\begin{aligned}
\frac{\Ba \wedge \BB}{\BI}
&= (-1)^{k-1} \frac{\Ba \left(\inv{\BI}\BB\right) + \left(\inv{\BI}\BB\right)\Ba}{2} \\
&= (-1)^{k-1} \Ba \cdot \left(\inv{\BI}\BB\right) \\
&= \Ba \cdot \left(\BB \inv{\BI}\right) \\
\end{aligned}
\end{equation}

With \(\BI = \Bu_1 \wedge \cdots \Bu_k\), and \(\BB = \Bu_1 \wedge \Bu_2 \cdots \check{\Bu_i} \cdots \wedge \Bu_k\),
back substitution back into \eqnref{eqn:reciprocal_frame:coordinateswedge} is thus

\begin{equation}\label{eqn:reciprocalFrame:563}
\begin{aligned}
\Ba
%&= \sum(-1)^{i-1} (-1)^{k-1} \Ba \cdot \left(\inv{\BI}\BB\right) \Bu_i \\
&= \sum
\Ba \cdot \left(
(-1)^{i-1}
\BB \inv{\BI}\right) \Bu_i
\end{aligned}
\end{equation}

The final result yields the reciprocal frame vector \(\Bu^k\), and we see how to arrive at this result naturally attempting
to answer the question of how to find the coordinates of a vector with respect to a (not necessarily orthonormal) basis.

\begin{equation}\label{eqn:reciprocal_frame:recipdot}
\Ba =
\sum
\Ba \cdot
\mathLabelBox{
\left((\Bu_1 \wedge \Bu_2 \cdots \check{\Bu_i} \cdots \wedge \Bu_k) \frac{ (-1)^{i-1} }{ \Bu_1 \wedge \cdots \wedge \Bu_k }\right)
}{\(\Bu^k\)}
\Bu_i
\end{equation}

\section{Components of a bivector}

To find the coordinates of a bivector with respect to an arbitrary basis we have a similar problem.
For a vector basis \({\Ba_i}\), introduce a bivector basis \({\Ba_i \wedge \Ba_j}\), and write

\begin{equation}\label{eqn:reciprocal_frame:bivectorcoord}
\BB = \sum_{u<v} b_{uv} \Ba_u \wedge \Ba_v
\end{equation}

Wedging with \(\Ba_i \wedge \Ba_j\) will select all but the \(ij\) component.  Specifically

\begin{equation}\label{eqn:reciprocalFrame:583}
\begin{aligned}
\BB \wedge
(\Ba_1 \wedge \cdots \check{\Ba_i} \cdots \check{\Ba_j} \cdots \wedge \Ba_k)
&= b_{ij} \Ba_i \wedge \Ba_j \wedge (\Ba_1 \wedge \cdots \check{\Ba_i} \cdots \check{\Ba_j} \cdots \wedge \Ba_k)  \\
&= b_{ij} (-1)^{j-2 + i-1}(\Ba_1 \wedge \cdots \wedge \Ba_k) \\
\end{aligned}
\end{equation}

Thus
\begin{equation}\label{eqn:reciprocal_frame:bivectorrecip}
b_{ij} = (-1)^{i+j-3}\BB \wedge
\frac{ (\Ba_1 \wedge \cdots \check{\Ba_i} \cdots \check{\Ba_j} \cdots \wedge \Ba_k) }
{\Ba_1 \wedge \cdots \wedge \Ba_k}
\end{equation}

We want to put this in dot product form like \eqnref{eqn:reciprocal_frame:recipdot}.  To do so we need a generalized grade reduction formula

\begin{equation}\label{eqn:reciprocal_frame:gradereduction}
(\BA_a \wedge \BA_b) \cdot \BA_c = \BA_a \cdot (\BA_b \cdot \BA_c)
\end{equation}

This holds when \(a + b \le c\).  Writing
\(\BA = \Ba_1 \wedge \cdots \check{\Ba_i} \cdots \check{\Ba_j} \cdots \wedge \Ba_k\), and
\(\BI = \Ba_1 \wedge \cdots \wedge \Ba_k\), we have

\begin{equation}\label{eqn:reciprocalFrame:603}
\begin{aligned}
(\BB \wedge \BA) \inv{\BI}
&= (\BB \wedge \BA) \cdot \inv{\BI} \\
&= \BB \cdot \left( \BA \cdot \inv{\BI} \right) \\
&= \BB \cdot \left( \BA \inv{\BI} \right) \\
\end{aligned}
\end{equation}

Thus the bivector in terms of its coordinates for this basis is:

\begin{equation}\label{eqn:reciprocal_frame:bivectordecomp}
\sum_{u<v}
\BB \cdot
\left(
(\Ba_1 \wedge \cdots \check{\Ba_u} \cdots \check{\Ba_v} \cdots \wedge \Ba_k)
\frac{(-1)^{u+v-2-1}}
{\Ba_1 \wedge \cdots \wedge \Ba_k}
\right)
\Ba_u \wedge \Ba_v
\end{equation}

It is easy to see how this generalizes to higher order blades since
\eqnref{eqn:reciprocal_frame:gradereduction} is good for all required grades.  In all cases, the form is going to be the same, with only differences
in sign and the number of omitted vectors in the \(\BA\) blade.

For example for a trivector

\begin{equation}\label{eqn:reciprocalFrame:323}
\BT = \sum_{u<v<w}t_{uvw} \Ba_u \wedge \Ba_v \wedge \Ba_w
\end{equation}

It is pretty straightforward to show that this can be decomposed as follows

\begin{equation}\label{eqn:reciprocal_frame:trivectordecomp}
\BT = \sum_{u<v<w} \BT \cdot
\left(
(\Ba_1 \wedge \cdots \check{\Ba_u} \cdots \check{\Ba_v} \cdots \check{\Ba_w} \cdots \wedge \Ba_k)
\frac{(-1)^{u+v+w-3-2-1}}
{\Ba_1 \wedge \cdots \wedge \Ba_k}
\right)
\Ba_u \wedge \Ba_v \wedge \Ba_w
\end{equation}

\subsection{Compare to GAFP}

Doran/Lasenby's GAFP
demonstrates \eqnref{eqn:reciprocal_frame:recipdot}, and with some incomprehensible steps skips to a generalized
result of the form
\footnote{ In retrospect I do not think that the in between steps had anything to do with logical sequence.  The authors wanted some of the results for subsequent stuff (like: rotor recovery) and sandwiched it between the vector and reciprocal frame multivector results somewhat out of sequence.}

\begin{equation}\label{eqn:reciprocal_frame:bivectordecompwithrecipbivector}
\BB = \sum_{i<j} \BB \cdot \left(\Ba^j \wedge \Ba^i\right) \Ba_i \wedge \Ba_j
\end{equation}

GAFP states this for general multivectors instead of bivectors, but the idea is the same.

This makes intuitive sense based on the very similar vector result.  This does not show that
the generalized reciprocal frame k-vectors calculated in
\eqnref{eqn:reciprocal_frame:bivectordecomp} or \eqnref{eqn:reciprocal_frame:trivectordecomp} can be produced simply
by wedging the corresponding individual reciprocal frame vectors.

To show that either takes algebraic identities that I do not know, or am not thinking of as applicable.
Alternately perhaps it would just take simple brute force.

Easier is to demonstrate the validity of the final result directly.  Then assuming my direct calculations
are correct implicitly demonstrates equivalence.

Starting with \(\BB\) as defined in \eqnref{eqn:reciprocal_frame:bivectorcoord}, take dot products with
\(\Ba^j \wedge \Ba^i\).

\begin{equation}\label{eqn:reciprocalFrame:623}
\begin{aligned}
\BB \cdot (\Ba^j \wedge \Ba^i)
 &= \sum_{u<v} b_{uv} (\Ba_u \wedge \Ba_v) \cdot (\Ba^j \wedge \Ba^i) \\
 &= \sum_{u<v} b_{uv}
\begin{vmatrix}
\Ba_u \cdot \Ba^i & \Ba_u \cdot \Ba^j \\
\Ba_v \cdot \Ba^i & \Ba_v \cdot \Ba^j \\
\end{vmatrix} \\
 &= \sum_{u<v} b_{uv}
\begin{vmatrix}
\delta_{ui} & \delta_{uj} \\
\delta_{vi} & \delta_{vj} \\
\end{vmatrix} \\
\end{aligned}
\end{equation}

Consider this determinant when \(u=i\) for example
\begin{equation}\label{eqn:reciprocalFrame:343}
\begin{vmatrix}
\delta_{ui} & \delta_{uj} \\
\delta_{vi} & \delta_{vj} \\
\end{vmatrix}
=
\begin{vmatrix}
1 & \delta_{ij} \\
\delta_{vi} & \delta_{vj} \\
\end{vmatrix}
=
\begin{vmatrix}
1 & 0 \\
\delta_{vi} & \delta_{vj} \\
\end{vmatrix}
= \delta_{vj}
\end{equation}

If any one index is common, then both must be common (\(ij=uv\)) for this determinant to have a non-zero (ie: one) value.  On the other hand, if no index is common then all the \(\delta\)'s are zero.

Like
\eqnref{eqn:reciprocal_frame:reciportho}
this demonstrates an orthonormal selection behavior like the reciprocal frame vector.  It has the action:

\begin{equation}
(\Ba_i \wedge \Ba_j) \cdot (\Ba^v \wedge \Ba^u) = \delta_{ij,uv}
\end{equation}

This means that we can write \(b_{uv}\) directly in terms of a bivector dot product

\begin{equation}\label{eqn:reciprocalFrame:643}
\begin{aligned}
b_{uv} = \BB \cdot (\Ba^v \wedge \Ba^u)
\end{aligned}
\end{equation}

and thus proves \eqnref{eqn:reciprocal_frame:bivectordecompwithrecipbivector}.  Proof of the general result
also follows from the determinant expansion of the respective blade dot products.

\subsection{Direct expansion of bivector in terms of reciprocal frame vectors}

Looking at linear operators I realized that the result for bivectors above can follow more easily from direct expansion of a bivector written in terms of vector factors:

\begin{equation}\label{eqn:reciprocalFrame:663}
\begin{aligned}
\Ba \wedge \Bb
&= \sum (\Ba \cdot \Bu_i \Bu^i) \wedge (\Bb \cdot \Bu_j \Bu^j) \\
&= \sum_{i<j} \left(\Ba \cdot \Bu_i \Bb \cdot \Bu_j - \Ba \cdot \Bu_j \Bb \cdot \Bu_i \right) \Bu^i \wedge \Bu^j \\
&= \sum_{i<j}
\begin{vmatrix}
\Ba \cdot \Bu_i  & \Ba \cdot \Bu_j  \\
\Bb \cdot \Bu_i & \Bb \cdot \Bu_j  \\
\end{vmatrix}
\Bu^i \wedge \Bu^j \\
\end{aligned}
\end{equation}

When the set of vectors \(\Bu_i = \Bu^i\) are orthonormal we have already
calculated this result when looking at the wedge product in a differential
forms context:

\begin{equation}
\Ba \wedge \Bb = \sum_{i<j} \DETuvij{a}{b}{i}{j} \Bu_i \wedge \Bu_j
\end{equation}

For this general case for possibly non-orthonormal frames, this
determinant of dot products can be recognized as the dot product of two blades

\begin{equation}\label{eqn:reciprocalFrame:683}
\begin{aligned}
( \Ba \wedge \Bb ) \cdot (\Bu_j \wedge \Bu_i)
&= \Ba \cdot (\Bb \cdot (\Bu_j \wedge \Bu_i)) \\
&= \Ba \cdot (\Bb \cdot \Bu_j \Bu_i - \Bb \cdot \Bu_i \Bu_j) \\
&= \Bb \cdot \Bu_j \Ba \cdot \Bu_i - \Bb \cdot \Bu_i \Ba \cdot \Bu_j \\
\end{aligned}
\end{equation}

Thus we have a decomposition of the bivector directly into a sum of components
for the reciprocal frame bivectors:

\begin{equation}
\Ba \wedge \Bb
= \sum_{i<j} \left((\Ba \wedge \Bb) \cdot (\Bu_j \wedge \Bu_i) \right) \Bu^i \wedge \Bu^j
\end{equation}
