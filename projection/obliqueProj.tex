%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Oblique projection and reciprocal frame vectors}
\index{reciprocal frame}
\index{oblique projection}
\label{chap:obliqueProj}
%\date{May 16, 2008.  obliqueProj.tex}

\section{Motivation}

Followup on wikipedia projection article's description of an oblique
projection.  Calculate this myself.

\section{Using GA.  Oblique projection onto a line}

INSERT DIAGRAM.

Problem is to project a vector \(\Bx\) onto a line with direction \(\pcap\), along a direction vector \(\dcap\).

Write:

\begin{equation}\label{eqn:oblique_proj:solveForprojLineUnit}
\Bx + \alpha \dcap = \beta \pcap
\end{equation}

and solve for \(\Bp = \beta \pcap\).  Wedging with \(\dcap\) provides the solution:

\begin{equation}\label{eqn:obliqueProj:20}
\Bx \wedge \dcap + \alpha \mathLabelBox{\dcap \wedge \dcap}{\(=0\)} = \beta \pcap \wedge \dcap
\end{equation}
\begin{equation}\label{eqn:obliqueProj:40}
\implies
\beta = \frac{\Bx \wedge \dcap}{\pcap \wedge \dcap}
\end{equation}

So the ``oblique'' projection onto this line (using direction \(\dcap\)) is:

\begin{equation}
\Proj_{\dcap \rightarrow \pcap}(\Bx) =
\frac{\Bx \wedge \dcap}{\pcap \wedge \dcap} \pcap
\end{equation}

This also shows that we do not need unit vectors for this sort of projection
operation, since we can scale these two vectors by any quantity since they are
in both the numerator and denominator.

Let \(\BD\), and \(\BP\) be vectors in the directions of \(\dcap\), and \(\pcap\) respectively.  Then the projection can also be written:

\begin{equation}\label{eqn:oblique_proj:obliqueGAproj}
\Proj_{\BD \rightarrow \BP}(\Bx) =
\frac{\Bx \wedge \BD}{\BP \wedge \BD} \BP
\end{equation}

It is interesting to see projection expressed here without any sort of dot
product when all our previous projection calculations had intrinsic
requirements for a metric.

Now, let us compare this to the matrix forms of projection that we have become familiar with.  For the matrix result we need a metric, but because
this result is intrinsically non-metric, we can introduce one if convenient and express this result with that too.  Such an expansion is:

\begin{equation}\label{eqn:obliqueProj:400}
\begin{aligned}
\frac{\Bx \wedge \BD}{\BP \wedge \BD} \BP
&=
\Bx \wedge \BD
\frac{\BD \wedge \BP}{\BD \wedge \BP}
\inv{\BP \wedge \BD}
\BP \\
&=
(\Bx \wedge \BD) \cdot (\BD \wedge \BP)
\inv{\abs{\BP \wedge \BD}^2}
\BP \\
&=
((\Bx \wedge \BD) \cdot \BD) \cdot \BP
\inv{\abs{\BP \wedge \BD}^2}
\BP \\
&=
(
\Bx \BD^2 - \Bx \cdot \BD \BD
) \cdot \BP
\inv{\abs{\BP \wedge \BD}^2}
\BP \\
&=
\frac{\Bx \cdot \BP \BD^2 - \Bx \cdot \BD \BD \cdot \BP}
{\BP^2 \BD^2 - (\BP \cdot \BD)^2
%p ^ d . d ^ p =http://antwrp.gsfc.nasa.gov/apod/ap080424.html p ^ d . d . p = (p d^2 - d.p d) . p = p^2 d^2 - (d.p)^2
}
\BP \\
\end{aligned}
\end{equation}

This gives us the projection explicitly:

\begin{equation}\label{eqn:oblique_proj:obliqueProjNearMatrixForm}
\Proj_{\BD \rightarrow \BP}(\Bx)
=
\left(\Bx \cdot \frac{\BP \BD^2 - \BD \BD \cdot \BP}{\BP^2 \BD^2 - (\BP \cdot \BD)^2}\right)
\BP
\end{equation}

It sure does not simplify things to expand things out, but we now have things prepared to express in matrix form.

Assuming a euclidean metric, and a bit of playing shows that the denominator can be written more simply as:

\begin{equation}\label{eqn:obliqueProj:60}
\BP^2 \BD^2 - (\BP \cdot \BD)^2 =
\begin{vmatrix}
U^\T U
\end{vmatrix}
\end{equation}

where:

\begin{equation}\label{eqn:obliqueProj:80}
U =
\begin{bmatrix}
\BP & \BD \\
\end{bmatrix}
\end{equation}

Similarly the numerator can be written:

\begin{equation}\label{eqn:obliqueProj:100}
\Bx \cdot \BP \BD^2 - \Bx \cdot \BD \BD \cdot \BP =
D^\T U
\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix}
U^\T \Bx.
\end{equation}

Combining these yields a projection matrix:

\begin{equation}\label{eqn:oblique_proj:matrixprojfromwedge}
\Proj_{\BD \rightarrow \BP}(\Bx) =
\left(
\BP
\inv{
\begin{vmatrix}
U^\T U
\end{vmatrix}
}
\BD^\T U
\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix}
U^\T\right) \Bx.
\end{equation}

The alternation above suggests that this is related to the matrix inverse of something.  Let us try to calculate this directly instead.

\section{Oblique projection onto a line using matrices}

Let us start at the same place as in \eqnref{eqn:oblique_proj:solveForprojLineUnit}, except that we know we can discard the unit vectors and work with any vectors in the projection directions:

\begin{equation}\label{eqn:oblique_proj:solveForprojLineNonUnit}
\Bx + \alpha \BD = \beta \BP
\end{equation}

Assuming an inner product, we have two sets of results:

\begin{equation}\label{eqn:obliqueProj:420}
\begin{aligned}
\innerprod{\BP}{\Bx} + \alpha \innerprod{\BP}{\BD} &= \beta \innerprod{\BP}{\BP} \\
\innerprod{\BD}{\Bx} + \alpha \innerprod{\BD}{\BD} &= \beta \innerprod{\BD}{\BP} \\
\end{aligned}
\end{equation}

and can solve this for \(\alpha\), and \(\beta\).

\begin{equation}\label{eqn:oblique_proj:matrixtosolve}
\begin{bmatrix}
\innerprod{\BP}{\BD} & \innerprod{\BP}{\BP} \\
\innerprod{\BD}{\BD} & \innerprod{\BD}{\BP} \\
\end{bmatrix}
\begin{bmatrix}
-\alpha \\
\beta \\
\end{bmatrix}
=
\begin{bmatrix}
\innerprod{\BP}{\Bx} \\
\innerprod{\BD}{\Bx} \\
\end{bmatrix}
\end{equation}

%This can be solved with matrix inversion, but we are only

If our inner product is defined by \(\innerprod{\Bu}{\Bv} = \Bu^* A \Bv\), we have:

\begin{equation}\label{eqn:obliqueProj:440}
\begin{aligned}
\begin{bmatrix}
\innerprod{\BP}{\BD} & \innerprod{\BP}{\BP} \\
\innerprod{\BD}{\BD} & \innerprod{\BD}{\BP} \\
\end{bmatrix}
&=
\begin{bmatrix}
{\BP}^* A {\BD} & {\BP}^* A {\BP} \\
{\BD}^* A {\BD} & {\BD}^* A {\BP} \\
\end{bmatrix} \\
&=
{
\begin{bmatrix}
\BP & \BD \\
\end{bmatrix}
}^*
A
\begin{bmatrix}
{\BD} & {\BP} \\
\end{bmatrix} \\
\end{aligned}
\end{equation}

Thus the solution to \eqnref{eqn:oblique_proj:matrixtosolve} is

\begin{equation}
\begin{bmatrix}
-\alpha \\
\beta \\
\end{bmatrix}
=
\left(
\inv{
{
\begin{bmatrix}
\BP & \BD \\
\end{bmatrix}
}^*
A
\begin{bmatrix}
{\BD} & {\BP} \\
\end{bmatrix}
}
{
\begin{bmatrix}
\BP & \BD \\
\end{bmatrix}
}^*
A
\right)
\Bx
\end{equation}

Again writing $U =
\begin{bmatrix}
\BP & \BD \\
\end{bmatrix}
$, this is:

\begin{equation}\label{eqn:obliqueProj:460}
\begin{aligned}
\begin{bmatrix}
-\alpha \\
\beta \\
\end{bmatrix}
&=
\left(
\inv{U^* A U
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix}
}
U^*
A
\right)
\Bx \\
&=
\left(
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix}
\inv{U^* A U
}
U^*
A
\right)
\Bx \\
\end{aligned}
\end{equation}

Since we only care about solution for \(\beta\) to find the projection, we have to discard half the inversion work, and just select
that part of the solution (suggests that a Cramer's rule method is more efficient than matrix inversion in this case) :

\begin{equation}\label{eqn:obliqueProj:120}
\beta =
\begin{bmatrix}
0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
-\alpha \\
\beta \\
\end{bmatrix}
\end{equation}

Thus the solution of this oblique projection problem in terms of matrices is:

\begin{equation}\label{eqn:obliqueProj:480}
\begin{aligned}
\Proj_{\BD \rightarrow \BP}(\Bx)
&=
\left(
\BP
\begin{bmatrix}
0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix}
\inv{U^* A U
}
U^*
A
\right)
\Bx
\end{aligned}
\end{equation}

Which is:

\begin{equation}
\Proj_{\BD \rightarrow \BP}(\Bx) =
\left(
\BP
\begin{bmatrix}
1 & 0 \\
\end{bmatrix}
\inv{U^* A U
}
U^*
A
\right)
\Bx
\end{equation}

Explicit expansion can be done easily enough to show that this is identical to \eqnref{eqn:oblique_proj:obliqueProjNearMatrixForm}, so
the question of what we were implicitly inverting in \eqnref{eqn:oblique_proj:matrixprojfromwedge} is answered.

\section{Oblique projection onto hyperplane}

Now that we have got this directed projection problem solved for a line in both GA and matrix form, the next logical step is a \(k\)-dimensional hyperplane projection.  The equation to solve is now:

\begin{equation}\label{eqn:oblique_proj:solveForprojPlane}
\Bx + \alpha \BD = \sum \beta_i \BP_i
\end{equation}

\subsection{Non metric solution using wedge products}

For \(\Bx\) with some component not in the hyperplane, we can wedge with \(P = \BP_1 \wedge \BP_2 \wedge \cdots \wedge \BP_k\)

\begin{equation*}
\Bx \wedge P + \alpha \BD \wedge P = \sum_{i=1}^k \beta_i \mathLabelBox{\BP_i \wedge P}{\(=0\)}
\end{equation*}
%- \Bx \wedge P = \alpha \BD \wedge P

Thus the projection onto the hyperplane spanned by \(P\) is going from \(\Bx\) along \(\BD\) is \(\Bx + \alpha \BD\):

\begin{equation}\label{eqn:oblique_proj:GAhyperplaneProjection}
\Proj_{\BD \rightarrow P}(\Bx) = \Bx - \frac{\Bx \wedge P}{\BD \wedge P} \BD
\end{equation}

\subsubsection{Q: reduction of this}

When P is a single vector we can reduce this to our previous result:

\begin{equation}\label{eqn:obliqueProj:500}
\begin{aligned}
\Proj_{\BD \rightarrow \BP}(\Bx)
&= \Bx - \frac{\Bx \wedge \BP}{\BD \wedge \BP} \BD \\
&= \inv{\BD \wedge \BP} \left((\BD \wedge \BP)\Bx - (\Bx \wedge \BP) \BD \right) \\
&= \inv{\BD \wedge \BP} \left( (\BD \wedge \BP) \cdot \Bx - (\Bx \wedge \BP)  \cdot \BD \right) \\
&= \inv{\BD \wedge \BP} \left( \BD \BP \cdot \Bx -\BP \BD \cdot \Bx -\Bx \BP \cdot \BD +\BP \Bx \cdot \BD \right) \\
&= \inv{\BD \wedge \BP} \left( \BD \BP \cdot \Bx -\Bx \BP \cdot \BD \right) \\
\end{aligned}
\end{equation}

Which is:
\begin{equation}\label{eqn:oblique_proj:followup}
\Proj_{\BD \rightarrow \BP}(\Bx)
= \inv{\BP \wedge \BD} \BP \cdot ( \BD \wedge \Bx ).
\end{equation}

A result that is equivalent to our original \eqnref{eqn:oblique_proj:obliqueGAproj}.  Can we similarly reduce the general result to something of this form.  Initially I wrote:

\begin{equation}\label{eqn:obliqueProj:520}
\begin{aligned}
\Proj_{\BD \rightarrow P}(\Bx)
&= \Bx - \frac{\Bx \wedge P}{\BD \wedge P} \BD \\
&= \frac{\BD \wedge P}{\BD \wedge P} \Bx - \frac{\Bx \wedge P}{\BD \wedge P} \BD \\
&= \inv{\BD \wedge P} \left( (\BD \wedge P) \Bx - (\Bx \wedge P) \BD \right) \\
&= \inv{\BD \wedge P} \left( (\BD \wedge P) \cdot \Bx - (\Bx \wedge P) \cdot \BD \right) \\
&= \inv{\BD \wedge P} \left( \BD P \cdot \Bx -P \BD \cdot \Bx - \Bx P \cdot \BD + P \Bx \cdot \BD \right) \\
&= \inv{\BD \wedge P} \left( \BD P \cdot \Bx - \Bx P \cdot \BD \right) \\
&= -\inv{\BD \wedge P} P \cdot (\BD \wedge \Bx ) \\
\end{aligned}
\end{equation}

However, I am not sure that about the manipulations done on the last few lines where P has grade greater than 1 (ie: the triple product expansion and recollection later).

\subsection{hyperplane directed projection using matrices}

To solve \eqnref{eqn:oblique_proj:solveForprojPlane} using matrices, we can take a set of inner products:

\begin{equation}\label{eqn:obliqueProj:540}
\begin{aligned}
\innerprod{\BD}{\Bx} + \alpha \innerprod{\BD}{\BD} &= \sum_{u=1}^k \beta_u \innerprod{\BD}{\BP_u} \\
\innerprod{\BP_i}{\Bx} + \alpha \innerprod{\BP_i}{\BD} &= \sum_{u=1}^k \beta_u \innerprod{\BP_i}{\BP_u}
\end{aligned}
\end{equation}

Write \(\BD = \BP_{k+1}\), and \(\alpha = -\beta_{k+1}\) for symmetry, which reduces this to:

\begin{equation}\label{eqn:obliqueProj:560}
\begin{aligned}
\innerprod{\BP_{k+1}}{\Bx} &= \sum_{u=1}^k \beta_u \innerprod{\BP_{k+1}}{\BP_u} + \beta_{k+1} \innerprod{\BP_{k+1}}{\BP_{k+1}}  \\
\innerprod{\BP_i}{\Bx} &= \sum_{u=1}^k \beta_u \innerprod{\BP_i}{\BP_u} + \beta_{k+1} \innerprod{\BP_i}{\BP_{k+1}}
\end{aligned}
\end{equation}

That is the following set of equations:

\begin{equation}\label{eqn:obliqueProj:140}
\innerprod{\BP_i}{\Bx} = \sum_{u=1}^{k+1} \beta_u \innerprod{\BP_i}{\BP_u}
\end{equation}

Which we can now express as a single matrix equation (for \(i,j \in [1,k+1]\)) :

\begin{equation}
{
\begin{bmatrix}
\innerprod{\BP_i}{\Bx}
\end{bmatrix}
}_i
=
{
\begin{bmatrix}
\innerprod{\BP_i}{\BP_j}
\end{bmatrix}
}_{ij}
{
\begin{bmatrix}
\beta_i
\end{bmatrix}
}_i
\end{equation}

Solving for $\Bbeta =
{
\begin{bmatrix}
\beta_i
\end{bmatrix}
}_i
$, gives:

\begin{equation}\label{eqn:obliqueProj:160}
\Bbeta =
\inv{
\begin{bmatrix}
\innerprod{\BP_i}{\BP_j}
\end{bmatrix}
}_{ij}
{
\begin{bmatrix}
\innerprod{\BP_i}{\Bx}
\end{bmatrix}
}_i
\end{equation}

The projective components of interest are \(\sum_{i=1}^k \beta_i \BP_i\).  In matrix form that is:

\begin{equation}\label{eqn:obliqueProj:580}
\begin{aligned}
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_k
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_k
\end{bmatrix}
=
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_k
\end{bmatrix}
\begin{bmatrix}
I_{k,k} & 0_{k,1}
\end{bmatrix}
\Bbeta
\end{aligned}
\end{equation}

Therefore the directed projection is:

\begin{equation}
\Proj_{\BD \rightarrow P}(\Bx)
=
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_k
\end{bmatrix}
\begin{bmatrix}
I_{k,k} & 0_{k,1}
\end{bmatrix}
\inv{
\begin{bmatrix}
\innerprod{\BP_i}{\BP_j}
\end{bmatrix}
}_{ij}
{
\begin{bmatrix}
\innerprod{\BP_i}{\Bx}
\end{bmatrix}
}_i
\end{equation}

As before writing \( U =
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_k & \BD
\end{bmatrix}
\), and write \(\innerprod{\Bu}{\Bv} = \Bu^* A \Bv \).  The directed projection is now:

\begin{equation*}
\Proj_{\BD \rightarrow P}(\Bx)
=
\left(
U
\begin{bmatrix}
I_{k,k} \\
0_{1,k} \\
\end{bmatrix}
\begin{bmatrix}
I_{k,k} & 0_{k,1}
\end{bmatrix}
\inv{
U^* A U
}
U^* A
\right)
\Bx
\end{equation*}
\begin{equation}
=
\left(
U
\begin{bmatrix}
I_{k,k} & 0_{k,1} \\
0_{1,k} & 0_{1,1} \\
\end{bmatrix}
\inv{
U^* A U
}
U^* A
\right)
\Bx
\end{equation}

\section{Projection using reciprocal frame vectors}

%The calculations above amount to finding components of a vector along the projection direction vector \(\BD\), and components in the hyperplane.
%We can generalize this by taking advantage of the fact that
%componentization with respect to an arbitrary basis can be naturally expressed using the reciprocal frame for that basis.  One of more of these basis vectors can define the ``direction'' to project towards the hyperplane spanned by
%the remaining vectors.

In a sense the projection operation is essentially a calculation of components of vectors that span a given subspace.  We can also calculate these components using a reciprocal frame.  To start with consider
just orthogonal projection, where the equation to solve is:

\begin{equation}
\Bx = \Be + \sum \beta_j \BP_j
\end{equation}

and \(\Be \cdot \BP_i = 0\).

Introduce a reciprocal frame \(\{\BP^j\}\) that also spans the space of \(\{\BP_j\}\), and is defined by:

\begin{equation}\label{eqn:obliqueProj:180}
\BP_i \cdot \BP^j = \delta_{ij}
\end{equation}

With this we have:

\begin{equation}\label{eqn:obliqueProj:600}
\begin{aligned}
\Bx \cdot \BP^i
&= \mathLabelBox{\Be \cdot \BP^i}{\(=0\)} + \sum \beta_j \BP_j \cdot \BP^i \\
&= \sum \beta_j \delta_{ij} \\
&= \beta_i \\
\end{aligned}
\end{equation}

\begin{equation*}
\Bx = \Be + \sum \BP_j (\BP^j \cdot \Bx)
\end{equation*}

For a Euclidean metric the projection part of this is:

\begin{equation}\label{eqn:oblique_proj:projmatrix}
\Proj_P(\Bx) = \left( \sum \BP_j (\BP^j)^\T \right) \Bx
\end{equation}

Note that there is a freedom to remove the dot product that was employed
to form the matrix representation of \eqnref{eqn:oblique_proj:projmatrix} that may not be obvious.
I did not find that this
was obvious, when seen in Prof. Gilbert Strang's MIT OCW lectures, and
had to prove it for myself.  That proof is available at the end of \chapcite{PJMatrixReview} comparing the geometric and matrix projection operations
, in the 'That we can remove parenthesis to form projection matrix in line projection equation.' subsection.

Writing
$P =
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_k
\end{bmatrix}
$
and for the reciprocal frame vectors:
$Q =
\begin{bmatrix}
\BP^1 & \BP^2 & \cdots & \BP^k
\end{bmatrix}
$

We now have the following simple calculation for the projection matrix onto a set of linearly independent vectors (columns of \(P\)):

\begin{equation}
\Proj_P(\Bx) = P Q^\T \Bx.
\end{equation}

Compare to the general projection matrix previously calculated when the columns of \(P\) weare not orthonormal:

\begin{equation}
\Proj_P(\Bx) = P \inv{P^\T P} P^\T \Bx
\end{equation}

With orthonormal columns the \(P^T P\) becomes identity and the inverse term drops out, and we get something similar with reciprocal frames.  As a side effect this shows us how to calculate without GA the reciprocal frame vectors.  Those vectors
are thus the columns of

\begin{equation}
Q = P \inv{P^\T P}
\end{equation}

We are thus able to get a specific understanding of some of the interior terms of the general orthogonal projection matrix.

Also note that the orthonormality of these columns is confirmed by observing that \(Q^\T P = \inv{P^\T P} P^\T P = I\).

\subsection{example/verification}

As an example to see that this works write:

\begin{equation}\label{eqn:obliqueProj:200}
P =
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
0 & 1 \\
\end{bmatrix}
\end{equation}

\begin{equation}\label{eqn:obliqueProj:220}
P^\T P =
\begin{bmatrix}
1 & 1 & 0 \\
1 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
0 & 1 \\
\end{bmatrix}
=
\begin{bmatrix}
2 & 1 \\
1 & 2 \\
\end{bmatrix}
\end{equation}

\begin{equation}\label{eqn:obliqueProj:240}
\inv{P^\T P} =
\inv{3}
\begin{bmatrix}
2 & -1 \\
-1 & 2 \\
\end{bmatrix}
\end{equation}

\begin{equation}\label{eqn:obliqueProj:260}
Q = P \inv{P^\T P} =
\inv{3}
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
2 & -1 \\
-1 & 2 \\
\end{bmatrix}
=
\inv{3}
\begin{bmatrix}
1 & 1 \\
2 & 1 \\
-1 & 2 \\
\end{bmatrix}
\end{equation}

By inspection these columns have the desired properties.

\section{Directed projection in terms of reciprocal frames}

Suppose that one has a set of vectors \(\{\BP_i\}\) that span the subspace that contains the vector
to be projected \(\Bx\).   If one wants to project onto a subset of those \(P_k\),
say, the first \(k\) of \(l\) of these vectors, and wants the projection directed along components
of the remaining \(l-k\) of these vectors, then solution of the following is required:

\begin{equation*}
\Bx = \sum_{j=1}^{l} \beta_j \BP_j
\end{equation*}

This (affine?) projection is then just the \(\sum_{j=1}^k \beta_j \BP_j\) components of this vector.

Given a reciprocal frame for the space, the solution follows immediately.

\begin{equation}\label{eqn:obliqueProj:280}
\BP^i \cdot \Bx = \sum \beta_j \BP^i \cdot \BP_j = \beta_i
\end{equation}
\begin{equation}\label{eqn:obliqueProj:300}
\beta_i = \BP^i \cdot \Bx
\end{equation}

Or,

\begin{equation}\label{eqn:obliqueProj:320}
\Proj_{P_k}(\Bx) = \sum_{j=1}^k \BP_j \BP^j \cdot \Bx
\end{equation}

In matrix form, with inner product \(\Bu \cdot \Bv = \Bu^* A \Bv\), and writing
\(P =
\begin{bmatrix}
\BP_1 & \BP_2 & \cdots & \BP_l
\end{bmatrix}\), and \(Q =
\begin{bmatrix}
\BP^1 & \BP^2 & \cdots & \BP^l
\end{bmatrix}\),
this is:

\begin{equation}
\Proj_{P_k}(\Bx) =
\left(P
\begin{bmatrix}
I_k & 0 \\
0   & 0 \\
\end{bmatrix}
Q^* A\right) \Bx
\end{equation}

Observe that the reciprocal frame vectors can be expressed as the rows of the matrix

\begin{equation*}
Q^* A = \inv{P^* A P} P^* A
\end{equation*}

Assuming the matrix of dot product is invertible, the reciprocal frame vectors are the columns of:
\begin{equation}
Q = P \inv{P^* A^* P}
\end{equation}

I had expect that the matrix of most dot product forms would also have

\(A = A^*\) (ie: Hermitian symmetric).

That is certainly true for all the vector dot products
I am interested in utilizing. ie: the standard euclidean dot product, Minkowski space time metrics,
and complex field vector space inner products (all of those are not only real symmetric, but are also
all diagonal).  For completeness, exploring this form for a more generalized
form of inner product was also explored in \chapcite{PJprojGen}.

\subsection{Calculation efficiency}

It would be interesting to compare the computational complexity for a set of reciprocal frame vectors calculated with the GA method (where overhat indicates omission) :

\begin{equation}\label{eqn:obliqueProj:340}
P^i = (-1)^{i-1} P_1 \wedge \cdots \hat{P_i} \cdots \wedge P_k \inv { P_1 \wedge P_2 \wedge \cdots \wedge P_k }
\end{equation}

The wedge in the denominator can be done just once for all the frame vectors.  Is there a way to use that for the numerator too (dividing out the wedge product with the vector in question)?

Calculation of the \(\inv{P^\T P}\) term could be avoided by using SVD.

Writing \(P = U \Sigma V^\T\), the reciprocal frame vectors will be \(Q = U \Sigma \inv{\Sigma^\T \Sigma} V^\T\).

Would that be any more efficient, or perhaps more importantly, for larger degree vectors is that a more numerically stable calculation?

\section{Followup}

Review: Have questionable GA algebra reduction earlier for grade \(>1\) (following \eqnref{eqn:oblique_proj:followup}).

Q: Can a directed frame vector projection be defined in terms of an ``oblique'' dot product.

Q: What applications would a non-diagonal bilinear form have?

Editorial: I have defined the inner product in matrix form with:

\begin{equation}\label{eqn:obliqueProj:360}
\innerprod{\Bu}{\Bv} = \Bu^* A \Bv
\end{equation}

This is slightly irregular since it is the conjugate of the normal complex
inner product, so in retrospect I would have been better to express things as:

\begin{equation}\label{eqn:obliqueProj:380}
\innerprod{\Bu}{\Bv} = \Bu^\T A \overbar{\Bv}
\end{equation}

Editorial: I have used the term oblique projection.  In retrospect I think I have really been describing what is closer to an affine (non-metric) projection so that would probably have been better to use.
