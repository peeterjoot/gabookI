%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Introductory concepts}
\label{chap:introGa}

%%%%%\section{My search for Geometric Algebra.}
%%%%%
%%%%%When you learned vector algebra initially, you learned how to add vectors and scale them, and you probably asked your teacher
%%%%%
%%%%%``How do we multiply vectors?''
%%%%%
%%%%%My teacher's response was something like:
%%%%%
%%%%%``It is impossible to multiply vectors, but some multiplication like operations can be defined.''
%%%%%
%%%%%This may have been followed with a lesson on the dot and cross product operators, or at least a mention that this topic would be covered later.
%%%%%
%%%%%You'll also learn how to generalize vectors from two and three dimensions to higher dimensions, and may learn to generalize vectors from real valued to complex valued.  The dot product generalizes nicely to higher dimensions and even complex valued vectors.  However, given the usefulness of the cross product, you probably find your self asking ``How does the cross product generalize?''
%%%%%
%%%%%The cross product is an explicitly three dimensional beast, and isn't even well defined in two.  You have to introduce a 3D normal direction to describe quanities like torque that are perfectly well defined in a plane.  When I was confronted with this oddity, my conclusion was that there must be a way to generalize the cross product to two dimensions or to greater than 3 dimenions.  A search for that generalization eventually led me to discover Geometric Algebra (with a stop over at differential forms on the way).
%%%%%
%%%%%Geometric Algebra answers the ``How do I multiply vectors?'' question and supplies the generalization of the cross product, among many other things.
%%%%%It also provides a mathematical toolbox that incorporates and extends many not-obviously related fields within mathematics.
%%%%%
%%%%%%Geometric Algebra is not the only answer to some of these questions.  The student of differential forms will know that the cross product can be found generalized using the wedge product of one-forms.  When I first found that, it was not obvious how to apply that generalization to many problems of geometry.  Vector objects have to be promoted to differential forms to apply the wedge product, even if a differential version of such an object does not make any sense.
%%%%%
%%%%%%Eventually, I blundered through an attempt of my own to generalize the cross product, and found a way that worked well for the generalized ``cross product'' of \( n -1 \) n-dimensional vectors.  Such a product worked nicely to define a normal to the \( n -1 \) dimensional subspace that was spanned by that set of vectors.  I was left wondering how to apply this generalization in other obvious contexts, such as a generalization of Stokes' Theorem, which is expressed in terms of the cross product in \R{3}.  I wasn't smart enough at the time to just go looking for existing generalizations of Stokes' theorem.  A friend, much smarter than I, did that search for me, and pointed me towards the field of differential forms which had a wedge product that generalized the cross product.
%%%%%
%%%%%%, and perhaps apply linear operators (i.e. matrices) to them.  One of your
%%%%%%one of your questions to the instructor was probably

Here is an attempt to provide a naturally sequenced introduction to Geometric Algebra.

\section{The Axioms}

Two basic axioms are required, contraction and associative multiplication respectively

\begin{equation}\label{eqn:introGa:40}
\begin{aligned}
a^2 &= \text{scalar} \\
a (b c) &= (a b) c = a b c
\end{aligned}
\end{equation}

Linearity and scalar multiplication should probably also be included for completeness, but even with those this is a surprisingly small set of rules.  The choice to impose these as the rules for vector multiplication will be seen to have a rich set of consequences once explored.  It will take a fair amount of work to extract all the consequences of this decision, and some of that will be done here.

\section{Contraction and the metric}

Defining \(a^2\) itself requires introduction of a metric, the specification of the multiplication rules for a particular basis for the vector space.  For Euclidean spaces, a requirement that

\begin{equation}\label{eqn:introGa:60}
\begin{aligned}
a^2 = \Abs{a}^2
\end{aligned}
\end{equation}

is sufficient to implicitly define this metric.  However, for the Minkowski spaces of special relativity one wants the squares of time and spatial basis vectors to be opposing in sign.  Deferring the discussion of metric temporarily one can work with the axioms above to discover their implications, and in particular how these relate to the coordinate vector space constructions that are so familiar.

\section{Symmetric sum of vector products}

Squaring a vector sum provides the first interesting feature of the general vector product

\begin{equation}\label{sumSquared}
\begin{aligned}
(a + b)^2 %&= (a + b)(a + b) \\
&= a^2 + b^2 + a b + b a
\end{aligned}
\end{equation}

Observe that the LHS is a scalar by the contraction identity, and on the RHS we have scalars \(a^2\) and \(b^2\) by the same.  This implies that the symmetric sum of products

\begin{equation}\label{eqn:introGa:80}
\begin{aligned}
a b + b a
\end{aligned}
\end{equation}

is also a scalar, independent of any choice of metric.  Symmetric sums of this form have a place in physics over the space of operators, often instantiated in matrix form.  There one writes this as the commutator and denotes it as

\begin{equation}\label{eqn:intro_ga:anticommutator}
\begin{aligned}
\symmetric{a}{b} \equiv a b + b a
\end{aligned}
\end{equation}

In an Euclidean space one can observe that equation \ref{sumSquared} has the same structure as the law of cosines so it should not be surprising that this symmetric sum is also related to the dot product.  For a Euclidean space where one the notion of perpendicularity can be expressed as

\begin{equation}\label{eqn:introGa:100}
\begin{aligned}
\Abs{ a + b }^2 = \Abs{a}^2 + \Abs{b}^2
\end{aligned}
\end{equation}

we can then see that an implication of the vector product is the fact that perpendicular vectors have the property

\begin{equation}\label{eqn:introGa:120}
\begin{aligned}
a b + ba = 0
\end{aligned}
\end{equation}

or

\begin{equation}\label{eqn:introGa:140}
\begin{aligned}
b a = - a b
\end{aligned}
\end{equation}

This notion of perpendicularity will also be seen to make sense for non-Euclidean spaces.

Although it retracts from a purist Geometric Algebra approach where things can be done in a coordinate free fashion, the connection between the symmetric product and the standard vector dot product can be most easily shown by considering an expansion with respect to an orthonormal basis.

Lets write two vectors in an orthonormal basis as

\begin{equation}\label{eqn:introGa:160}
\begin{aligned}
a &= \sum_\mu a^\mu e_\mu \\
b &= \sum_\mu b^\mu e_\mu
\end{aligned}
\end{equation}

Here the choice to utilize raised indices rather than lower for the coordinates is taken from physics where summation is typically implied when upper and lower indices are matched as above.

Forming the symmetric product we have

\begin{equation}\label{eqn:introGa:180}
\begin{aligned}
a b + b a
&=
\sum_{\mu,\nu} a^\mu e_\mu b^\nu e_\nu + b^\mu e_\mu a^\nu e_\nu \\
&=
\sum_{\mu,\nu} a^\mu b^\nu \left( e_\mu e_\nu + e_\nu e_\mu \right) \\
&=
2 \sum_{\mu} a^\mu b^\mu {e_\mu}^2 + \sum_{\mu \ne \nu} a^\mu b^\nu \left( e_\mu e_\nu + e_\nu e_\mu \right) \\
\end{aligned}
\end{equation}

For an Euclidean space we have \({e_\mu}^2 = 1\), and \(e_\nu e_\mu = -e_\mu e_\nu\), so we are left with

\begin{equation}\label{eqn:introGa:200}
\begin{aligned}
\sum_{\mu} a^\mu b^\mu = \inv{2} ( a b + b a)
\end{aligned}
\end{equation}

This shows that we can make an identification between the symmetric product, and the anticommutator of physics with the dot product, and then define

\begin{equation}\label{eqn:intro_ga:dotDefined}
\begin{aligned}
a \cdot b \equiv \inv{2} \symmetric{a}{b} = \inv{2} (a b + ba)
\end{aligned}
\end{equation}

\section{Antisymmetric product of two vectors (wedge product)}

Having identified or defined the symmetric product with the dot product we are now prepared to examine a general product of two vectors.  Employing a symmetric + antisymmetric decomposition we can write such a general product as

\begin{equation}\label{eqn:introGa:220}
\begin{aligned}
a b = \mathLabelBox{\inv{2}(a b + b a)}{\(a \cdot b\)} + \mathLabelBox{ \inv{2} ( a b - b a ) }{\(a\) something \(b\)}
\end{aligned}
\end{equation}

What is this remaining vector operation between the two vectors

\begin{equation}\label{eqn:introGa:240}
\begin{aligned}
a \something b = \inv{2} ( a b - b a )
\end{aligned}
\end{equation}

One can continue the comparison with the quantum mechanics, and like the
anticommutator operator that expressed our symmetric sum in equation
\eqnref{eqn:intro_ga:anticommutator} one can introduce a commutator operator

\begin{equation}\label{eqn:intro_ga:commutator}
\begin{aligned}
\antisymmetric{a}{b} \equiv a b - b a
\end{aligned}
\end{equation}

The commutator however, does not naturally extend to more than two vectors, so
as with the scalar part of the vector product (the dot product part),
it is desirable to make a different identification for this part of the vector
product.

One observation that we can make is that this vector operation changes sign when the operations are reversed.  We have

\begin{equation}\label{eqn:introGa:260}
\begin{aligned}
b \something a = \inv{2} ( b a - a b) = - a \something b
\end{aligned}
\end{equation}

Similarly, if \(a\) and \(b\) are colinear, say \(b = \alpha a\), this product is zero

\begin{equation}\label{eqn:introGa:280}
\begin{aligned}
a \something (\alpha a)
&= \inv{2} ( a  (\alpha a) - (\alpha a) a ) \\
&= 0
\end{aligned}
\end{equation}

This complete antisymmetry, aside from a potential difference in sign, are precisely the properties of the wedge product used in the mathematics of differential forms.  In this differential geometry the wedge product of \(m\) one-forms (vectors in this context) can be defined as

\begin{equation}\label{eqn:intro_ga:wedge}
\begin{aligned}
a_1 \wedge a_2 \cdots \wedge a_m
&= \inv{m!} \sum a_{i_1} a_{i_2} \cdots a_{i_m} \sgn(\pi(i_1 i_2 \cdots i_m))
\end{aligned}
\end{equation}

Here \(\sgn(\pi(\cdots))\) is the sign of the permutation of the indices.  While we have not gotten yet to products of more than two vectors it is helpful to know that the wedge product will have a place in such a general product.   An equation like \eqnref{eqn:intro_ga:wedge} makes a lot more sense after writing it out in full for a few specific cases.  For two vectors \(a_1\) and \(a_2\) this is

\begin{equation}\label{eqn:intro_ga:wedgeTwo}
\begin{aligned}
a_1 \wedge a_2 = \inv{2}
\left( a_1 a_2 (1) + a_2 a_1 (-1) \right)
\end{aligned}
\end{equation}

and for three vectors this is

\begin{equation}\label{eqn:introGa:300}
\begin{aligned}
a_1 \wedge a_2 \wedge a_3 = \inv{6}
(
&a_1 a_2 a_3 (1) + a_1 a_3 a_2 (-1) \\
+&a_2 a_1 a_3 (-1) + a_3 a_1 a_2 (1) \\
+&a_2 a_3 a_1 (1) + a_3 a_2 a_1 (-1) )
\end{aligned}
\end{equation}

We will see later that this completely antisymmetrized sum, the wedge product of differential forms will have an important place in this algebra, but like the dot product it is a specific construction of the more general vector product.  The choice to identify the antisymmetric sum with the wedge product is an action that amounts to a definition of the wedge product.  Explicitly, and complementing
the dot product definition of \eqnref{eqn:intro_ga:dotDefined} for the dot product
of two vectors, we say

\begin{equation}\label{eqn:intro_ga:wedgeDefined}
\begin{aligned}
a \wedge b \equiv \inv{2} \antisymmetric{a}{b} = \inv{2} ( a b - b a )
\end{aligned}
\end{equation}

Having made this definition, the symmetric and antisymmetric decomposition of two vectors leaves us with a peculiar looking hybrid construction:

\begin{equation}\label{eqn:intro_ga:dotPlusWedge}
\begin{aligned}
a b %&= \inv{2} (a b + b a) + \inv{2} ( a b - b a ) \\
&= a \cdot b + a \wedge b
\end{aligned}
\end{equation}

We had already seen that part of this vector product was not a vector, but was in fact a scalar.  We now see that the remainder is also not a vector but is instead something that resides in a different space.  In differential geometry this object is called a two form, or a simple element in \(\bigwedge^2\).  Various labels are available for this object are available in Geometric (or Clifford) algebra, one of which is a 2-blade.  2-vector or bivector is also used in some circumstances, but in dimensions greater than three there are reasons to reserve these labels for a slightly more general construction.

The definition of \eqnref{eqn:intro_ga:dotPlusWedge} is often used as the starting point in Geometric Algebra introductions.  While there is value to this approach I have personally found that the non-axiomatic approach becomes confusing if one attempts to sort out which of the many identities in the algebra are the fundamental ones.  That is why my preference is to treat this as a consequence rather than the starting point.

\section{Expansion of the wedge product of two vectors}

Many introductory geometric algebra treatments try very hard to avoid explicit coordinate treatment.  It is true that GA provides infrastructure for coordinate free treatment, however, this avoidance perhaps contributes to making the subject less accessible.  Since we are so used to coordinate geometry in vector and tensor algebra, let us take advantage of this comfort, and express the wedge product explicitly in coordinate form to help get some comfort for it.

Employing the definition of \eqnref{eqn:intro_ga:wedgeDefined}, and an orthonormal basis expansion in coordinates for two vectors \(a\), and \(b\), we have

\begin{equation}\label{eqn:introGa:320}
\begin{aligned}
2 (a \wedge b)
&= ( a b - b a ) \\
&=
\sum_{\mu,\nu} a^\mu b^\nu e_\mu e_\nu
-\sum_{\alpha,\beta} a^\alpha b^\beta e_\alpha e_\beta \\
&=
\mathLabelBox{\sum_{\mu} a^\mu b^\mu - \sum_{\alpha} a^\alpha b^\alpha }{\(=0\)}
+ \sum_{\mu \ne \nu} a^\mu b^\nu e_\mu e_\nu
- \sum_{\alpha \ne \beta} a^\alpha b^\beta e_\alpha e_\beta \\
&=
\sum_{\mu < \nu} (a^\mu b^\nu e_\mu e_\nu + a^\nu b^\mu e_\nu e_\mu)
- \sum_{\alpha < \beta} (a^\alpha b^\beta e_\alpha e_\beta + a^\beta b^\alpha e_\beta e_\alpha )
\\
&=
2 \sum_{\mu < \nu} ( a^\mu b^\nu - a^\nu b^\mu ) e_\mu e_\nu
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:introGa:340}
\begin{aligned}
a \wedge b
&= \sum_{\mu < \nu} \uDETuvij{a}{b}{\mu}{\nu} e_\mu e_\nu
\end{aligned}
\end{equation}

The similarity to the \R{3} vector cross product is not accidental.  This similarity can be made explicit by observing the following

\begin{equation}\label{eqn:introGa:360}
\begin{aligned}
e_1 e_2 &= e_1 e_2 (e_3 e_3) = (e_1 e_2 e_3) e_3 \\
e_2 e_3 &= e_2 e_3 (e_1 e_1) = (e_1 e_2 e_3) e_1 \\
e_1 e_3 &= e_1 e_3 (e_2 e_2) = -(e_1 e_2 e_3) e_2 \\
\end{aligned}
\end{equation}

This common factor, a product of three normal vectors, or grade three blade, is called the pseudoscalar for \R{3}.  We write
\(i = e_1 e_2 e_3\), and can then express the \R{3} wedge product in terms of the cross product

\begin{equation}\label{eqn:introGa:380}
\begin{aligned}
a \wedge b
&=
\uDETuvij{a}{b}{2}{3} e_2 e_3
+\uDETuvij{a}{b}{1}{3} e_1 e_3
+\uDETuvij{a}{b}{1}{2} e_1 e_2  \\
&=
(e_1 e_2 e_3) \left( \uDETuvij{a}{b}{2}{3} e_1
-\uDETuvij{a}{b}{1}{3} e_2
+\uDETuvij{a}{b}{1}{2} e_3 \right) \\
\end{aligned}
\end{equation}

This is

\begin{equation}\label{eqn:intro_ga:wedgeAsCross}
\begin{aligned}
a \wedge b &= i (a \cross b)
\end{aligned}
\end{equation}

With this identification we now also have a curious integrated relation where the dot and cross products are united into
a single structure

\begin{equation}\label{eqn:intro_ga:scalarPlusIcross}
\begin{aligned}
a b = a \cdot b + i (a \cross b)
\end{aligned}
\end{equation}

\section{Vector product in exponential form}

One naturally expects there is an inherent connection between the dot and cross products, especially when expressed in terms of
the angle between the vectors, as in

\begin{equation}\label{eqn:introGa:400}
\begin{aligned}
a \cdot b &= \Abs{a}\Abs{b} \cos\theta_{a,b} \\
a \cross b &= \Abs{a}\Abs{b} \sin\theta_{a,b} \ncap_{a,b}
\end{aligned}
\end{equation}

However, without the structure of the geometric product the specifics of what
connection is is not obvious.  In particular the use of \eqnref{eqn:intro_ga:scalarPlusIcross} and the angle relations, one can easily
blunder upon the natural complex structure of the geometric product

\begin{equation}\label{eqn:introGa:420}
\begin{aligned}
a b
&= a \cdot b + i (a \cross b) \\
&=
\Abs{a}\Abs{b} \left( \cos\theta_{a,b} + i\ncap_{a,b} \sin\theta_{a,b} \right) \\
\end{aligned}
\end{equation}

As we have seen pseudoscalar multiplication in \R{3} provides a mapping between a grade 2 blade and a vector, so
this \(i\ncap\) product is a 2-blade.

In \R{3} we also have \(i \ncap = \ncap i\) (exercise for reader) and also \(i^2 = -1\) (again for the reader), so this
2-blade \(i\ncap\) has all the properties of the \(i\) of complex arithmetic.  We can, in fact, write

\begin{equation}\label{eqn:introGa:440}
\begin{aligned}
a b
&= a \cdot b + i (a \cross b) \\
&=
\Abs{a}\Abs{b} \exp( i\ncap_{a,b} \theta_{a,b} )
\end{aligned}
\end{equation}

In particular, for unit vectors \(a\), \(b\) one is able to quaternion exponentials of this form to rotate from one vector to the other

\begin{equation}\label{eqn:introGa:460}
\begin{aligned}
b &= a \exp( i\ncap_{a,b} \theta_{a,b} )
\end{aligned}
\end{equation}

This natural GA use
of multivector exponentials to implement rotations is not restricted to \R{3} or even Euclidean space, and is one of the most
powerful features of the algebra.

\section{Pseudoscalar}

In general the
pseudoscalar for \R{N} is a product of \(N\) normal vectors and multiplication by such an object maps m-blades to (N-m) blades.

For \R{2} the unit pseudoscalar has a negative square

\begin{equation}\label{eqn:introGa:480}
\begin{aligned}
(e_1 e_2) (e_1 e_2)
&=
- (e_2 e_1) (e_1 e_2) \\
&=
- e_2 (e_1 e_1) e_2 \\
&=
- e_2 e_2 \\
&=
-1
\end{aligned}
\end{equation}

and we have seen an example of such a planar pseudoscalar in the subspace of the span of two vectors above (where \(\ncap i\) was a pseudoscalar
for that subspace).  In general the sign of the square of the pseudoscalar depends on both the dimension and the metric of the space,
so the ``complex'' exponentials that rotate one vector into another may represent hyperbolic rotations.

For example we have for a four dimensional space the pseudoscalar square is

\begin{equation}\label{eqn:introGa:500}
\begin{aligned}
i^2 &=
(e_0 e_1 e_2 e_3) (e_0 e_1 e_2 e_3) \\
&=
- e_0 e_0 e_1 e_2 e_3 e_1 e_2 e_3 \\
&=
- e_0 e_0 e_1 e_2 e_3 e_1 e_2 e_3 \\
&=
- e_0 e_0 e_1 e_1 e_2 e_3 e_2 e_3 \\
&=
e_0 e_0 e_1 e_1 e_2 e_2 e_3 e_3 \\
\end{aligned}
\end{equation}

For a Euclidean space where each of the \({e_k}^2 = 1\), we have \(i^2 = 1\), but for a Minkowski space where one would have for \(k\ne0\), \({e_0}^2 {e_k}^2 = -1\), we have \(i^2 = -1\)

Such a mixed signature metric will allow for implementation of Lorentz transformations as exponentials (hyperbolic) rotations
in a fashion very much like the quaternionic spatial rotations for Euclidean spaces.

It is also worth pointing out that the pseudoscalar multiplication naturally provides a mapping operator into a dual space, as we have seen
in the cross product example, mapping vectors to bivectors, or bivectors to vectors.  Pseudoscalar multiplication in fact provides an
implementation of the Hodge duality operation of differential geometry.

In higher than three dimensions, such as four, this duality operation can in fact map 2-blades to orthogonal 2-blades (orthogonal in the sense
of having no common factors).  Take for example the typical example of a non-simple element from differential geometry

\begin{equation}\label{eqn:introGa:520}
\begin{aligned}
\omega = e_1 \wedge e_2 + e_3 \wedge e_4
\end{aligned}
\end{equation}

The two blades that compose this sum have no common factors and thus cannot be formed as the wedge product of two vectors.  These two blades
are orthogonal in a sense that can be made more exact later.   As this time we just wish to make the observation that
the pseudoscalar provides a natural duality operation between these two subspaces of \(\bigwedge^2\).  Take for example

\begin{equation}\label{eqn:introGa:540}
\begin{aligned}
i e_1 \wedge e_2
&=
 e_1 e_2 e_3 e_4 e_1 e_2  \\
&=
- e_1 e_1 e_2 e_3 e_4 e_2  \\
&=
- e_1 e_1 e_2 e_2 e_3 e_4 \\
&\propto
e_3 e_4 \\
\end{aligned}
\end{equation}

\section{FIXME: orphaned}
As an exercise work out axiomatically some of the key vector identities of Geometric Algebra.

Want to at least derive the vector bivector dot product distribution
identity

\begin{equation}\label{eqn:introGa:20}
\begin{aligned}
a \cdot ( b \wedge c) = (a \cdot b) c - (a \cdot c) b
\end{aligned}
\end{equation}

%\section{Higher order products}
