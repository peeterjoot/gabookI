%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Blade grade reduction}
\index{grade reduction}
\label{chap:bladegradereduction}
%\date{Mar 25, 2008.  bladegradereduction.tex}

\section{General triple product reduction formula}

Consideration of the reciprocal frame bivector decomposition required the following identity

\begin{equation}
(\BA_a \wedge \BA_b) \cdot \BA_c =
\BA_a \cdot (\BA_b \cdot \BA_c)
\end{equation}

This holds when \(a + b \le c\), and \(a <= b\).  Similar equations for vector wedge blade dot blade reduction can be found in NFCM, but intuition let me to believe the above generalization was valid.

To prove this use the definition of the generalized dot product of two blades:

\begin{equation}\label{eqn:bladegradereduction:282}
\begin{aligned}
(\BA_a \wedge \BA_b) \cdot \BA_c
&= \gpgrade{ (\BA_a \wedge \BA_b) \BA_c }{\abs{c-(a+b)}} \\
\end{aligned}
\end{equation}

The subsequent discussion
is restricted to the \(b \ge a\) case.  Would have to think whether this restriction is required.

\begin{equation}
\label{eqn:bladegradereduction:bladewedge}
\begin{aligned}
\BA_a \wedge \BA_b
&= \BA_a \BA_b - \sum_{i=\abs{b-a},i+=2}^{a+b}\gpgrade{\BA_a\BA_b}{i} \\
&= \BA_a \BA_b - \sum_{k=0}^{a-1}\gpgrade{\BA_a\BA_b}{2k + b - a} \\
\end{aligned}
\end{equation}

Back substitution gives:

\begin{equation}\label{eqn:bladegradereduction:322}
\begin{aligned}
\gpgrade{ (\BA_a \wedge \BA_b) \BA_c }{\abs{c-(a+b)}}
&=
\gpgrade{ \BA_a \BA_b \BA_c }{\abs{c-(a+b)}}
-
\sum_{k=0}^{a-1}
\gpgrade{ \gpgrade{\BA_a\BA_b}{2k + b - a} \BA_c }{c-a-b}
\end{aligned}
\end{equation}

Temporarily writing \(\gpgrade{\BA_a\BA_b}{2k + b - a} = \BC_i\),
\begin{equation}\label{eqn:bladegradereduction:342}
\begin{aligned}
\gpgrade{\BA_a\BA_b}{2k + b - a} \BA_c
&= \sum_{j=c-i,j+=2}^{c+i} \gpgrade{ \BC_i \BA_c }{j} \\
&= \sum_{r=0}^{i} \gpgrade{ \BC_i \BA_c }{c-i+2r} \\
&= \sum_{r=0}^{2k+b-a} \gpgrade{ \BC_i \BA_c }{c-2k-b+a+2r} \\
&= \sum_{r=0}^{2k+b-a} \gpgrade{ \BC_i \BA_c }{c-b+a +2(r-k)} \\
\end{aligned}
\end{equation}

We want the only the following grade terms:

\begin{equation}\label{eqn:bladegradereduction:42}
c-b+a+2(r-k) = c - b - a
\implies
r=k-a
\end{equation}

There are many such \(k,r\) combinations, but we have a \(k \in [0,a-1]\) constraint, which implies \(r \in [-a,-1]\).  This contradicts with \(r\) strictly
positive,
so there are no such grade elements.

This gives an intermediate result, the reduction of the triple product to a direct product, removing the explicit wedge:

\begin{equation}
(\BA_a \wedge \BA_b) \cdot \BA_c =
\gpgrade{\BA_a \BA_b \BA_c}{c-a-b}
\end{equation}

\begin{equation}\label{eqn:bladegradereduction:362}
\begin{aligned}
\gpgrade{\BA_a \BA_b \BA_c}{c-a-b}
&= \gpgrade{\BA_a (\BA_b \BA_c)}{c-a-b} \\
&= \gpgrade{\BA_a \sum_{i}\gpgrade{\BA_b \BA_c}{i}}{c-a-b} \\
&= \gpgrade{\sum_{j}\gpgrade{\BA_a \sum_{i}\gpgrade{\BA_b \BA_c}{i}}{j}}{c-a-b} \\
\end{aligned}
\end{equation}

Explicitly specifying the grades here is omitted for simplicity.  The lowest grade of these is \((c-b)-a\), and all others are higher,
so grade selection excludes them.

By definition

\begin{equation}\label{eqn:bladegradereduction:62}
\gpgrade{\BA_b \BA_c}{c-b} = \BA_b \cdot \BA_c
\end{equation}

so that lowest grade term is thus

\begin{equation}\label{eqn:bladegradereduction:82}
\gpgrade{\BA_a \gpgrade{\BA_b \BA_c}{c-b}}{c-a-b}
= \gpgrade{\BA_a (\BA_b \cdot \BA_c)}{c-a-b}
= \BA_a \cdot (\BA_b \cdot \BA_c)
\end{equation}

This completes the proof.

\section{reduction of grade of dot product of two blades}

The result above can be applied to reducing the dot product of two blades.  For \(k<=s\):

\begin{equation}\label{eqn:bladegradereduction:102}
(\Ba_1 \wedge \Ba_2 \wedge \Ba_3 \cdots \wedge \Ba_k) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_s)
\end{equation}
\begin{equation}\label{eqn:bladegradereduction:382}
\begin{aligned}
&= (\Ba_1 \wedge (\Ba_2 \wedge \Ba_3 \cdots \wedge \Ba_k)) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_s) \\
&= (\Ba_1 \cdot ((\Ba_2 \wedge \Ba_3 \cdots \wedge \Ba_k)) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_s)) \\
&= (\Ba_1 \cdot (\Ba_2 \cdot (\Ba_3 \cdots \wedge \Ba_k)) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_s)) \\
&= \cdots \\
&= \Ba_1 \cdot (\Ba_2 \cdot (\Ba_3 \cdot (\cdots \cdot (\Ba_k \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_s))))) \\
\end{aligned}
\end{equation}

This can be reduced to a single determinant, as is done in
the Flanders' differential forms book definition of the
\({\bigwedge}^k\) inner product (which is then used to define the Hodge dual).

The first such product is:

\begin{equation}\label{eqn:bladegradereduction:122}
\Ba_k \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)
= \sum (-1)^{u-1} (\Ba_k \cdot \Bb_u) \Bb_1 \wedge \cdots \check{\Bb_u} \cdots \wedge \Bb_k
\end{equation}

Next, take dot product with \(\Ba_{k-1}\):

\begin{enumerate}
\item \(k = 2\)

\begin{equation}\label{eqn:bladegradereduction:402}
\begin{aligned}
&\Ba_{k-1} \cdot (\Ba_k \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)) \\
&= \sum_{v \ne u} (-1)^{u-1} (\Ba_k \cdot \Bb_u) (\Ba_1 \cdot \Bb_v) \\
&=
 \sum_{u < v} (-1)^{v-1} (\Ba_k \cdot \Bb_v) (\Ba_1 \cdot \Bb_u)
+\sum_{u < v} (-1)^{u-1} (\Ba_k \cdot \Bb_u) (\Ba_1 \cdot \Bb_v) \\
&=
+\sum_{u < v} (\Ba_k \cdot \Bb_u) (\Ba_1 \cdot \Bb_v)
-\sum_{u < v} (\Ba_k \cdot \Bb_v) (\Ba_1 \cdot \Bb_u) \\
&=
+\sum_{u< v} (\Ba_k \cdot \Bb_u) (\Ba_1 \cdot \Bb_v)
- (\Ba_k \cdot \Bb_v) (\Ba_1 \cdot \Bb_u) \\
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:bladegradereduction:k2dot}
-\sum_{u< v}
\begin{vmatrix}
\Ba_{k-1} \cdot \Bb_u & \Ba_{k-1} \cdot \Bb_v \\
\Ba_k \cdot \Bb_u & \Ba_k \cdot \Bb_v \\
\end{vmatrix}
\end{equation}

\item \(k>2\)
\end{enumerate}

\begin{equation}\label{eqn:bladegradereduction:142}
\Ba_{k-1} \cdot (\Ba_k \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k))
\end{equation}
\begin{equation}\label{eqn:bladegradereduction:422}
\begin{aligned}
&= \sum (-1)^{u-1} (\Ba_k \cdot \Bb_u) \Ba_{k-1} \cdot (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \wedge \Bb_k) \\
&= \sum_{v<u} (-1)^{u-1} (\Ba_k \cdot \Bb_u) (-1)^{v-1} (\Ba_{k-1} \cdot \Bb_v) (\Bb_1 \wedge \cdots \check{\Bb_v} \cdots \check{\Bb_u} \cdots \wedge \Bb_k) \\
&+ \sum_{v>u} (-1)^{u-1} (\Ba_k \cdot \Bb_u) (-1)^{v} (\Ba_{k-1} \cdot \Bb_v) (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k) \\
\end{aligned}
\end{equation}

Add negation exponents, and use a change of variables for the first sum
\begin{equation}\label{eqn:bladegradereduction:442}
\begin{aligned}
&= \sum_{u<v} (-1)^{v+u} (\Ba_k \cdot \Bb_v) (\Ba_{k-1} \cdot \Bb_u) (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k) \\
&- \sum_{u<v} (-1)^{u+v} (\Ba_k \cdot \Bb_u) (\Ba_{k-1} \cdot \Bb_v) (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k) \\
\end{aligned}
\end{equation}

Merge sums:
\begin{equation}\label{eqn:bladegradereduction:462}
\begin{aligned}
&= \sum_{u<v} (-1)^{u+v}
\left(
(\Ba_k \cdot \Bb_v) (\Ba_{k-1} \cdot \Bb_u)
-(\Ba_k \cdot \Bb_u) (\Ba_{k-1} \cdot \Bb_v)
\right) \\
& \; (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k)
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:bladegradereduction:bivectordotkvector}
\Ba_{k-1} \cdot (\Ba_k \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k))
=
\end{equation}
\begin{equation*}
\sum_{u<v} (-1)^{u+v}
\begin{vmatrix}
\Ba_{k-1} \cdot \Bb_u & \Ba_{k-1} \cdot \Bb_v \\
\Ba_k \cdot \Bb_u & \Ba_k \cdot \Bb_v \\
\end{vmatrix}
(\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k) \\
\end{equation*}

Note that special casing \(k=2\) does not seem to be required because in that
case \(-1^{u+v} = -1^{1+2}=-1\), so this is identical to \eqnref{eqn:bladegradereduction:k2dot} after all.

\subsection{Pause to reflect}

Although my initial aim was to show that \(\BA_k \cdot \BB_k\) could be
expressed as a determinant as in the differential forms book (different
sign though), and to determine exactly what that determinant is, there
are some useful identities that fall out of this even just for this
bivector kvector dot product expansion.

Here is a summary of some of the things figured out so far

\begin{enumerate}
\item Dot product of grade one blades.

Here we have a result that can be expressed as a one by one determinant.  Worth mentioning to explicitly show the sign.

\begin{equation}\label{eqn:bladegradereduction:dotoneblades}
\Ba \cdot \Bb = \det[\Ba \cdot \Bb]
\end{equation}

%(Used \(\det{}\) here instead of \(\Det{}\) to avoid confusing with absolute value).
\item Dot product of grade two blades.

\begin{equation}\label{eqn:bladegradereduction:k2k2dot}
(\Ba_1 \wedge \Ba_2) \cdot (\Bb_1 \wedge \Bb_2)
=
-
\begin{vmatrix}
\Ba_1 \cdot \Bb_1 & \Ba_1 \cdot \Bb_2 \\
\Ba_2 \cdot \Bb_1 & \Ba_2 \cdot \Bb_2 \\
\end{vmatrix}
=
-\det[\Ba_i \cdot \Bb_j]
\end{equation}

\item Dot product of grade two blade with grade \(>2\) blade.

\begin{equation*}
(\Ba_{1} \wedge \Ba_2) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:bivectordot}
=
\sum_{u<v} (-1)^{u+v-1}
(\Ba_1 \wedge \Ba_2) \cdot (\Bb_u \wedge \Bb_v)
(\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k)
\end{equation}
\end{enumerate}

Observe how similar this is to the vector blade dot product expansion:

\begin{equation}\label{eqn:bladegradereduction:vectordot}
\Ba \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)
=
\sum (-1)^{i-1}
(\Ba \cdot \Bb_i) (\Bb_1 \wedge \cdots \check{\Bb_i} \cdots \wedge \Bb_k)
\end{equation}

\subsubsection{Expand it for \texorpdfstring{\(k=3\)}{k equal 3}}

Explicit expansion of \eqnref{eqn:bladegradereduction:bivectordot} for the \(k=3\) case, is also helpful to get a feel for
the equation:

\begin{equation}\label{eqn:bladegradereduction:482}
\begin{aligned}
(\Ba_{1} \wedge \Ba_2) \cdot (\Bb_1 \wedge \Bb_2 \wedge \Bb_3)
&=
(\Ba_1 \wedge \Ba_2) \cdot (\Bb_1 \wedge \Bb_2) \Bb_3 \\
&+(\Ba_1 \wedge \Ba_2) \cdot (\Bb_3 \wedge \Bb_1) \Bb_2 \\
&+(\Ba_1 \wedge \Ba_2) \cdot (\Bb_2 \wedge \Bb_3) \Bb_1
\end{aligned}
\end{equation}

Observe the cross product like alternation in sign and indices.
This suggests that a more natural way to express the sign coefficient may be via a \(\Sgn(\pi)\) expression for the sign of the
permutation of indices.

\section{trivector dot product}

With the result of \eqnref{eqn:bladegradereduction:bivectordot}, or the earlier equivalent determinant expression in equation
\eqnref{eqn:bladegradereduction:bivectordotkvector} we are now in a position to evaluate the dot product of a trivector and a greater or equal grade blade.

\begin{equation*}
\Ba_1 \cdot ((\Ba_{2} \wedge \Ba_3) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k))
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:502}
\begin{aligned}
&=
\sum_{u<v} (-1)^{u+v-1}
(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v)
\Ba_1 \cdot (\Bb_1 \wedge \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k)  \\
&=
\sum_{w<u<v} (-1)^{u+v+w}
(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v)
(\Ba_1 \cdot \Bb_w) (\Bb_1 \wedge \cdots \check{\Bb_w} \cdots \check{\Bb_u} \cdots \check{\Bb_v} \cdots \wedge \Bb_k)  \\
&+\sum_{u<w<v} (-1)^{u+v+w-1}
(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v)
(\Ba_1 \cdot \Bb_w) (\Bb_1 \wedge \cdots \check \Bb_u \cdots \check{\Bb_w} \cdots \check{\Bb_v} \cdots \wedge \Bb_k)  \\
&+\sum_{u<v<w} (-1)^{u+v+w}
(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v)
(\Ba_1 \cdot \Bb_w) (\Bb_1 \wedge \cdots \check \Bb_u \cdots \check{\Bb_v} \cdots \check{\Bb_w} \cdots \wedge \Bb_k)  \\
\end{aligned}
\end{equation}

Change the indices of summation and grouping like terms we have:
\begin{equation}\label{eqn:bladegradereduction:522}
\begin{aligned}
\sum_{u<v<w} (-1)^{u+v+w}
(
&(\Ba_2 \wedge \Ba_3) \cdot (\Bb_v \wedge \Bb_w) (\Ba_1 \cdot \Bb_u)  \\
&-(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_w) (\Ba_1 \cdot \Bb_v)  \\
&+(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v) (\Ba_1 \cdot \Bb_w)  \\
)
(\Bb_1 \wedge \cdots \check \Bb_u \cdots \check{\Bb_v} \cdots \check{\Bb_w} \cdots \wedge \Bb_k)  \\
\end{aligned}
\end{equation}

Now, each of the embedded dot products were in fact determinants:
\begin{equation}\label{eqn:bladegradereduction:162}
(\Ba_2 \wedge \Ba_3) \cdot (\Bb_x \wedge \Bb_y)
=
-
\begin{vmatrix}
\Ba_2 \cdot \Bb_x & \Ba_2 \cdot \Bb_y \\
\Ba_3 \cdot \Bb_x & \Ba_3 \cdot \Bb_y \\
\end{vmatrix}
\end{equation}

Thus, we can expand these triple dot products like so (factor of \(-1\) omitted):
\begin{equation}\label{eqn:bladegradereduction:542}
\begin{aligned}
&(\Ba_2 \wedge \Ba_3) \cdot (\Bb_v \wedge \Bb_w) (\Ba_1 \cdot \Bb_u) \\
&-(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_w) (\Ba_1 \cdot \Bb_v) \\
&+(\Ba_2 \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v) (\Ba_1 \cdot \Bb_w)  \\
&=
(\Ba_1 \cdot \Bb_u)
\begin{vmatrix}
\Ba_2 \cdot \Bb_v & \Ba_2 \cdot \Bb_w \\
\Ba_3 \cdot \Bb_v & \Ba_3 \cdot \Bb_w \\
\end{vmatrix} \\
&-
(\Ba_1 \cdot \Bb_v)
\begin{vmatrix}
\Ba_2 \cdot \Bb_u & \Ba_2 \cdot \Bb_w \\
\Ba_3 \cdot \Bb_u & \Ba_3 \cdot \Bb_w \\
\end{vmatrix} \\
&+
(\Ba_1 \cdot \Bb_w)
\begin{vmatrix}
\Ba_2 \cdot \Bb_u & \Ba_2 \cdot \Bb_v \\
\Ba_3 \cdot \Bb_u & \Ba_3 \cdot \Bb_v \\
\end{vmatrix} \\
%&=
%\begin{vmatrix}
%\Ba_1 \cdot \Bb_u & 0 & 0 \\
%0 & \Ba_2 \cdot \Bb_v & \Ba_2 \cdot \Bb_w \\
%0 & \Ba_3 \cdot \Bb_v & \Ba_3 \cdot \Bb_w \\
%\end{vmatrix} \\
%&+
%\begin{vmatrix}
%0 & \Ba_1 \cdot \Bb_v & 0 \\
%\Ba_2 \cdot \Bb_u & 0 & \Ba_2 \cdot \Bb_w \\
%\Ba_3 \cdot \Bb_u & 0 & \Ba_3 \cdot \Bb_w \\
%\end{vmatrix} \\
%&+
%\begin{vmatrix}
%0 & 0 & \Ba_1 \cdot \Bb_w \\
%\Ba_2 \cdot \Bb_u & \Ba_2 \cdot \Bb_v & 0 \\
%\Ba_3 \cdot \Bb_u & \Ba_3 \cdot \Bb_v & 0 \\
%\end{vmatrix} \\
&=
\begin{vmatrix}
\Ba_1 \cdot \Bb_u & \Ba_1 \cdot \Bb_v & \Ba_1 \cdot \Bb_w \\
\Ba_2 \cdot \Bb_u & \Ba_2 \cdot \Bb_v & \Ba_2 \cdot \Bb_w \\
\Ba_3 \cdot \Bb_u & \Ba_3 \cdot \Bb_v & \Ba_3 \cdot \Bb_w \\
\end{vmatrix} \\
\end{aligned}
\end{equation}

Final back substitution gives:

\begin{equation*}
(\Ba_1 \wedge \Ba_{2} \wedge \Ba_3) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:trivectordotdet}
=
\sum_{u<v<w} (-1)^{u+v+w-1}
\begin{vmatrix}
\Ba_1 \cdot \Bb_u & \Ba_1 \cdot \Bb_v & \Ba_1 \cdot \Bb_w \\
\Ba_2 \cdot \Bb_u & \Ba_2 \cdot \Bb_v & \Ba_2 \cdot \Bb_w \\
\Ba_3 \cdot \Bb_u & \Ba_3 \cdot \Bb_v & \Ba_3 \cdot \Bb_w \\
\end{vmatrix}
(\Bb_1 \wedge \cdots \check \Bb_u \cdots \check{\Bb_v} \cdots \check{\Bb_w} \cdots \wedge \Bb_k)  \\
\end{equation}

In particular for \(k=3\) we have
\begin{equation*}
(\Ba_1 \wedge \Ba_{2} \wedge \Ba_3) \cdot (\Bb_1 \wedge \Bb_2 \wedge \Bb_3)
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:trivectordotdettri}
=
-\begin{vmatrix}
\Ba_1 \cdot \Bb_1 & \Ba_1 \cdot \Bb_2 & \Ba_1 \cdot \Bb_3 \\
\Ba_2 \cdot \Bb_1 & \Ba_2 \cdot \Bb_2 & \Ba_2 \cdot \Bb_3 \\
\Ba_3 \cdot \Bb_1 & \Ba_3 \cdot \Bb_2 & \Ba_3 \cdot \Bb_3 \\
\end{vmatrix}
=
-\det[\Ba_i \cdot \Bb_j]
\end{equation}

This can be substituted back into \eqnref{eqn:bladegradereduction:trivectordotdet} to put it in a non determinant form.

\begin{equation*}
(\Ba_1 \wedge \Ba_{2} \wedge \Ba_3) \cdot (\Bb_1 \wedge \Bb_2 \cdots \wedge \Bb_k)
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:trivectordotnondet}
=
\sum_{u<v<w} (-1)^{u+v+w}
(\Ba_1 \wedge \Ba_{2} \wedge \Ba_3) \cdot (\Bb_u \wedge \Bb_v \wedge \Bb_w)
(\Bb_1 \wedge \cdots \check \Bb_u \cdots \check{\Bb_v} \cdots \check{\Bb_w} \cdots \wedge \Bb_k)  \\
\end{equation}

\section{Induction on the result}

It is pretty clear that recursively performing these calculations will yield similar determinant and inner dot product reduction
results.

\subsection{dot product of like grade terms as determinant}

Let us consider the equal grade case first, summarizing the results so far

\begin{equation}\label{eqn:bladegradereduction:562}
\begin{aligned}
\Ba \cdot \Bb &= \det[\Ba \cdot \Bb] \\
(\Ba_1 \wedge \Ba_2) \cdot (\Bb_1 \wedge \Bb_2) &= -\det[\Ba_i \cdot \Bb_j] \\
(\Ba_1 \wedge \Ba_2 \wedge \Ba_3) \cdot (\Bb_1 \wedge \Bb_2 \wedge \Bb_3) &= -\det[\Ba_i \cdot \Bb_j] \\
\end{aligned}
\end{equation}

What will the sign be for the higher grade equivalents?  It has the appearance of being related to the sign associated with blade
reversion.  To verify this calculate the dot product of a blade formed from a set of perpendicular unit vectors with itself.

\begin{equation}\label{eqn:bladegradereduction:582}
\begin{aligned}
&(\Be_1 \wedge \cdots \wedge \Be_k) \cdot (\Be_1 \wedge \Be_2 \wedge \cdots \wedge \Be_k) \\
&= (-1)^{k(k-1)/2}(\Be_1 \wedge \cdots \wedge \Be_k) \cdot (\Be_k \wedge \cdots \wedge \Be_2 \wedge \Be_1) \\
&= (-1)^{k(k-1)/2}\Be_1 \cdot (\Be_2 \cdots (\Be_k \cdot (\Be_k \wedge \cdots \wedge \Be_2 \wedge \Be_1))) \\
&= (-1)^{k(k-1)/2}\Be_1 \cdot (\Be_2 \cdots (\Be_{k-1} \cdot (\Be_{k-1} \wedge \cdots \wedge \Be_2 \wedge \Be_1))) \\
&= \cdots \\
&= (-1)^{k(k-1)/2}
\end{aligned}
\end{equation}

This fixes the sign, and provides the induction hypothesis for the general case:

\begin{equation}\label{eqn:bladegradereduction:bladedothyp}
(\Ba_1 \wedge \cdots \wedge \Ba_k) \cdot (\Bb_1 \wedge \Bb_2 \wedge \cdots \wedge \Bb_k) = (-1)^{k(k-1)/2}\det[\Ba_i \cdot \Bb_j]
\end{equation}

Alternately, one can remove the sign change coefficient with reversion of one of the blades:

\begin{equation}\label{eqn:bladegradereduction:bladedothyprev}
(\Ba_1 \wedge \cdots \wedge \Ba_k) \cdot (\Bb_k \wedge \Bb_{k-1} \wedge \cdots \wedge \Bb_1) = \det[\Ba_i \cdot \Bb_j]
\end{equation}

\subsection{Unlike grades}

Let us summarize the results for unlike grades at the same time reformulating the previous results in terms of index
permutation, also writing for brevity \(\BA_s = \Ba_1 \wedge \cdots \wedge \Ba_s\), and \(\BB_k = \Bb_1 \wedge \cdots \wedge \Bb_k\):

\begin{equation}\label{eqn:bladegradereduction:182}
\BA_1 \cdot \BB_k =
\sum_i \Sgn(\pi(i,1,2,\cdots\check{i}\cdots,k)) (\BA_1 \cdot \Bb_i) (\Bb_1 \wedge \cdots \check{\Bb_i} \cdots \wedge \Bb_k)
\end{equation}

\begin{equation}\label{eqn:bladegradereduction:202}
\BA_2 \cdot \BB_k =
\sum_{i_1<i_2} \Sgn(\pi(i_1,i_2,1,2,\cdots\check{i_1}\cdots\check{i_2}\cdots,k))
\end{equation}
\begin{equation}\label{eqn:bladegradereduction:222}
   \BA_2 \cdot (\Bb_{i_1} \wedge \Bb_{i_2})
   (\Bb_1 \wedge \cdots \check{\Bb_{i_1}} \cdots \check{\Bb_{i_2}} \cdots \wedge \Bb_k)
\end{equation}

\begin{equation}\label{eqn:bladegradereduction:242}
\BA_3 \cdot \BB_k =
\sum_{i_1<i_2<i_3} \Sgn(\pi(i_1,i_2,i_3,1,2,\cdots\check{i_1}\cdots\check{i_2}\cdots\check{i_3}\cdots,k))
\end{equation}
\begin{equation}\label{eqn:bladegradereduction:262}
\BA_3 \cdot (\Bb_{i_1} \wedge \Bb_{i_2} \wedge \Bb_{i_3})
(\Bb_1 \wedge \cdots \check{\Bb_{i_1}} \cdots \check{\Bb_{i_2}} \cdots \check{\Bb_{i_3}} \cdots \wedge \Bb_k)
\end{equation}

We see that the dot product consumes any of the excess sign variation not described by the sign of the permutation of indices.

The induction hypothesis is basically described above (change \(3\) to \(s\), and add extra dots):

\begin{equation*}
\BA_s \cdot \BB_k =
\sum_{i_1<i_2\cdots<i_s} \Sgn(\pi(i_1,i_2\cdots,i_s,1,2,\cdots\check{i_1}\cdots\check{i_2}\cdots\check{i_s}\cdots,k))
\end{equation*}
\begin{equation}\label{eqn:bladegradereduction:inductionbigdotblade}
\BA_s \cdot (\Bb_{i_1} \wedge \Bb_{i_2} \cdots \wedge \Bb_{i_s})
(\Bb_1 \wedge \cdots \check{\Bb_{i_1}} \cdots \check{\Bb_{i_2}} \cdots \check{\Bb_{i_s}} \cdots \wedge \Bb_k)
\end{equation}

\subsection{Perform the induction}

In a sense this has already been done.  The steps will be pretty much the same as the logic that produced the bivector and trivector
results.  Thinking about typing this up in latex is not fun, so this will be left for a paper proof.
